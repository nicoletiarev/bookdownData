[["index.html", "R Notebook 1 que es?", " R Notebook 1 que es? "],["conceptos-básicos.html", "2 Conceptos Básicos 2.1 Instalación 2.2 La consola de R 2.3 R- cloud, plataforma online de R-Studio", " 2 Conceptos Básicos En este capítulo veremos cómo instalar R y una breve explicación de sus diferentes ventanas, además veremos cómo utilizar la versión online de R conocido como R-cloud. 2.1 Instalación Para instalar R primero debemos ir al sitio oficial en el link siguiente: https://cran.r-project.org/bin/windows/base/ y lo descargamos según las característica del computador. Se debe realizar doble click en el instalar de R, luego se debe ejecutar en el computador seleccionado sí esto nos permite dar la autorización para que se instale R, luego escogemos el idioma deseado y luego se selecciona siguiente y no para la opción de configuraciones. {r echo=FALSE, out.width = 653px, out.height=320px,fig.align=center} knitr::include_graphics(static/img/instalarR.PNG) Es recomendable crear un acceso directo desde el escritorio y una ves descargado procedemos a abrir RSturdio como se observa en la siguente imagen. {r echo=FALSE, out.width = 653px, out.height=320px,fig.align=center} knitr::include_graphics(static/img/instlacionR1.PNG) 2.2 La consola de R Existen 4 ventanas, además de la barra de opciones en la parte superior. {r echo=FALSE, out.width = 653px, out.height=320px,fig.align=center} knitr::include_graphics(static/img/1.PNG) El editor (Ventana 1): se trata del lugar donde editamos la sintaxis para posteriormente ejecutarla. Al escribir allí no sucederá nada, a no ser que se apriete algún botón para ejecutar los comandos o la tecla ctrl+enter. El entorno de variables (Ventana 2) : es el entorno de trabajo del programa: en este lugar se muestra el conjunto de datos y los objetos (resultados, variables, gráficos, etc.) que se almacenan al ejecutar diferentes análisis. La consola (Ventana 3) : Corresponde a lo que sería el software R en su versión básica. Allí el software ejecuta las operaciones realizadas desde el editor de sintaxis. El editor (Ventana 4): Tiene varias subpestañas: La pestaña files permite ver el historial de archivos trabajados con el programa; La pestaña plots permite visualizar los gráficos que se generen; La pestaña packages permite ver los paquetes descargados y guardados en el disco duro así como gestionar su instalación o actualización; La ventana help permite acceder al CRAN - Comprehensive R Archive Network (siempre que se cuente con conexión a Internet), página oficial del software que ofrece diferentes recursos para el programa: manuales para el usuario, cursos on line, información general, descarga de paquetes, información de los paquetes instalados, etc. Esta última pestaña es bastante útil: empleando el motor de búsqueda se accede de manera rápida a manuales de uso de los diferentes paquetes (y sus funciones) instalados en el computador (esto no requiere conexión a Internet); La ventana viewer muestra los resultados al construir reportes mediante funcionalidades tipo rmarkdown . 2.2.1 Crear un script Los scripts son documentos de texto con la extensión de archivo . Estos archivos son iguales a cualquier documento de texto, pero R los puede leer y ejecutar el código que contienen.Aunque R permite el uso interactivo, es recomendable que guardes tu código en un archivo .R, de esta manera puedes usarlo después y compartirlo con otras personas. #```{r echo=FALSE, out.width = 653px, out.height=320px,fig.align=center} #knitr::include_graphics(static/img/2.PNG) #``` En la siguiente imagen, designado con el número 1 se observa la forma en que se debe guardar un script y con la asignación del numero 2 la forma que se corre un código esta genera en la consola la respuesta, que en este caso se observa en la venta 2 la creación de los vectores y el de la data frame. También se puede observar la diferencia en cómo se escribe un código y un comentario. 2.3 R- cloud, plataforma online de R-Studio RStudio Cloud es una solución liviana basada en la nube que permite a cualquiera hacer, compartir, enseñar y aprender ciencia de datos en línea. 2.3.1 Crear un script Primer caso es crear un cuenta con el correo institucional en R-Cloud en el siguiente enlace: https://rstudio.com/products/cloud/ una vez creada se crea un nuevo proyecto, como se observa en la siguiente imagen. #```{r echo=FALSE, out.width = 653px, out.height=320px,fig.align=center} #knitr::include_graphics(static/img/3.PNG) #``` "],["lenguaje.html", "3 Lenguaje 3.1 Tipos de datos 3.2 Estructura de control 3.3 Estructura de datos", " 3 Lenguaje En este capítulo abordaremos los tipos de datos que se usan en una data frame, como también la estructura de control que nos permiten controlar el flujo de ejecución de una secuencia de comandos y la estructura de datos de la data frame. 3.1 Tipos de datos Los tipos de datos simples o fundamentales en R son los siguientes: TIPO EJEMPLO NOMBRE EN INGLÉS CÓDIGO enteros 1 integer is.integer() numericos 1.3 numeric is.numeric() cadena de texto uno character is.character() factor uno factor is.factor() lógico true logical is.logical() perdido NA NA is.na() vacío NULL NULL is.null() complex 1+2i complex is.complex() 3.2 Estructura de control Las estructuras de control nos permiten controlar el flujo de ejecución de una secuencia de comandos. De este modo, podemos poner lógica en el código de R y lograr así reutilizar fragmentos del código una y otra vez. A continuación, veremos la sentencia de control . 3.2.1 Sentencias de Control - If / Else Esta sentencia nos permite decidir si ejecutar o no un fragmento de código en función de una condición. numero &lt;- 6 if (numero &gt; 5){ x &lt;- TRUE } else { x &lt;- FALSE } Otra forma de realizar lo anterior es de la siguiente manera. ifelse(numero &gt; 5, TRUE, FALSE) ## [1] TRUE 3.2.2 Control de Flujo La función while() ejecuta un bucle o ciclo mientras sea verdadera una condición. while(numero &lt; 10) { print(numero) numero &lt;- numero + 1 } ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 El comando for() toman una variable a la que se le asignan los elementos de un objeto (en general, vectores o listas) en forma sucesiva a medida que se van recorriendo los ciclos. #for(&lt;variable&gt; in &lt;objeto iterable&gt;) { # código for(X in 1:5) { print(X) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 El comando switch() permite ejecutar un bloque de código distinto en función del valor de una variable. W &lt;- 1:10 type &lt;- &#39;mean&#39; switch(type, mean = mean(W), median = median(W), sd = sd(W)) ## [1] 5.5 3.3 Estructura de datos 3.3.1 Variables El siguiente código asigna (= o &lt;-) el valor 12 a las variables valor. valor = 25 # o valor &lt;- 25 3.3.2 Vectores Los vectores pueden representar las columnas de una tabla de datos. un ejemplo podría ser la lista del curso con las notas. Si queremos crear vectores en R, utilizamos la función c(). En este caso, la c es de «combinar» (combinar uno o más valores o elementos). Lo más común, es que los vectores sean de tipo numérico, carácter o lógico. #vector 1 Nombre &lt;- c(&quot;Carlos&quot;, &quot;Camila&quot;, &quot;Erick&quot;) #vector 2 Apellido &lt;- c(&quot;Acosta&quot;, &quot;Madrid&quot;, &quot;Ortega&quot;) #vector 3 Prueba_1 &lt;- c(40, 55, 65) #vector 4 Prueba_2 &lt;- c(60, 40, 60) 3.3.2.1 Operar vectores Podemos operar vectores contra valores numéricos o con otros vectores. En la siguiente tabla se mostrarán las operaciones básicas que se pueden hacer con vectores. Operaciones Sintaxis Adición + Sustracción - Dividir / Producto Escalar %*% Producto de Elementos * Suma de Elementos sum() Por ejemplo, se asigna dos décimas a la prueba 1 para cada estudiantes. Prueba_1 + 2 ## [1] 42 57 67 Se resta 5 décimas a las pruebas 2 para cada estudiante. Prueba_2 - 5 ## [1] 55 35 55 Se quiere saber cuánto suman las dos notas por estudiante. Prueba_1 + Prueba_2 ## [1] 100 95 125 Como se observa se sumaron las primeras notas y para obtener las notas modificadas se realiza lo siguiente. # se cambia las notas en los vectores con la nueva asignacion Prueba_1 &lt;- Prueba_1 + 2 Prueba_2 &lt;- Prueba_2 - 5 Prueba_1 + Prueba_2 ## [1] 97 92 122 Queremos saber la suma de las notas de la prueba 1. sum(Prueba_1) ## [1] 166 Ahora queremos saber el promedio entre las dos pruebas por cada estudiante. 3.3.2.2 Operaciones Estadisticas Operaciones Sintaxis Media mean() Mediana median() Máximo max() Mínimo min() Cuantiles quantile() Producto prod() Suma Acumulada cumsum() Producto Acumulado cumprod() Diferencias diff() Coeficiente de Correlación cor() Ahora si queremos calcular la media aritmética o promedio de la prueba 1, se tendrá que utilizar el comando mean(). mean(Prueba_1) ## [1] 55.33333 Queremos ver la nota mínima y máxima de la prueba 2. min(Prueba_2) ## [1] 35 max(Prueba_2) ## [1] 55 Si queremos realizar una suma acumulada de la prueba 1, se utiliza el comando cumsum(), lo que hace es sumar los dos primeros elemento; este resultado lo suma al tercer elemento y así sucesivamente. cumsum(Prueba_1) ## [1] 42 99 166 3.3.2.3 Orden de los vectores Si queremos ordenar los vectores de formas creciente usaremos la función sort(). sort(Prueba_1) ## [1] 42 57 67 La función rev() coloca las componentes de un vector en orden inverso a como han sido introducidas. rev(Prueba_1) ## [1] 67 57 42 También se puede realizar de la siguente forma agregamos la decreasing= TRUE para hacerlo decreciente. sort(Prueba_1, decreasing = TRUE) ## [1] 67 57 42 Si queremos invertir el orden del vector prueba 1. rev(Prueba_1) ## [1] 67 57 42 Ahora si queremos saber el tipo de clase que es el vector class(Prueba_1) ## [1] &quot;numeric&quot; class(Nombre) ## [1] &quot;character&quot; Un vector tiene el mismo tipo que los datos que contiene. Prueba_1&gt;71 ## [1] FALSE FALSE FALSE Prueba_1&lt;71 ## [1] TRUE TRUE TRUE Prueba_1 == 42 ## [1] TRUE FALSE FALSE 3.3.3 Matrices Una matriz R no es más que un vector que cuenta con un atributo llamado dim indicando el número de filas y columnas de la matriz. Se trata, por tanto, de una estructura de datos en la que todos los elementos han de ser del mismo tipo. Podemos crear una matriz a partir de un vector, asi como indicando el número de filas y columnas dejando todos sus elementos vacíos. Ahora crearemos una matrix que contenga números del 4 al 55 que tenga 8 columas y 7 filas Matriz &lt;- matrix(4:55,ncol=8) # Dos formas de generar exactamente ## Warning in matrix(4:55, ncol = 8): data length [52] is not a sub-multiple or multiple of the number of rows [7] Matriz &lt;- matrix(4:55,nrow=7) # la misma matriz ## Warning in matrix(4:55, nrow = 7): data length [52] is not a sub-multiple or multiple of the number of rows [7] Matriz ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 4 11 18 25 32 39 46 53 ## [2,] 5 12 19 26 33 40 47 54 ## [3,] 6 13 20 27 34 41 48 55 ## [4,] 7 14 21 28 35 42 49 4 ## [5,] 8 15 22 29 36 43 50 5 ## [6,] 9 16 23 30 37 44 51 6 ## [7,] 10 17 24 31 38 45 52 7 la función length() devuelve el número de elementos de una matriz. length(Matriz) ## [1] 56 Para conocer el número de filas y columnas podemos usar las función nrow(),ncol() y dim(). nrow(Matriz) ## [1] 7 dim(Matriz) ## [1] 7 8 si queremos nombrar las columnas y filas de la matriz usamos la funciones colnames() y rownomes() respectivamemente. colnames(Matriz) &lt;- c(&#39;Semana1&#39;,&#39;Semana2&#39;,&#39;Semana3&#39;, &#39;Semana4&#39;,&#39;Semana5&#39;, &#39;Semana6&#39;, &#39;Semana7&#39;, &#39;Semana8&#39;) Matriz ## Semana1 Semana2 Semana3 Semana4 Semana5 Semana6 Semana7 Semana8 ## [1,] 4 11 18 25 32 39 46 53 ## [2,] 5 12 19 26 33 40 47 54 ## [3,] 6 13 20 27 34 41 48 55 ## [4,] 7 14 21 28 35 42 49 4 ## [5,] 8 15 22 29 36 43 50 5 ## [6,] 9 16 23 30 37 44 51 6 ## [7,] 10 17 24 31 38 45 52 7 3.3.4 Data frames El data frame es seguramente el tipo de dato más utilizado en R, el data frame está compuesto de múltiples columnas y filas, como una matriz, pero cada columna puede ser un tipo distinto. Ahora con las anteriores variables de las notas de los alumnos crearemos un data frame con función data.frame (). lista_alum &lt;- data.frame(Nombre, Apellido, Prueba_1, Prueba_2) lista_alum ## Nombre Apellido Prueba_1 Prueba_2 ## 1 Carlos Acosta 42 55 ## 2 Camila Madrid 57 35 ## 3 Erick Ortega 67 55 Veremos el número de observaciones con la función nrow(). nrow(lista_alum) ## [1] 3 Nombres de las columnas con la función names(). names(lista_alum) ## [1] &quot;Nombre&quot; &quot;Apellido&quot; &quot;Prueba_1&quot; &quot;Prueba_2&quot; El tipo de caracter que poseen las columnas. str(lista_alum) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ Nombre : Factor w/ 3 levels &quot;Camila&quot;,&quot;Carlos&quot;,..: 2 1 3 ## $ Apellido: Factor w/ 3 levels &quot;Acosta&quot;,&quot;Madrid&quot;,..: 1 2 3 ## $ Prueba_1: num 42 57 67 ## $ Prueba_2: num 55 35 55 "],["tidyverse.html", "4 Tidyverse 4.1 ggplot2 4.2 dplyr 4.3 tidyr 4.4 readr 4.5 purrr 4.6 tibble 4.7 stringr 4.8 forcats 4.9 forcats", " 4 Tidyverse Tidyverse es un conjunto de paquetes en R diseñados para ciencia de datos, el cual nos ayuda en todo el proceso de la importación, transformación, visualizar y modelación datos. Una de las principales ventajas es que tienen estos paquetes que comparten nombre y estructuras comunes., esto quiere decir que todos los nombres están en minúscula o que utilizan la barra baja para llamar a las funciones. #install.packages(&#39;tidyverse&#39;) library(tidyverse) Algunas librería que contiene disponible tidyverse son los siguientes: ggplot2 dplyr tidyr readr purrr tibble stringr forcats 4.1 ggplot2 Esta Liberia nos ayuda a construir los gráficos para la data frame y se basa en la idea que cualquier gráfica se puede construir usando los tres componentes: datos, coordenadas y objetos geométricos (geoms), esto es llamado como gramática de las gráficas. Para más información vea el siguiente enlace. Cheatsheet: https://rstudio.com/wp-content/uploads/2016/12/ggplot2-cheatsheet-2.1-Spanish.pdf 4.2 dplyr la librería dplyr es una gramática de manipulación de datos que proporciona verbos que ayudan a la manipulación de datos algunos funciones más importante son las siguientes: CODIGO DESCRIPCIÓN mutate() Agrega nuevas variables que son funciones de variables existentes select() Elige variables en función de sus nombres. filter() Filtra datos basados en sus valores. summarise() Reduce varios valores a un solo resumen. arrange() Cambia el orden de las filas. slice() Elige filas según la ubicación. relocate() Cambia el orden de las columnas. Todos ellos se combinan de forma natural con group_by(). Para más información vea el siguiente link. Cheatsheet : https://rstudio.com/wp-content/uploads/2015/03/data-wrangling-spanish.pdf A continuación a traves de la base de datos de Airbnb new york airbnb ## id neighbourhood_group room_type price ## 48886 36482809 Manhattan Private room 75 ## 48887 36483010 Manhattan Entire home/apt 200 ## 48888 36483152 Brooklyn Entire home/apt 170 ## 48889 36484087 Manhattan Private room 125 ## 48890 36484363 Queens Private room 65 ## 48891 36484665 Brooklyn Private room 70 ## 48892 36485057 Brooklyn Private room 40 ## 48893 36485431 Manhattan Entire home/apt 115 ## 48894 36485609 Manhattan Shared room 55 ## 48895 36487245 Manhattan Private room 90 str(airbnb) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ id : int 36482809 36483010 36483152 36484087 36484363 36484665 36485057 36485431 36485609 36487245 ## $ neighbourhood_group: Factor w/ 5 levels &quot;Bronx&quot;,&quot;Brooklyn&quot;,..: 3 3 2 3 4 2 2 3 3 3 ## $ room_type : Factor w/ 3 levels &quot;Entire home/apt&quot;,..: 2 1 1 2 2 2 2 1 3 2 ## $ price : int 75 200 170 125 65 70 40 115 55 90 4.2.1 Agregar nuevas columnas con mutate(). Es útil agregar nuevas columnas que sean funciones de columnas existentes.Con la función mutate() sirve más que nada para realizar operaciones matemáticas. por ejemplo, queremos crear una columna de comisión con uno 12% airbnb %&gt;% mutate(comision = price * 0.12) ## id neighbourhood_group room_type price comision ## 1 36482809 Manhattan Private room 75 9.0 ## 2 36483010 Manhattan Entire home/apt 200 24.0 ## 3 36483152 Brooklyn Entire home/apt 170 20.4 ## 4 36484087 Manhattan Private room 125 15.0 ## 5 36484363 Queens Private room 65 7.8 ## 6 36484665 Brooklyn Private room 70 8.4 ## 7 36485057 Brooklyn Private room 40 4.8 ## 8 36485431 Manhattan Entire home/apt 115 13.8 ## 9 36485609 Manhattan Shared room 55 6.6 ## 10 36487245 Manhattan Private room 90 10.8 airbnb ## id neighbourhood_group room_type price ## 48886 36482809 Manhattan Private room 75 ## 48887 36483010 Manhattan Entire home/apt 200 ## 48888 36483152 Brooklyn Entire home/apt 170 ## 48889 36484087 Manhattan Private room 125 ## 48890 36484363 Queens Private room 65 ## 48891 36484665 Brooklyn Private room 70 ## 48892 36485057 Brooklyn Private room 40 ## 48893 36485431 Manhattan Entire home/apt 115 ## 48894 36485609 Manhattan Shared room 55 ## 48895 36487245 Manhattan Private room 90 4.2.2 Seleccionar columnas con select(). La función select() le permite acercarse rápidamente a un subconjunto. airbnb %&gt;% select( id, room_type, neighbourhood_group) ## id room_type neighbourhood_group ## 48886 36482809 Private room Manhattan ## 48887 36483010 Entire home/apt Manhattan ## 48888 36483152 Entire home/apt Brooklyn ## 48889 36484087 Private room Manhattan ## 48890 36484363 Private room Queens ## 48891 36484665 Private room Brooklyn ## 48892 36485057 Private room Brooklyn ## 48893 36485431 Entire home/apt Manhattan ## 48894 36485609 Shared room Manhattan ## 48895 36487245 Private room Manhattan Una forma contraria de usarla es agregando ! que nos dices que queremos sacar esas variables de la data . airbnb %&gt;% select(!( id:room_type:neighbourhood_group)) ## Warning in x:y: numerical expression has 3 elements: only the first used ## room_type price ## 48886 Private room 75 ## 48887 Entire home/apt 200 ## 48888 Entire home/apt 170 ## 48889 Private room 125 ## 48890 Private room 65 ## 48891 Private room 70 ## 48892 Private room 40 ## 48893 Entire home/apt 115 ## 48894 Shared room 55 ## 48895 Private room 90 Hay una serie de funciones auxiliares se pueden utilizar dentro select(), como starts_with(), ends_with(), matches()y contains(). 4.2.3 Filtrar filas con filter(). Esta función nos permite seleccionar un subconjunto de filas en un marco de datos. Usaremos %&gt;% para concatenar la data frame luego dentro de la función podremos el nombre de las variable que deseamos buscar o filtrar luego ponemos == todas la observaciones que tenga Brooklyn y para realizar el segundo filtro se separa con una coma para luego repetir lo anteriormente mencionado. Ahora queremos saber los alojamientos que se encuentra en la Brooklyn con habitación Entire home/apt. airbnb %&gt;% filter(neighbourhood_group == &quot;Brooklyn&quot;, room_type == &quot;Entire home/apt&quot;) ## id neighbourhood_group room_type price ## 1 36483152 Brooklyn Entire home/apt 170 Un equivalente a lo anterior sin usar la función filter()sería el siguiente código. airbnb[airbnb$neighbourhood_group == &quot;Brooklyn&quot;, airbnb$room_type == &quot;Entire home/apt&quot;] 4.2.4 Organizar filas con arrange() La funciona arrange() selecciona filas ylas reordena, para ello se necesita un marco de datos y un conjunto de nombres de columna para ordenar. airbnb %&gt;% arrange(neighbourhood_group , room_type) ## id neighbourhood_group room_type price ## 1 36483152 Brooklyn Entire home/apt 170 ## 2 36484665 Brooklyn Private room 70 ## 3 36485057 Brooklyn Private room 40 ## 4 36483010 Manhattan Entire home/apt 200 ## 5 36485431 Manhattan Entire home/apt 115 ## 6 36482809 Manhattan Private room 75 ## 7 36484087 Manhattan Private room 125 ## 8 36487245 Manhattan Private room 90 ## 9 36485609 Manhattan Shared room 55 ## 10 36484363 Queens Private room 65 Esta se suele usar con la función desc()para ordenar de forma descendente. airbnb %&gt;% arrange( desc(room_type)) ## id neighbourhood_group room_type price ## 1 36485609 Manhattan Shared room 55 ## 2 36482809 Manhattan Private room 75 ## 3 36484087 Manhattan Private room 125 ## 4 36484363 Queens Private room 65 ## 5 36484665 Brooklyn Private room 70 ## 6 36485057 Brooklyn Private room 40 ## 7 36487245 Manhattan Private room 90 ## 8 36483010 Manhattan Entire home/apt 200 ## 9 36483152 Brooklyn Entire home/apt 170 ## 10 36485431 Manhattan Entire home/apt 115 4.2.5 Elija filas usando su posición con slice() Esta función permite indexar filas por sus ubicaciones (enteras). Le permite seleccionar, eliminar y duplicar filas. por ejemplo, si queremos saber los caracteres de los números de filas 6 al 13. airbnb %&gt;% slice(6:13) ## id neighbourhood_group room_type price ## 1 36484665 Brooklyn Private room 70 ## 2 36485057 Brooklyn Private room 40 ## 3 36485431 Manhattan Entire home/apt 115 ## 4 36485609 Manhattan Shared room 55 ## 5 36487245 Manhattan Private room 90 Otras variaciones son slice_head() y slice_tail() seleccione la primera o la última fila. airbnb %&gt;% slice_tail(n = 6) ## id neighbourhood_group room_type price ## 1 36484363 Queens Private room 65 ## 2 36484665 Brooklyn Private room 70 ## 3 36485057 Brooklyn Private room 40 ## 4 36485431 Manhattan Entire home/apt 115 ## 5 36485609 Manhattan Shared room 55 ## 6 36487245 Manhattan Private room 90 slice_sample()selecciona filas al azar. airbnb %&gt;% slice_tail(n = 5) ## id neighbourhood_group room_type price ## 1 36484665 Brooklyn Private room 70 ## 2 36485057 Brooklyn Private room 40 ## 3 36485431 Manhattan Entire home/apt 115 ## 4 36485609 Manhattan Shared room 55 ## 5 36487245 Manhattan Private room 90 Con las funciones slice_min() y slice_max() selecciona las filas con los valores más altos o más bajos de una variable. airbnb %&gt;% slice_max(price, n = 3) ## id neighbourhood_group room_type price ## 1 36483010 Manhattan Entire home/apt 200 ## 2 36483152 Brooklyn Entire home/apt 170 ## 3 36484087 Manhattan Private room 125 4.2.6 Cambiar el orden de las columnas con relocate() Utilice una sintaxis similar select()a la de mover bloques de columnas a la vez. airbnb %&gt;% relocate(price:id) ## price room_type neighbourhood_group id ## 48886 75 Private room Manhattan 36482809 ## 48887 200 Entire home/apt Manhattan 36483010 ## 48888 170 Entire home/apt Brooklyn 36483152 ## 48889 125 Private room Manhattan 36484087 ## 48890 65 Private room Queens 36484363 ## 48891 70 Private room Brooklyn 36484665 ## 48892 40 Private room Brooklyn 36485057 ## 48893 115 Entire home/apt Manhattan 36485431 ## 48894 55 Shared room Manhattan 36485609 ## 48895 90 Private room Manhattan 36487245 4.2.7 Resumir valores con summarise() El último verbo es summarise(). Colapsa un marco de datos en una sola fila. airbnb %&gt;% summarise(price_mean = mean(price)) ## price_mean ## 1 100.5 4.3 tidyr la libreria tidyr crear datos ordenados . Los datos ordenados son datos donde: Cada columna es variable. Cada fila es una observación. Cada celda es un valor único. Tidy data describe una forma estándar de almacenar datos que se utiliza siempre que sea posible en tidyverse . Si se asegura de que sus datos estén ordenados, pasará menos tiempo luchando con las herramientas y más tiempo trabajando en su análisis. 4.4 readr El objetivo de readr es proporcionar una forma rápida y sencilla de leer datos con formato csv, tsv y fwf . Está diseñado para analizar de manera flexible muchos tipos de datos . readr admite siete formatos de archivo con siete read_funciones: CODIGO DESCRIPCIÓN read_csv() archivos separados por comas (CSV) read_tsv() archivos separados por tabulaciones read_delim() archivos delimitados generales read_fwf() archivos de ancho fijo read_table() archivos tabulares donde las columnas están separadas por espacios en blanco read_log() archivos de registro web Para más información vea el siguiente link: https://readr.tidyverse.org/ 4.5 purrr La liberia purrr mejora el conjunto de herramientas de programación funcional (FP) de R al proporcionar un conjunto completo y consistente de herramientas para trabajar con funciones y vectores y proporciona las funciones map(). Para más información vea el siguiente link: https://github.com/rstudio/cheatsheets/blob/master/purrr.pdf 4.6 tibble Un tibble , o tbl_df, Los tibbles son data frames, pero modifican algunas características antiguas para hacer su uso mas más fácil. La mayoría de las veces usaremos el término tibble y data frame de manera indistinta; cuando queramos referirnos de manera particular a la data frame que viene incluido en R lo llamaremos data.frame. 4.7 stringr Con la librería stringr Hay cuatro principales de funciones, el primero es la manipulación de caracteres; estas funciones le permiten manipular caracteres individuales dentro de las cadenas en vectores de caracteres. La segunda son las herramientas de espacios en blanco para agregar, eliminar y manipular espacios en blanco, el tercero son las operaciones sensibles a la configuración regional cuyas operaciones variarán de una ubicación a otra y por último son las funciones de coincidencia de patrones. . Algunas funciones destacadas son las siguientes. library(stringr) 4.7.1 manipulación de caracteres Podemos obtener la longitud longitud de la cadena con la función str_length(): str_length(&quot;hola&quot;) ## [1] 4 Se puede acceder a una letra específica con la función str_sub() colocando la variable y luego la posición que deseamos buscar en este caso la 2. A &lt;- c(&quot;hola&quot;, &quot;hello&quot; ) str_sub(A, 2, 2) ## [1] &quot;o&quot; &quot;e&quot; Estos nos sirven también para modificar la cadena por ejemplo queremos remplazar la o y la e por una X . str_sub(A, 2, 2) &lt;- &quot;X&quot; A ## [1] &quot;hXla&quot; &quot;hXllo&quot; También podemos duplicar las cadenas con la función str_dup(). str_dup(A, c(2, 2)) ## [1] &quot;hXlahXla&quot; &quot;hXllohXllo&quot; 4.7.2 Especio en blanco Con la función str_pad() rellena una cuerda a una longitud fija agregando espacios en blanco adicionales a la izquierda, derecha o ambos lados. str_pad(A, 10) ## [1] &quot; hXla&quot; &quot; hXllo&quot; str_pad(A, 10, &quot;both&quot;) ## [1] &quot; hXla &quot; &quot; hXllo &quot; Entonces, si desea asegurarse de que todas las cadenas tengan la misma longitud, combine str_pad() y str_trunc(): 4.8 forcats A %&gt;% str_trunc(10) %&gt;% str_pad(10, &quot;right&quot;) ## [1] &quot;hXla &quot; &quot;hXllo &quot; Lo contrario de lo anterior es la función str_trim() que elimina los espacios en blanco A &lt;- c(&quot; hola &quot;, &quot; hello &quot; ) str_trim(A) ## [1] &quot;hola&quot; &quot;hello&quot; Agregaremos left para indicar que dirección eliminar str_trim(A, &quot;left&quot;) ## [1] &quot;hola &quot; &quot;hello &quot; Puede utilizar la función str_wrap() para modificar los espacios en blanco existentes para ajustar un párrafo de texto, de modo que la longitud de cada línea sea lo más similar posible. 4.8.1 Sensible a la configuración regional Si queremos modificar las palabras de pasarla de minúscula a mayúscula se utiliza la función str_to_upper(). A &lt;- c(&quot;hola&quot;, &quot;hello&quot; ) A &lt;- str_to_upper(A) A ## [1] &quot;HOLA&quot; &quot;HELLO&quot; De mayúscula a minúscula. str_to_title(A) ## [1] &quot;Hola&quot; &quot;Hello&quot; Sí queremos ordenar y clasificar la cadena usaremos la función str_order() y str_sort(). B &lt;- c(&quot;c&quot;, &quot;b&quot;, &quot;e&quot;) str_order(B) ## [1] 2 1 3 str_sort(B) ## [1] &quot;b&quot; &quot;c&quot; &quot;e&quot; 4.8.2 La coincidencia de patrones La gran mayoría de funciones de stringr funcionan con patrones, estos están parametrizados por la tarea que realizan y los tipos de patrones que coinciden. numeros &lt;- c( &quot;celular&quot;, &quot;219 733 8965&quot;, &quot;329-293-8753&quot;, &quot;casa: 579-499-7527; trabajo: 543.355.3679&quot; ) telefonos &lt;- &quot;([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})&quot; Usaremos la función str_detect() detecta la presencia o ausencia de un patrón y devuelve un vector lógico. Por ejemplo, para saber cuál pertenece a un número telefónico. str_detect(numeros, telefonos) ## [1] FALSE TRUE TRUE TRUE Con la función str_subset() devuelve los elementos de un vector de caracteres que coinciden con una expresión regular. str_subset(numeros, telefonos) ## [1] &quot;219 733 8965&quot; &quot;329-293-8753&quot; &quot;casa: 579-499-7527; trabajo: 543.355.3679&quot; Ahora con la función str_count() cuenta el número de coincidencias, por ejemplo, queremos saber cuántos números de teléfonos existe en la cadena. str_count(numeros, telefonos) ## [1] 0 1 1 2 Con la función str_locate() localiza la primera posición de un patrón y devuelve una matriz numérica con columnas al comienzo y al final, por ejemplo, si queremos saber en qué parte de la cadena se encuentra el número de teléfono. str_locate(numeros, telefonos) ## start end ## [1,] NA NA ## [2,] 1 12 ## [3,] 1 12 ## [4,] 7 18 La función str_locate_all() localiza todas las coincidencias, devolviendo una lista de matrices numéricas. str_locate_all(numeros, telefonos) ## [[1]] ## start end ## ## [[2]] ## start end ## [1,] 1 12 ## ## [[3]] ## start end ## [1,] 1 12 ## ## [[4]] ## start end ## [1,] 7 18 ## [2,] 30 41 Si queremos extraer el texto correspondiente a la primera coincidencia, devolviendo un vector de caracteres con la función str_extract(). str_extract(numeros, telefonos) ## [1] NA &quot;219 733 8965&quot; &quot;329-293-8753&quot; &quot;579-499-7527&quot; Si queremos ver todos los números telefónicos usaremos la función str_extract_all(). str_extract_all(numeros, telefonos) ## [[1]] ## character(0) ## ## [[2]] ## [1] &quot;219 733 8965&quot; ## ## [[3]] ## [1] &quot;329-293-8753&quot; ## ## [[4]] ## [1] &quot;579-499-7527&quot; &quot;543.355.3679&quot; Ahora si queremos remplazar los números usaremos la función str_replace() reemplaza el primer patrón coincidente y devuelve un vector de caracteres. str_replace(numeros, telefonos, &quot;XXX-XXX-XXXX&quot;) ## [1] &quot;celular&quot; &quot;XXX-XXX-XXXX&quot; &quot;XXX-XXX-XXXX&quot; ## [4] &quot;casa: XXX-XXX-XXXX; trabajo: 543.355.3679&quot; str_replace_all(numeros, telefonos, &quot;XXX-XXX-XXXX&quot;) ## [1] &quot;celular&quot; &quot;XXX-XXX-XXXX&quot; &quot;XXX-XXX-XXXX&quot; ## [4] &quot;casa: XXX-XXX-XXXX; trabajo: XXX-XXX-XXXX&quot; 4.9 forcats El paquete forcats proporciona un conjunto de herramientas útiles que resuelvan problemas comunes con factores. Los factores son útiles cuando tiene datos categóricos, variables que tienen un conjunto de valores fijo y conocido, y cuando desea mostrar los vectores de caracteres en orden no alfabético. "],["analisis-de-datos.html", "5 Analisis de datos 5.1 Cargar de datos 5.2 tipos de datos en tibble 5.3 Combinar base de datos 5.4 exploración de la data", " 5 Analisis de datos 5.1 Cargar de datos Ahora demostraremos como cargar la data tanto para r-studio de escritorio como para R- cloud (versión online). 5.1.1 En R-Studio Para importar bases de datos en formato Excel en R- studio se ingresa en la ventana 2 en donde dice Import Dataset como se observa en la siguiente imagen. Luego aparece una ventana en donde buscamos nuestra base de datos en nuestro computador, y una vez seleccionado el archivo aparece una segunda ventana llamada Import Dataset como se muestra en las dos siguientes imágenes. En la primera es la forma correcta de subir la base de datos como se encuentra demarcado el Heading colocamos YES esto es con el fin de que la base de datos que subamos tenga el encabezado correspondiente como se encuentra demostrado en la imagen. De lo contrario se puede ver en la imagen dos lo que sucede si ponemos NO en donde el encabezado se muestra como una observación más. Una vez importada la base de datos podemos ver que aparece como Data Frame en la ventana 2, en esta también nos muestra características básicas como la cantidad de observaciones que posee el Data Frame. También se puede observar en la consola que se importó el archivo correctamente, es recomendable copiar y pegar en el script lo demarcado , así cada vez que queramos trabajar se subirá automáticamente la base de datos. 5.1.2 R- cloud En cuanto a R-Cloud se suben los archivos un poco diferentes, en este nos tenemos que dirigir la ventana 4 en donde dice Upload, esto abrirá una ventana llamada Target directory presionamos donde die elegir archivo, con esto nos abre otra ventana en donde debemos buscar el archivo que dejamos importar. Esto creara un archivo en la ventana 4 como se muestra en la siguiente imagen. Y ahora se procede a realizar la importación del archivo en la segunda ventana como se realiza en R- Studio. 5.2 tipos de datos en tibble 5.3 Combinar base de datos Cuando se trabaja con grandes cantidades de datos, a veces es necesario combinar bases de datos para consolidarlo en una que contenga toda la información. En este capítulo veremos diferente forma de realizar datos relacionales, pero primero hay que tener en cuenta la siguiente recomendación: Debes extraer los datos básicos de cada base de datos como. La cantidad de filas y columnas por cada base de datos. De qué tipo son las columnas, por ejemplo, si son numéricas o carácter . Revisar cuidadosamente la visualización de la base de datos para observar si existen algún punto en común entre las bases de datos. Debes conocer que representa cada variable de base de datos. luego realiza un diagrama para ver las variables en común. Como se observa en la siguiente imagen. En segundo lugar, hay que tener en cuenta las tres familias de verbos diseñados para trabajar con datos relacionales: filtering joins (Filtrado de combinaciones) , que filtran las observaciones de un marco de datos en función de si coinciden o no con una observación de la otra tabla. set operations (Establecer operaciones) , que tratan las observaciones como si fueran elementos establecidos. mutating joins (Uniones de trasformación) , que agregan nuevas variables a un marco de datos a partir de observaciones coincidentes en otro. 5.3.1 mutating joins (Uniones de trasformación) Esta busca combinar variables a partir de dos tablas, en la que busca coincidencias de observaciones de acuerdo con su put y luego copia las variables de una tabla en la otra. Tal como mutate(), las funciones de unión agregan variables hacia la derecha, por lo que, si tienes muchas variables inicialmente, las nuevas variables no se imprimirán. Uno de los más simples es a través de mage() para hacer cruces de tablas imagen Para ello crearemos 2 bases baseA y baseB con una llave o columna en común ID. baseA ## ID A ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 ## 4 4 x4 ## 5 5 x5 ## 6 6 x6 baseB ## ID B ## 1 1 y1 ## 2 2 y2 ## 3 3 y3 ## 4 4 y4 ## 5 5 y5 ## 6 6 y6 Para unir la baseA y en la baseB utilizaremos merge() para crear la base AB. baseAB = merge(baseA, baseB, by = &quot;ID&quot;) baseAB ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 x5 y5 ## 6 6 x6 y6 5.3.1.1 Unión interior La forma más simple de unión es la unión interior (del inglés inner join). Como se muestra en la siguiente imagen: Pero primero crearemos 3 vectores ID, A y B para luego crear un baseA y baseB. A continuación, en vez de crear un base llamada baseAB se concatena la baseA con la baseB para llamarse baseA. baseA %&gt;% inner_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 4 x4 y4 ## 5 5 x5 y5 ## 6 6 x6 y6 Como se observas se unieron las baseA y baseB, una unión interior mantiene las observaciones que aparecen en ambas tablas. 5.3.2 Uniones exteriores Una unión exterior mantiene las observaciones que aparecen en al menos una de las tablas. Existen tres tipos de uniones exteriores: Una unión izquierda left_join() mantiene todas las observaciones en x. baseA %&gt;% left_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; ## 4 4 x4 y4 ## 5 5 x5 y5 ## 6 6 x6 y6 Una unión derecha right_join() mantiene todas las observaciones en y. baseA %&gt;% right_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 y3 ## 4 5 x5 y5 ## 5 6 x6 y6 ## 6 4 &lt;NA&gt; y4 Una unión completa full_join() mantiene todas las observaciones en x e y. observaciones en y. baseA %&gt;% full_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 4 x4 y4 ## 4 5 x5 &lt;NA&gt; ## 5 6 x6 y6 ## 6 3 &lt;NA&gt; y3 5.3.2.1 claves duplicadas Pero que sucede si la id esta duplicada ¿cómo podemos unirlas? para ellos nos pondremos en dos escenarios diferentes. El primero en una tabla tiene claves duplicadas. Esto es útil cuando quieres agregar información adicional dado que típicamente existe una relación uno a muchos. baseA %&gt;% left_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 1 x3 y1 ## 4 2 x4 y2 Ambas tablas tienen claves duplicadas. Esto es usualmente un error debido a que en ninguna de las tablas las claves identifican de manera única una observación. Cuando unes claves duplicadas, se obtienen todas las posibles combinaciones, es decir, el producto cartesiano: baseA %&gt;% left_join(baseB, by = &quot;ID&quot;) ## ID A B ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 2 x2 y3 ## 4 2 x3 y2 ## 5 2 x3 y3 ## 6 3 x4 y4 5.3.2.2 otras formas con dplyr con merge inner_join(base1, base2) merge(base1, base2) full_join(base1, base2) merge(base1, base2, all.x = TRUE, all.y = TRUE) right_join(base1, base2) merge(base1, base2, all.y = TRUE) left_join(base1, base2) merge(base1, base2, all.x = TRUE) La ventaja de los verbos específicos de dplyr es que muestran de manera clara la intención del código: la diferencia entre las uniones es realmente importante, pero se esconde en los argumentos de merge(). Las uniones de dplyr son considerablemente más rápidas y no generan problemas con el orden de las filas. Si las variables clave tiene diferentes numbre (solo cosas en comun) merge(base1, base2, by.x=\"nombre variable base 1\", by.y=\"nombre variable base 2\") Si la variable se llame igual en las dos bases (solo cosas en comun) merge(base1, base2, by=\"nombre variable\") Si queremos que se unan todos los casos all=TRUE: merge(base1, base2, by=\"nombre variable\", all=TRUE) Si queremos unir por más de una variable: (si las variables se llaman igual en ambas bases) merge(base1, base2, by=c(\"variable1\", \"variable2\")) Si las variables se llaman diferente en ambas bases merge(base1, base2, by.x=c(\"variable1\", \"variable2\"), by.y=c(\"variable1\", \"variable2\")) Otras formas efectivas de realizar una union de base son los siguientes codigos. 5.3.3 Uniones de filtros Las uniones de filtro unen observaciones de la misma forma que las uniones de transformación, pero afectan a las observaciones no a las variables. Existen dos tipos: semi_join(x, y) mantiene todas las observaciones en x con coincidencias en y. baseA %&gt;% semi_join(baseB) ## Joining, by = &quot;ID&quot; ## ID A ## 1 1 x1 ## 2 2 x2 ## 3 4 x4 ## 4 5 x5 anti_join(x, y) descarta todas las observaciones en x con coincidencias en y. baseA %&gt;% anti_join(baseB) ## Joining, by = &quot;ID&quot; ## ID A ## 1 3 x3 Las semi uniones son útiles para unir tablas resumen previamente filtradas con las filas originales. Por ejemplo, imagina que encontraste los diez destinos más populares: 5.4 exploración de la data Ahora lo realizaremos una exploración de la base de datos , en este caso usaremos la base de datos disponibles llamado AB_NYC_2019.csv, que se encuentra disponible en el siguiente link: https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data Pero primero necesitamos la siguientes Liberia: library(dplyr) library(tidyr) library(readxl) Importamos la base de datos a R-Studio como se vio en capítulos anteriores. airbnb &lt;- read.csv(&quot;C:/Users/nicol/OneDrive/Escritorio/DATA/AB_NYC_2019.csv&quot;) 5.4.1 Inspección de una tabla Luego realizamos la visualización de la data. airbnb ## Warning in instance$preRenderHook(instance): It seems your data is too big for client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html Ahora realizaremos una breve descripción de las columnas de la data : Variable Descripción id Código de hostal name Nombre de Airbnb host_id Código de Airbnb host_name Nombre del anfitrión neighbourhood_group Distritos de new york neighbourhood Vecindarios de los distritos latitude Coordenada latitud longitude Coordenada de longitud room_type Tipo de habitación price Precio minimum_nights cantidad mínima de noches para una reserva number_of_reviews Numero de revisiones por mes last_review Ultimas reseña reviews_per_month Cantidad de comentarios por mes para la propiedad calculated_host_listings_count Numero de anuncios availability_365 número de días en los que la lista está disponible para la reserva. Para obtener más información revisamos a través de los códigos como lo siguientes: Queremos determinar si la base de datos posee algún problema usamos función problems(). problems(airbnb) ## [1] row col expected actual ## &lt;0 rows&gt; (or 0-length row.names) Para ver la cantidad de observaciones usaremos la función nrow(). nrow(airbnb) ## [1] 48895 Cuantas columnas o variables poseen la data con la función ncol(). ncol(airbnb) ## [1] 16 si queremos ver las primeras observaciones usamos la función head(). head(airbnb) # primeras seis filas ## id name host_id host_name neighbourhood_group neighbourhood latitude longitude room_type ## 1 2539 Clean &amp; quiet apt home by the park 2787 John Brooklyn Kensington 40.64749 -73.97237 Private room ## 2 2595 Skylit Midtown Castle 2845 Jennifer Manhattan Midtown 40.75362 -73.98377 Entire home/apt ## 3 3647 THE VILLAGE OF HARLEM....NEW YORK ! 4632 Elisabeth Manhattan Harlem 40.80902 -73.94190 Private room ## 4 3831 Cozy Entire Floor of Brownstone 4869 LisaRoxanne Brooklyn Clinton Hill 40.68514 -73.95976 Entire home/apt ## 5 5022 Entire Apt: Spacious Studio/Loft by central park 7192 Laura Manhattan East Harlem 40.79851 -73.94399 Entire home/apt ## 6 5099 Large Cozy 1 BR Apartment In Midtown East 7322 Chris Manhattan Murray Hill 40.74767 -73.97500 Entire home/apt ## price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count availability_365 ## 1 149 1 9 2018-10-19 0.21 6 365 ## 2 225 1 45 2019-05-21 0.38 2 355 ## 3 150 3 0 NA 1 365 ## 4 89 1 270 2019-07-05 4.64 1 194 ## 5 80 10 9 2018-11-19 0.10 1 0 ## 6 200 3 74 2019-06-22 0.59 1 129 tail(airbnb) # últimas seis filas ## id name host_id host_name neighbourhood_group neighbourhood latitude ## 48890 36484363 QUIT PRIVATE HOUSE 107716952 Michael Queens Jamaica 40.69137 ## 48891 36484665 Charming one bedroom - newly renovated rowhouse 8232441 Sabrina Brooklyn Bedford-Stuyvesant 40.67853 ## 48892 36485057 Affordable room in Bushwick/East Williamsburg 6570630 Marisol Brooklyn Bushwick 40.70184 ## 48893 36485431 Sunny Studio at Historical Neighborhood 23492952 Ilgar &amp; Aysel Manhattan Harlem 40.81475 ## 48894 36485609 43rd St. Time Square-cozy single bed 30985759 Taz Manhattan Hell&#39;s Kitchen 40.75751 ## 48895 36487245 Trendy duplex in the very heart of Hell&#39;s Kitchen 68119814 Christophe Manhattan Hell&#39;s Kitchen 40.76404 ## longitude room_type price minimum_nights number_of_reviews last_review reviews_per_month calculated_host_listings_count ## 48890 -73.80844 Private room 65 1 0 NA 2 ## 48891 -73.94995 Private room 70 2 0 NA 2 ## 48892 -73.93317 Private room 40 4 0 NA 2 ## 48893 -73.94867 Entire home/apt 115 10 0 NA 1 ## 48894 -73.99112 Shared room 55 1 0 NA 6 ## 48895 -73.98933 Private room 90 7 0 NA 1 ## availability_365 ## 48890 163 ## 48891 9 ## 48892 36 ## 48893 27 ## 48894 2 ## 48895 23 Si queremos saber los nombre las columnas o variables se usa la función colnames() colnames(airbnb) ## [1] &quot;id&quot; &quot;name&quot; &quot;host_id&quot; &quot;host_name&quot; ## [5] &quot;neighbourhood_group&quot; &quot;neighbourhood&quot; &quot;latitude&quot; &quot;longitude&quot; ## [9] &quot;room_type&quot; &quot;price&quot; &quot;minimum_nights&quot; &quot;number_of_reviews&quot; ## [13] &quot;last_review&quot; &quot;reviews_per_month&quot; &quot;calculated_host_listings_count&quot; &quot;availability_365&quot; 5.4.2 Selección de filas y columnas A veces queremos trabajar con un subconjunto de la tabla, como por ejemplo una selección de filas y/o columnas. Para este tipo de selecciones se usa el corchete []. airbnb[1:5, ] # las 5 primeras filas ## Warning in instance$preRenderHook(instance): It seems your data is too big for client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html airbnb[,3:4] # las 5 primeras filas ## Warning in instance$preRenderHook(instance): It seems your data is too big for client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html Como se observó los corchetes tienen dos partes separadas por una coma la que la precede se refiere a las filas que se encuentra a las izquierdas de la coma y a la derecha las columnas. Pero no es necesario conocer índice de una determinada columna para seleccionarla, ya que los corchetes se pueden utilizar como poniendo el nombre de la columna como se ve a continuación. airbnb[1:4,&quot;id&quot;] ## [1] 2539 2595 3647 3831 si queremo extraer y operar sobre columnas individuales de una tabla se utiliza el signo $. airbnb$id # las 5 primeras filas Ahora con la función str() veremos cómo están compuestas las columnas si son numéricas o factor, como también veremos la cantidad de variables y observaciones que posee la data. str(airbnb) ## &#39;data.frame&#39;: 48895 obs. of 16 variables: ## $ id : int 2539 2595 3647 3831 5022 5099 5121 5178 5203 5238 ... ## $ name : Factor w/ 47906 levels &quot;&quot;,&quot;&#39;Fan&#39;tastic&quot;,..: 12661 38172 45171 15702 19366 25001 8337 25048 15597 17682 ... ## $ host_id : int 2787 2845 4632 4869 7192 7322 7356 8967 7490 7549 ... ## $ host_name : Factor w/ 11453 levels &quot;&quot;,&quot;&#39;Cil&quot;,&quot;-TheQueensCornerLot&quot;,..: 5051 4846 2962 6264 5982 1970 3601 9699 6935 1264 ... ## $ neighbourhood_group : Factor w/ 5 levels &quot;Bronx&quot;,&quot;Brooklyn&quot;,..: 2 3 3 2 3 3 2 3 3 3 ... ## $ neighbourhood : Factor w/ 221 levels &quot;Allerton&quot;,&quot;Arden Heights&quot;,..: 109 128 95 42 62 138 14 96 203 36 ... ## $ latitude : num 40.6 40.8 40.8 40.7 40.8 ... ## $ longitude : num -74 -74 -73.9 -74 -73.9 ... ## $ room_type : Factor w/ 3 levels &quot;Entire home/apt&quot;,..: 2 1 2 1 1 1 2 2 2 1 ... ## $ price : int 149 225 150 89 80 200 60 79 79 150 ... ## $ minimum_nights : int 1 1 3 1 10 3 45 2 2 1 ... ## $ number_of_reviews : int 9 45 0 270 9 74 49 430 118 160 ... ## $ last_review : Factor w/ 1765 levels &quot;&quot;,&quot;2011-03-28&quot;,..: 1503 1717 1 1762 1534 1749 1124 1751 1048 1736 ... ## $ reviews_per_month : num 0.21 0.38 NA 4.64 0.1 0.59 0.4 3.47 0.99 1.33 ... ## $ calculated_host_listings_count: int 6 2 1 1 1 1 1 1 1 4 ... ## $ availability_365 : int 365 355 365 194 0 129 0 220 0 188 ... Podemos observar a grandes rasgos que las variables room_type posee 3 niveles., que existe datos vacíos NA en la variable reviews_per_month y que neighbourhood posee 221 niveles. Ahora veremos si existen datos vacíos en la data con la función sapply() y para buscar los datos vacíos en se utiliza is.na(). sapply(airbnb, function(airbnb) sum(is.na(airbnb))) ## id name host_id host_name ## 0 0 0 0 ## neighbourhood_group neighbourhood latitude longitude ## 0 0 0 0 ## room_type price minimum_nights number_of_reviews ## 0 0 0 0 ## last_review reviews_per_month calculated_host_listings_count availability_365 ## 0 10052 0 0 Observamos que en las variables reviews_per_month posee 10052 datos vacíos y para ver los datos vacíos es lo mismo pero con is.null(). sapply(airbnb, function(airbnb) sum(is.null(airbnb))) ## id name host_id host_name ## 0 0 0 0 ## neighbourhood_group neighbourhood latitude longitude ## 0 0 0 0 ## room_type price minimum_nights number_of_reviews ## 0 0 0 0 ## last_review reviews_per_month calculated_host_listings_count availability_365 ## 0 0 0 0 No existe datos nulos en la data como se puede observar. Ahora para eliminar los datos vacíos observados anteriormente. airbnb &lt;- airbnb[!is.na(airbnb$reviews_per_month),] Veremos si las observaciones fueron eliminadas. nrow(airbnb) ## [1] 38843 5.4.3 Exploración de las variables Realizaremos la exploración de la variable ID, lo primero que veremos si los datos de esta variable son datos únicos, para ello crearemos la data id y usaremos la función select() que nos ayudara seleccionar la base y las variables que deseamos observar, y además utilizaremos la función unique para ver si son únicas en su valor. id &lt;- select(airbnb, id) %&gt;% unique nrow(id) ## [1] 38843 La data tiene 48895 datos y la columna ID igual por lo que no existen datos repetidos. Con la variable name veremos si existen datos vacíos en la data y cual es id de estos datos vacíos. airbnb%&gt;% select(id, name)%&gt;% filter (is.na (name)) ## [1] id name ## &lt;0 rows&gt; (or 0-length row.names) No existe datos vacíos. Ahora siguiéremos saber el nombre especifico de un id, usaremos el mismo código anterior, pero en la función filter colocamos id == que representa el igual y el id que deseamos buscar en este caso es el 2232600.. airbnb%&gt;% select(id, name)%&gt;% filter (id == &quot;2232600&quot;) ## id name ## 1 2232600 Como observamos no existe nombre, esto demuestra a a veces es bueno realizar más de una revisión. para solucionar estos problemas remplazaremos los datos en blanco por No . airbnb$name[airbnb$name == &quot;&quot;] &lt;- &quot;No&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, airbnb$name == &quot;&quot;, value = structure(c(12661L, : invalid factor level, NA generated airbnb%&gt;% select(id, name)%&gt;% filter(is.na (name)) ## id name ## 1 2232600 &lt;NA&gt; ## 2 4209595 &lt;NA&gt; ## 3 4370230 &lt;NA&gt; ## 4 9325951 &lt;NA&gt; ## 5 10052289 &lt;NA&gt; ## 6 22275821 &lt;NA&gt; Antes de eliminarlo tenemos que ver si estos datos son relevantes para nuestro estudio y si esas variables la utilizaremos más adelante. airbnb$host_name[airbnb$host_name == &quot;&quot;] &lt;- &quot;No&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, airbnb$host_name == &quot;&quot;, value = structure(c(5051L, : invalid factor level, NA generated Ahora veremos la variable neighbourhood_group cuales son los niveles de esta variable. neighbourhood_group &lt;- select(airbnb, neighbourhood_group) %&gt;% unique neighbourhood_group ## neighbourhood_group ## 1 Brooklyn ## 2 Manhattan ## 47 Queens ## 170 Staten Island ## 172 Bronx Vemos que son 5 distritos. Y en cuento a la variable neighbourhood con el mismo condigo anterior. neighbourhood &lt;- select(airbnb, neighbourhood) %&gt;% unique neighbourhood ## neighbourhood ## 1 Kensington ## 2 Midtown ## 4 Clinton Hill ## 5 East Harlem ## 6 Murray Hill ## 7 Bedford-Stuyvesant ## 8 Hell&#39;s Kitchen ## 9 Upper West Side ## 10 Chinatown ## 13 South Slope ## 15 West Village ## 16 Williamsburg ## 17 Fort Greene ## 18 Chelsea ## 19 Crown Heights ## 22 Park Slope ## 26 Windsor Terrace ## 29 Inwood ## 30 East Village ## 31 Harlem ## 33 Greenpoint ## 38 Bushwick ## 40 Lower East Side ## 46 Prospect-Lefferts Gardens ## 47 Long Island City ## 55 Kips Bay ## 60 SoHo ## 63 Upper East Side ## 64 Prospect Heights ## 74 Washington Heights ## 78 Woodside ## 84 Flatbush ## 86 Brooklyn Heights ## 102 Carroll Gardens ## 110 Gowanus ## 137 Flatlands ## 143 Cobble Hill ## 144 Flushing ## 159 Boerum Hill ## 162 Sunnyside ## 164 DUMBO ## 170 St. George ## 172 Highbridge ## 175 Financial District ## 182 Ridgewood ## 183 Morningside Heights ## 197 Jamaica ## 200 Middle Village ## 212 NoHo ## 219 Ditmars Steinway ## 224 Flatiron District ## 228 Roosevelt Island ## 235 Greenwich Village ## 242 Little Italy ## 247 East Flatbush ## 250 Tompkinsville ## 258 Astoria ## 262 Eastchester ## 310 Kingsbridge ## 343 Two Bridges ## 361 Queens Village ## 368 Rockaway Beach ## 404 Forest Hills ## 420 Nolita ## 434 Woodlawn ## 485 University Heights ## 495 Gramercy ## 511 Allerton ## 522 East New York ## 554 Theater District ## 558 Concourse Village ## 561 Sheepshead Bay ## 572 Emerson Hill ## 576 Fort Hamilton ## 578 Bensonhurst ## 588 Tribeca ## 599 Shore Acres ## 609 Sunset Park ## 611 Concourse ## 651 Elmhurst ## 672 Brighton Beach ## 684 Jackson Heights ## 695 Cypress Hills ## 699 St. Albans ## 703 Arrochar ## 737 Rego Park ## 738 Wakefield ## 867 Clifton ## 884 Bay Ridge ## 958 Graniteville ## 967 Spuyten Duyvil ## 968 Stapleton ## 975 Briarwood ## 976 Ozone Park ## 977 Columbia St ## 1034 Vinegar Hill ## 1061 Mott Haven ## 1070 Longwood ## 1071 Canarsie ## 1109 Battery Park City ## 1140 Civic Center ## 1163 East Elmhurst ## 1179 New Springville ## 1194 Morris Heights ## 1328 Arverne ## 1341 Gravesend ## 1425 Tottenville ## 1508 Mariners Harbor ## 1558 Concord ## 1608 Borough Park ## 1689 Bayside ## 1690 Downtown Brooklyn ## 1725 Port Morris ## 1750 Fieldston ## 1803 Kew Gardens ## 1872 Midwood ## 1883 College Point ## 1966 Mount Eden ## 2106 City Island ## 2195 Glendale ## 2259 Red Hook ## 2442 Richmond Hill ## 2828 Maspeth ## 2840 Port Richmond ## 2931 Williamsbridge ## 2984 Soundview ## 3021 Woodhaven ## 3051 Co-op City ## 3102 Stuyvesant Town ## 3240 Parkchester ## 3350 North Riverdale ## 3396 Dyker Heights ## 3440 Bronxdale ## 3762 Sea Gate ## 3810 Riverdale ## 3825 Kew Gardens Hills ## 3965 Bay Terrace ## 4036 Norwood ## 4141 Claremont Village ## 4167 Whitestone ## 4227 Fordham ## 4374 Bayswater ## 4478 Navy Yard ## 4805 Brownsville ## 4862 Eltingville ## 5218 Mount Hope ## 5467 Clason Point ## 5546 Lighthouse Hill ## 5612 Springfield Gardens ## 5841 Howard Beach ## 5994 Belle Harbor ## 6054 Jamaica Estates ## 6109 Van Nest ## 6369 Bellerose ## 6539 Fresh Meadows ## 6933 Morris Park ## 6942 West Brighton ## 7135 Far Rockaway ## 7226 South Ozone Park ## 7227 Tremont ## 7301 Corona ## 7770 Great Kills ## 8019 Manhattan Beach ## 8289 Marble Hill ## 8783 Dongan Hills ## 9564 East Morrisania ## 9764 Hunts Point ## 9987 Neponsit ## 10133 Pelham Bay ## 10261 Randall Manor ## 10711 Throgs Neck ## 10723 Todt Hill ## 10735 West Farms ## 10835 Silver Lake ## 11212 Laurelton ## 11624 Grymes Hill ## 11739 Holliswood ## 11895 Pelham Gardens ## 12766 Rosedale ## 13683 Castleton Corners ## 13738 Edgemere ## 13816 New Brighton ## 14236 Baychester ## 14749 Melrose ## 15431 Bergen Beach ## 15778 Cambria Heights ## 16036 Richmondtown ## 17313 Howland Hook ## 17398 Schuylerville ## 17596 Coney Island ## 18445 Prince&#39;s Bay ## 18519 South Beach ## 19110 Bath Beach ## 19470 Midland Beach ## 19556 Jamaica Hills ## 19900 Oakwood ## 20197 Castle Hill ## 21630 Douglaston ## 21848 Huguenot ## 22282 Edenwald ## 22311 Belmont ## 22595 Grant City ## 23450 Westerleigh ## 23868 Morrisania ## 25147 Bay Terrace, Staten Island ## 25189 Westchester Square ## 25292 Little Neck ## 25862 Rosebank ## 25881 Unionport ## 26236 Mill Basin ## 26403 Hollis ## 29605 Arden Heights ## 29739 Bull&#39;s Head ## 30608 Olinville ## 33262 Rossville ## 33699 Breezy Point ## 34162 Willowbrook ## 34939 New Dorp Beach Con la variable price veremos el promedio , máximo, varianza y mínimo. summarise(airbnb, mean = mean(price), variance = var(price), min = min(price), max = max(price)) ## mean variance min max ## 1 142.3179 38787.58 0 10000 El mínimo es 0 por que existen hospedaje con precio 0 esto es ilógico para una empresa por lo que se busca identificar cuáles son estos datos. airbnb%&gt;% select(id, price)%&gt;% filter (price == &quot;0&quot;) ## id price ## 1 18750597 0 ## 2 20333471 0 ## 3 20523843 0 ## 4 20608117 0 ## 5 20624541 0 ## 6 20639628 0 ## 7 20639792 0 ## 8 20639914 0 ## 9 21291569 0 ## 10 21304320 0 Son 10 datos y como los ID son datos únicos se entiende que estos datos no se repitieran. esta columna es relevante por lo que se elimina los 11 datos nulos. airbnb &lt;-airbnb [airbnb$price!=&quot;0&quot;,] #elimina los 0 summarise(airbnb, mean = mean(price), variance = var(price), min = min(price), max = max(price)) ## mean variance min max ## 1 142.3546 38792.35 10 10000 Con la variable minimum_nights. summarise(airbnb, mean = mean(minimum_nights), variance = var(minimum_nights), min = min(minimum_nights), max = max(minimum_nights)) ## mean variance min max ## 1 5.867561 302.2754 1 1250 Existen máximos con más de 365 días por lo que se determina que no es coherente por lo que se elimina las celdas mayores a 365. nrow(filter(airbnb, minimum_nights&gt;=366)) ## [1] 6 cuando queremos eliminar observaciones especificas utilizaos el siguente codigo. airbnb&lt;-airbnb[!(airbnb$minimum_nights %in% c(&quot;999&quot;,&quot;370&quot;,&quot;1000&quot;,&quot;1250&quot;,&quot;500&quot;,&quot;480&quot;,&quot;500&quot;,&quot;999&quot;,&quot;500&quot;,&quot;366&quot;,&quot;999&quot;,&quot;400&quot;,&quot;990&quot;,&quot;500&quot;)),] Se elimina para no afectar la data y verificamos. summarise(airbnb, mean = mean(minimum_nights), variance = var(minimum_nights), min = min(minimum_nights), max = max(minimum_nights)) ## mean variance min max ## 1 5.74953 195.6422 1 365 Ahora veremos la variable number_of_reviews. summarise(airbnb, mean = mean(number_of_reviews), variance = var(number_of_reviews), min = min(number_of_reviews), max = max(number_of_reviews)) ## mean variance min max ## 1 29.29801 2322.392 1 629 "],["machine-learning.html", "6 Machine Learning 6.1 Algoritmo Supervisados 6.2 Algoritmo no supervisados", " 6 Machine Learning Machine Learning o tambien conocido como Aprendizaje automático es una rama de la inteligencia artificial, que tiene como objetivo desarrollar técnicas que permitan que las computadoras aprendao ne otras palabaras es la capacidad de una máquina o software para aprender mediante la adaptación de ciertos algoritmos de su programación respecto a cierta entrada de datos en su sistema. Un sistema informático de Machine Lenarning (ML) utiliza la experiencias y evidencias en forma de datos a partir de un gran número de ejemplos de una situación, con los que obtiene por sí mismo patrones o comportamientos. De este modo, se pueden elaborar predicciones de escenarios o iniciar operaciones que son la solución para una tarea específica. El Machine Lenarning (ML) posee dos direrentes tipos el Aprendizaje Supervisado y No Supervisado, que veremos con mas detalle a continuación. 6.1 Algoritmo Supervisados Los modelos de aprendizaje supervisado es una técnica para deducir una función a partir de datos de entrenamiento. En donde el desarrollador actúa como una guía para enseñar al algoritmo las conclusiones a las que debe llegar, es decir la salida del algoritmo ya es conocida. Requiere que los posibles resultados del algoritmo ya sean conocidos y que los datos utilizados para entrenar el algoritmo ya estén etiquetados (labels) con las respuestas correctas. Un claro ejemplo es al clasificar correo entrante entre Spam o no. Entre las diversas características que queremos entrenar deberemos incluir si es correo basura o no con un 1 o un 0. Otro ejemplo son al predecir valores numéricos por ejemplo precio de vivienda a partir de sus características (metros cuadrados, nº de habitaciones, incluye calefacción, distancia del centro, etc.) y deberemos incluir el precio que averiguamos en nuestro set de datos. Existe una subcategoría que diferencia entre modelos de clasificación, si la salida es un valor categórico (por ejemplo, una enumeración, o un conjunto finito de clases) , y modelos de regresión, si la salida es un valor de un espacio continuo. En otras palabras cuando la variable sea discreta los llamaremos clasificación. En la siguiente imagen se muestra las diferentes tipos de algoritmos supervisados: {r echo=FALSE, out.width = 953px, out.height=620px,fig.align=center} knitr::include_graphics(static/img/ML1.PNG) 6.1.1 Vecinos mas cercanos(KNN) Vecinos mas cercanos (K-Nearest-Neighbor) es un método que busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean. Por ejemplo Imaginemos que tenemos dos variable como input y como output nos da si es Clase A(naranjo) o Clase B (azul). Esta data es nuestra data de entrenamiento. #{r echo=FALSE, out.width = \"534px\", out.height=\"364px\",fig.align='center'} #knitr::include_graphics(\"static/img/X1.PNG\") # Ahora que ya tenemos nuestra data de entrenamiento empezaremos a usar la data de test. Como queremos predecir la clase, el output, veremos cómo uno de estos datos se vería visualmente y lo pintaremos de color amarillo. A continuación calculamos la distancia entre este punto y los demás datos. #```{r echo=FALSE, out.width = 534px, out.height=364px,fig.align=center} #knitr::include_graphics(static/img/X3.PNG) Hemos trazado solo algunas distancias, pero podríamos hacerlo con todos. Para este ejemplo tomaremos los k = 4 vecinos más cercanos. #```{r echo=FALSE, out.width = &quot;534px&quot;, out.height=&quot;364px&quot;,fig.align=&#39;center&#39;} #knitr::include_graphics(&quot;static/img/X2.PNG&quot;) Notamos que si nos enfocamos solo en los 3 vecinos más cercanos hay más rojos que azules, entonces nuestra predicción será que este punto ha de ser clase R (rojo). Calcular la distancia en un plano cartesiano es relativamente sencillo, solo tenemos variables como input: en el eje x y eje y. Sin embargo, la misma lógica se puede llevar a más variables. 6.1.1.1 Multiples variables como input Ahora veremos lo anterior en el caso de multiples variables con la data data frame iris. El famoso conjunto de datos de Ronald Fisher, que se encuentra incluido en todas las instalaciones de R. El dataset se compone de 150 observaciones de flores de la planta iris . Existen tres tipos de clases de flores iris: virginica, setosa y versicolor(Hay 50 observaciones de cada una). Las variables o atributos que se miden de cada flor son: El tipo de flor como variable categorica. ´ El largo y el ancho del petalo en cm como variables num ´ ericas. ´ El largo y el ancho del sepalo en cm como variables num ´ ericas. #```{r echo=FALSE, out.width = 534px, out.height=364px,fig.align=center} #knitr::include_graphics(static/img/MLA1.PNG) **Requisitos** ```r install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) #Bibliotecas library(datasets) library(tidyverse) library(mlr) library(XML) Una vez instalada las librerias, se procede a llamar la base de datos iris, para luego ver a traves del codigo str() las variables ()que la componen. Analizar el dataset IRIS iris str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Se observa que posee 5 columnas las cuales 4 son mumericas y una de factor que conresponde a la especie , esta posee 3 niveles diferentes. para luego crear un as_tibble llamado iristib con la funcion as_tibble() que convierte un objeto existente, como un marco de datos o una matriz, en un llamado tibble, un marco de datos con clase tbl_df. irisTib &lt;- as_tibble(iris) Luego se realiza una summary() para ver la media, maxiama, minima etc. Estos nosa sirves para ver si extien datos fuera de rango o datos que ensucie la data. summary(irisTib) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Visualizamos la data Veremos los datos con un grafico para ver como se comportan los datos. ggplot(irisTib, aes(Sepal.Width, Sepal.Length, col=Species )) + geom_point() + theme_bw() realizamos un agrupar elementos por colores ggplot(irisTib, aes(Petal.Length, Petal.Width, col=Species)) + geom_point() + theme_bw() Se puede observar que las distintas entre las especies presentan distintas asociaciones. Una forma rápida de visualizarlo es coloreando los puntos según el nivel del factor Species. Ademas los datos presentan una estructura de asociación entre el largo de los sépalos y el de los pétalos es decir que a mayor largo de sépalos, mayor largo de pétalos. Crear el clasificador irisTask &lt;- makeClassifTask(id=&quot;iris&quot;, data = irisTib, target = &quot;Species&quot;) ## Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class ## tbl_df, hence it will be converted. irisTask ## Supervised task: iris ## Type: classif ## Target: Species ## Observations: 150 ## Features: ## numerics factors ordered functionals ## 4 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ## setosa versicolor virginica ## 50 50 50 ## Positive class: NA Entrenamiento y pruebas Un modelo generalmente se entrena en un subconjunto de datos, y el resto de la data se utiliza para evaluar su desempeño. n=getTaskSize(irisTask) n ## [1] 150 Primero botenemos a traves de getTaskSize() el número de observaciones en la tarea. En este caso n = 150. ratio=3/4 Luego llamaremos set.seed() para que estos resultados sean replicables. Esto es para generar numeros aleatorios replicables. set.seed(1234) train = sample(1:n, n*ratio) train ## [1] 28 80 101 111 137 133 144 132 98 103 90 70 79 116 14 126 62 4 143 40 93 122 5 66 135 47 131 123 84 48 108 3 87 41 115 ## [36] 100 72 32 42 43 2 138 54 49 102 56 51 6 107 130 96 106 57 8 26 17 63 97 22 35 117 149 119 86 142 10 55 92 25 88 ## [71] 50 139 20 140 94 71 61 104 109 27 121 60 65 36 150 19 9 134 30 52 95 38 83 141 21 105 113 13 69 110 118 73 16 11 67 ## [106] 91 146 46 129 59 89 64 Revisamos la composión de train y podemos ver que esta compuesto por numeros entermos y son 112 observaciones. str(train) ## int [1:112] 28 80 101 111 137 133 144 132 98 103 ... Para luego calcular la diferencia de conjuntos (no simétricos) de subconjuntos de un espacio de probabilidad. test = setdiff(1:n, train) test ## [1] 1 7 12 15 18 23 24 29 31 33 34 37 39 44 45 53 58 68 74 75 76 77 78 81 82 85 99 112 114 120 124 125 127 128 136 ## [36] 145 147 148 Crear Clasificador Cremos un clasificador que busque los 3 mas cercanos al punto con K =3, esto lo hacemos con makeLearner(\"classif.knn\", k = 3). Se debe Tener en cuenta que classif.knnse llama desde el classpaquete a través de mlr. knn = makeLearner(&quot;classif.knn&quot;, k=3) knn ## Learner classif.knn from package class ## Type: classif ## Name: k-Nearest Neighbor; Short name: knn ## Class: classif.knn ## Properties: twoclass,multiclass,numerics ## Predict-Type: response ## Hyperparameters: k=3 Luego creamos el modelo con el dataset de entrenamiento con train() primero colocamos en clasificador creado knn, luego usamos iristask y por ultimo train que fue creado anteriormente. mod = train(knn, irisTask, subset = train) mod ## Model for learner.id=classif.knn; learner.class=classif.knn ## Trained on: task.id = iris; obs = 112; features = 4 ## Hyperparameters: k=3 Podemos ver que tenemos 112 observaciones con 4 variables con un valor de k=3. Predicciones preds = predict(mod, irisTask, subset = test) preds ## Prediction: 38 observations ## predict.type: response ## threshold: ## time: 0.00 ## id truth response ## 1 1 setosa setosa ## 7 7 setosa setosa ## 12 12 setosa setosa ## 15 15 setosa setosa ## 18 18 setosa setosa ## 23 23 setosa setosa ## ... (#rows: 38, #cols: 3) Evaluemos las predicciones Se calcula varias medidas de rendimiento a la vez como : * acc: define la precisión general como la probabilidad de correspondencia entre una decisión positiva y una condición verdadera (es decir, la proporción de decisiones de clasificación correctas o de casos). * mmce(Error medio de clasificación errónea) https://mlr.mlr-org.com/articles/tutorial/measures.html ms = list(&quot;mmce&quot; = mmce, &quot;acc&quot; = acc, &quot;timetrain&quot; = timetrain) performance() Mide la calidad de una predicción y esta compuesta por las siguientes codigos; pred como el objeto de predicción; measures es la Medida(s) de rendimiento a evaluar, es decir el valor predeterminado es la medida predeterminada para la tarea; task es la tarea de aprendizaje, podría solicitarse por medida de rendimiento, por lo general no es necesario, excepto para la agrupación en clústeres o la supervivencia; model Modelo basado en datos de entrenamiento, podría solicitarse por medida de rendimiento, por lo general no es necesario, excepto para la supervivencia, performance(preds, measures = ms, task = irisTask, mod) ## mmce acc timetrain ## 0.1052632 0.8947368 0.0000000 con un error de clasificación errónea media de 0.1052632 (el mas bajo)y acc de 0.8947368. Matriz de confusión Además, podemos colocar un resumen en una tabla, también conocida como matriz de confusión, para ver cuántos valores predichos fueron iguales a los reales utilizando la función con la funcion calculateConfusionMatrix. Esto nos nos permiten la visualización del rendimiento de un algoritmo, generalmente uno de aprendizaje supervisado (en el aprendizaje no supervisado generalmente se denomina matriz coincidente). Cada fila de la matriz representa las instancias en una clase predicha, mientras que cada columna representa las instancias en una clase real (o viceversa). El nombre se deriva del hecho de que hace que sea fácil ver si el sistema confunde dos clases (es decir, comúnmente etiquetar incorrectamente una como otra). #Matriz de confusiÃ³n calculateConfusionMatrix(preds, relative = FALSE) ## predicted ## true setosa versicolor virginica -err.- ## setosa 15 0 0 0 ## versicolor 0 11 1 1 ## virginica 0 3 8 3 ## -err.- 0 3 1 4 Intepretemos celda a celda este resultado: El modelo kNN predijo 15 valores como especie setosa y resulta que en nuestro test el valor real, output era también setosa. Ademas el modelo predijo 12 como especie versicolor. Sin embargo, en la data real-test, de esos 12, solo 11 son versicolor y 1 son virginica. El modelo predijo 12 como especie virginica. Sin embargo, en la data real-test, de esos 16, solo 8 son virginica y 3 versicolor. 6.1.1.1.1 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos Soporta classif.kknn kknn x x prob, dos clase, clase multiclase classif.knn clase x dos clase, multiclase 6.1.2 Regresiones logistica requisitos install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) library(mlr) library(tidyverse) Instalar datos del titanic Quieres saber si los factores socioeconómicos influyó en la probabilidad de una persona de sobrevivir al desastre. Por lo tanto el objetivo es construir un modelo de regresión logística binomial para predecir si un pasajero sobreviviría al desastre del Titanic, según datos como su género y cómo Cuánto pagaron por su boleto. Ahora instalamos la base de datos del titanic a continuación. install.packages(&quot;titanic&quot;) data(titanic_train, package = &quot;titanic&quot;) Ahora carguemos los datos, que están integrados en el paquete titanic, combertiremos la base con la función as_tibble (). titanicTib &lt;- as_tibble(titanic_train) Ahora veremos la visualizacion de la data titanic. Tenemos un tibble que contiene 891 casos (observaciones) y que contiene 12 variables. El objetivo de realizar este modelo es para predecir si un pasajero sobreviviría al desastre utilizando la información en estas variables. str(titanicTib) ## tibble [891 x 12] (S3: tbl_df/tbl/data.frame) ## $ PassengerId: int [1:891] 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : int [1:891] 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : int [1:891] 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : chr [1:891] &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ... ## $ Sex : chr [1:891] &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Age : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ... ## $ SibSp : int [1:891] 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int [1:891] 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : chr [1:891] &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ... ## $ Fare : num [1:891] 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr [1:891] &quot;&quot; &quot;C85&quot; &quot;&quot; &quot;C123&quot; ... ## $ Embarked : chr [1:891] &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ... la base contiene las siguientes variables: Variable descripción PassengerId un número arbitrario único para cada pasajero. Sobrevivido un número entero que denota supervivencia (1 = sobrevivió, 0 = murió) Pclass si el pasajero estaba alojado en primera, segunda o tercera clase Nombre un vector de caracteres de los nombres de los pasajeros Sexo un vector de caracteres que contiene masculino y femenino Edad la edad del pasajero SibSp el número combinado de hermanos y cónyuges a bordo Parch el número combinado de padres e hijos a bordo Boleto un vector de caracteres con el número de boleto de cada pasajero arifa la cantidad de dinero que cada pasajero pagó por su boleto Cabina un vector de caracteres del número de cabina de cada pasajero Embarked un vector de caracteres del que los pasajeros del puerto se embarcaron Convertir datos a factores Ahora necesitamos limpiarlo antes de que podamos pasarlo al algoritmo de regresión logística. Lo primero que haremos es cambiar la variables Survived, Sex y Pclass a factor como se muestra a continuación. fctrs &lt;- c(&quot;Survived&quot;, &quot;Sex&quot;, &quot;Pclass&quot;) Para convertirlas usaremos función mutate_at(), que nos permite mutar varios factores a la vez. Suministramos las variables existentes como vector de caracteres al argumento .vars que en este caso es la anterior base creada con las columnas que se creo anteriormente llamada fctrsy para luego utilizar el argumento .funs para que las trasforme a factor las comunas. Posteriormente utilizamos la función mutate() para poder crear una nueva variable llamada FamSize que seria la suma SibSp (el número combinado de hermanos y cónyuges a bordo) y Parch (el número combinado de padres e hijos a bordo). Para finalizar con la función select() el cual nos ayuda a seleccionar solo las variables que creemos pueden tener algún valor predictivo para nuestro modelo, las cuales son las variables Survived, Pclass, Sex, Age, Fare y FamSize. titanicClean &lt;- titanicTib %&gt;% mutate_at(.vars = fctrs, .funs = factor) %&gt;% mutate(FamSize = SibSp + Parch) %&gt;% select(Survived, Pclass, Sex, Age, Fare, FamSize) Ahora visualizamos el cambio realizado anteriormente. Y utlizamos str() para ver como estan constituidas las variables de la data, y como se observas las variables cambiaron a factor. str(titanicClean) ## tibble [891 x 6] (S3: tbl_df/tbl/data.frame) ## $ Survived: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 1 1 1 2 2 ... ## $ Pclass : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ Age : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ... ## $ Fare : num [1:891] 7.25 71.28 7.92 53.1 8.05 ... ## $ FamSize : int [1:891] 1 1 0 1 0 0 0 4 2 1 ... Ademas tenemos nuestra nueva variable,FamSize y hemos eliminado las variables irrelevantes. Graficamos algunos datos Ahora para poder graficar usaremos los siguentes codigos. titanicUntidy &lt;- gather(titanicClean, key = &quot;Variable&quot;, value = &quot;Value&quot;, -Survived) ## Warning: attributes are not identical across measure variables; ## they will be dropped titanicUntidy %&gt;% filter(Variable != &quot;Pclass&quot; &amp; Variable != &quot;Sex&quot;) %&gt;% ggplot(aes(Survived, as.numeric(Value))) + facet_wrap(~ Variable, scales = &quot;free_y&quot;) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + theme_bw() ## Warning: Removed 177 rows containing non-finite values (stat_ydensity). ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties)): collapsing to unique &#39;x&#39; values titanicUntidy %&gt;% filter(Variable == &quot;Pclass&quot; | Variable == &quot;Sex&quot;) %&gt;% ggplot(aes(Value, fill = Survived)) + facet_wrap(~ Variable, scales = &quot;free_x&quot;) + geom_bar(position = &quot;fill&quot;) + theme_bw() imp &lt;- impute(titanicClean, cols = list(Age = imputeMean())) titanicTask &lt;- makeClassifTask(data = imp$data, target = &quot;Survived&quot;) imp$data$Age ## [1] 22.00000 38.00000 26.00000 35.00000 35.00000 29.69912 54.00000 2.00000 27.00000 14.00000 4.00000 58.00000 20.00000 39.00000 14.00000 ## [16] 55.00000 2.00000 29.69912 31.00000 29.69912 35.00000 34.00000 15.00000 28.00000 8.00000 38.00000 29.69912 19.00000 29.69912 29.69912 ## [31] 40.00000 29.69912 29.69912 66.00000 28.00000 42.00000 29.69912 21.00000 18.00000 14.00000 40.00000 27.00000 29.69912 3.00000 19.00000 ## [46] 29.69912 29.69912 29.69912 29.69912 18.00000 7.00000 21.00000 49.00000 29.00000 65.00000 29.69912 21.00000 28.50000 5.00000 11.00000 ## [61] 22.00000 38.00000 45.00000 4.00000 29.69912 29.69912 29.00000 19.00000 17.00000 26.00000 32.00000 16.00000 21.00000 26.00000 32.00000 ## [76] 25.00000 29.69912 29.69912 0.83000 30.00000 22.00000 29.00000 29.69912 28.00000 17.00000 33.00000 16.00000 29.69912 23.00000 24.00000 ## [91] 29.00000 20.00000 46.00000 26.00000 59.00000 29.69912 71.00000 23.00000 34.00000 34.00000 28.00000 29.69912 21.00000 33.00000 37.00000 ## [106] 28.00000 21.00000 29.69912 38.00000 29.69912 47.00000 14.50000 22.00000 20.00000 17.00000 21.00000 70.50000 29.00000 24.00000 2.00000 ## [121] 21.00000 29.69912 32.50000 32.50000 54.00000 12.00000 29.69912 24.00000 29.69912 45.00000 33.00000 20.00000 47.00000 29.00000 25.00000 ## [136] 23.00000 19.00000 37.00000 16.00000 24.00000 29.69912 22.00000 24.00000 19.00000 18.00000 19.00000 27.00000 9.00000 36.50000 42.00000 ## [151] 51.00000 22.00000 55.50000 40.50000 29.69912 51.00000 16.00000 30.00000 29.69912 29.69912 44.00000 40.00000 26.00000 17.00000 1.00000 ## [166] 9.00000 29.69912 45.00000 29.69912 28.00000 61.00000 4.00000 1.00000 21.00000 56.00000 18.00000 29.69912 50.00000 30.00000 36.00000 ## [181] 29.69912 29.69912 9.00000 1.00000 4.00000 29.69912 29.69912 45.00000 40.00000 36.00000 32.00000 19.00000 19.00000 3.00000 44.00000 ## [196] 58.00000 29.69912 42.00000 29.69912 24.00000 28.00000 29.69912 34.00000 45.50000 18.00000 2.00000 32.00000 26.00000 16.00000 40.00000 ## [211] 24.00000 35.00000 22.00000 30.00000 29.69912 31.00000 27.00000 42.00000 32.00000 30.00000 16.00000 27.00000 51.00000 29.69912 38.00000 ## [226] 22.00000 19.00000 20.50000 18.00000 29.69912 35.00000 29.00000 59.00000 5.00000 24.00000 29.69912 44.00000 8.00000 19.00000 33.00000 ## [241] 29.69912 29.69912 29.00000 22.00000 30.00000 44.00000 25.00000 24.00000 37.00000 54.00000 29.69912 29.00000 62.00000 30.00000 41.00000 ## [256] 29.00000 29.69912 30.00000 35.00000 50.00000 29.69912 3.00000 52.00000 40.00000 29.69912 36.00000 16.00000 25.00000 58.00000 35.00000 ## [271] 29.69912 25.00000 41.00000 37.00000 29.69912 63.00000 45.00000 29.69912 7.00000 35.00000 65.00000 28.00000 16.00000 19.00000 29.69912 ## [286] 33.00000 30.00000 22.00000 42.00000 22.00000 26.00000 19.00000 36.00000 24.00000 24.00000 29.69912 23.50000 2.00000 29.69912 50.00000 ## [301] 29.69912 29.69912 19.00000 29.69912 29.69912 0.92000 29.69912 17.00000 30.00000 30.00000 24.00000 18.00000 26.00000 28.00000 43.00000 ## [316] 26.00000 24.00000 54.00000 31.00000 40.00000 22.00000 27.00000 30.00000 22.00000 29.69912 36.00000 61.00000 36.00000 31.00000 16.00000 ## [331] 29.69912 45.50000 38.00000 16.00000 29.69912 29.69912 29.00000 41.00000 45.00000 45.00000 2.00000 24.00000 28.00000 25.00000 36.00000 ## [346] 24.00000 40.00000 29.69912 3.00000 42.00000 23.00000 29.69912 15.00000 25.00000 29.69912 28.00000 22.00000 38.00000 29.69912 29.69912 ## [361] 40.00000 29.00000 45.00000 35.00000 29.69912 30.00000 60.00000 29.69912 29.69912 24.00000 25.00000 18.00000 19.00000 22.00000 3.00000 ## [376] 29.69912 22.00000 27.00000 20.00000 19.00000 42.00000 1.00000 32.00000 35.00000 29.69912 18.00000 1.00000 36.00000 29.69912 17.00000 ## [391] 36.00000 21.00000 28.00000 23.00000 24.00000 22.00000 31.00000 46.00000 23.00000 28.00000 39.00000 26.00000 21.00000 28.00000 20.00000 ## [406] 34.00000 51.00000 3.00000 21.00000 29.69912 29.69912 29.69912 33.00000 29.69912 44.00000 29.69912 34.00000 18.00000 30.00000 10.00000 ## [421] 29.69912 21.00000 29.00000 28.00000 18.00000 29.69912 28.00000 19.00000 29.69912 32.00000 28.00000 29.69912 42.00000 17.00000 50.00000 ## [436] 14.00000 21.00000 24.00000 64.00000 31.00000 45.00000 20.00000 25.00000 28.00000 29.69912 4.00000 13.00000 34.00000 5.00000 52.00000 ## [451] 36.00000 29.69912 30.00000 49.00000 29.69912 29.00000 65.00000 29.69912 50.00000 29.69912 48.00000 34.00000 47.00000 48.00000 29.69912 ## [466] 38.00000 29.69912 56.00000 29.69912 0.75000 29.69912 38.00000 33.00000 23.00000 22.00000 29.69912 34.00000 29.00000 22.00000 2.00000 ## [481] 9.00000 29.69912 50.00000 63.00000 25.00000 29.69912 35.00000 58.00000 30.00000 9.00000 29.69912 21.00000 55.00000 71.00000 21.00000 ## [496] 29.69912 54.00000 29.69912 25.00000 24.00000 17.00000 21.00000 29.69912 37.00000 16.00000 18.00000 33.00000 29.69912 28.00000 26.00000 ## [511] 29.00000 29.69912 36.00000 54.00000 24.00000 47.00000 34.00000 29.69912 36.00000 32.00000 30.00000 22.00000 29.69912 44.00000 29.69912 ## [526] 40.50000 50.00000 29.69912 39.00000 23.00000 2.00000 29.69912 17.00000 29.69912 30.00000 7.00000 45.00000 30.00000 29.69912 22.00000 ## [541] 36.00000 9.00000 11.00000 32.00000 50.00000 64.00000 19.00000 29.69912 33.00000 8.00000 17.00000 27.00000 29.69912 22.00000 22.00000 ## [556] 62.00000 48.00000 29.69912 39.00000 36.00000 29.69912 40.00000 28.00000 29.69912 29.69912 24.00000 19.00000 29.00000 29.69912 32.00000 ## [571] 62.00000 53.00000 36.00000 29.69912 16.00000 19.00000 34.00000 39.00000 29.69912 32.00000 25.00000 39.00000 54.00000 36.00000 29.69912 ## [586] 18.00000 47.00000 60.00000 22.00000 29.69912 35.00000 52.00000 47.00000 29.69912 37.00000 36.00000 29.69912 49.00000 29.69912 49.00000 ## [601] 24.00000 29.69912 29.69912 44.00000 35.00000 36.00000 30.00000 27.00000 22.00000 40.00000 39.00000 29.69912 29.69912 29.69912 35.00000 ## [616] 24.00000 34.00000 26.00000 4.00000 26.00000 27.00000 42.00000 20.00000 21.00000 21.00000 61.00000 57.00000 21.00000 26.00000 29.69912 ## [631] 80.00000 51.00000 32.00000 29.69912 9.00000 28.00000 32.00000 31.00000 41.00000 29.69912 20.00000 24.00000 2.00000 29.69912 0.75000 ## [646] 48.00000 19.00000 56.00000 29.69912 23.00000 29.69912 18.00000 21.00000 29.69912 18.00000 24.00000 29.69912 32.00000 23.00000 58.00000 ## [661] 50.00000 40.00000 47.00000 36.00000 20.00000 32.00000 25.00000 29.69912 43.00000 29.69912 40.00000 31.00000 70.00000 31.00000 29.69912 ## [676] 18.00000 24.50000 18.00000 43.00000 36.00000 29.69912 27.00000 20.00000 14.00000 60.00000 25.00000 14.00000 19.00000 18.00000 15.00000 ## [691] 31.00000 4.00000 29.69912 25.00000 60.00000 52.00000 44.00000 29.69912 49.00000 42.00000 18.00000 35.00000 18.00000 25.00000 26.00000 ## [706] 39.00000 45.00000 42.00000 22.00000 29.69912 24.00000 29.69912 48.00000 29.00000 52.00000 19.00000 38.00000 27.00000 29.69912 33.00000 ## [721] 6.00000 17.00000 34.00000 50.00000 27.00000 20.00000 30.00000 29.69912 25.00000 25.00000 29.00000 11.00000 29.69912 23.00000 23.00000 ## [736] 28.50000 48.00000 35.00000 29.69912 29.69912 29.69912 36.00000 21.00000 24.00000 31.00000 70.00000 16.00000 30.00000 19.00000 31.00000 ## [751] 4.00000 6.00000 33.00000 23.00000 48.00000 0.67000 28.00000 18.00000 34.00000 33.00000 29.69912 41.00000 20.00000 36.00000 16.00000 ## [766] 51.00000 29.69912 30.50000 29.69912 32.00000 24.00000 48.00000 57.00000 29.69912 54.00000 18.00000 29.69912 5.00000 29.69912 43.00000 ## [781] 13.00000 17.00000 29.00000 29.69912 25.00000 25.00000 18.00000 8.00000 1.00000 46.00000 29.69912 16.00000 29.69912 29.69912 25.00000 ## [796] 39.00000 49.00000 31.00000 30.00000 30.00000 34.00000 31.00000 11.00000 0.42000 27.00000 31.00000 39.00000 18.00000 39.00000 33.00000 ## [811] 26.00000 39.00000 35.00000 6.00000 30.50000 29.69912 23.00000 31.00000 43.00000 10.00000 52.00000 27.00000 38.00000 27.00000 2.00000 ## [826] 29.69912 29.69912 1.00000 29.69912 62.00000 15.00000 0.83000 29.69912 23.00000 18.00000 39.00000 21.00000 29.69912 32.00000 29.69912 ## [841] 20.00000 16.00000 30.00000 34.50000 17.00000 42.00000 29.69912 35.00000 28.00000 29.69912 4.00000 74.00000 9.00000 16.00000 44.00000 ## [856] 18.00000 45.00000 51.00000 24.00000 29.69912 41.00000 21.00000 48.00000 29.69912 24.00000 42.00000 27.00000 31.00000 29.69912 4.00000 ## [871] 26.00000 47.00000 33.00000 47.00000 28.00000 15.00000 20.00000 19.00000 29.69912 56.00000 25.00000 33.00000 22.00000 28.00000 25.00000 ## [886] 39.00000 27.00000 19.00000 29.69912 26.00000 32.00000 titanicTask ## Supervised task: imp$data ## Type: classif ## Target: Survived ## Observations: 891 ## Features: ## numerics factors ordered functionals ## 3 2 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ## 0 1 ## 549 342 ## Positive class: 0 podemos observar que hay 5 variables de las cuales 3 son muericas y dos de factor, tambien podemos observar que la data no presenta ningun dato vacio o perdidido.A continuación se crea el modelo logistico, la base de data la llamaremos logReg . logReg &lt;- makeLearner(&quot;classif.logreg&quot;, predict.type = &quot;prob&quot;) Entrenamos el modelo Para extraer los parámetros del modelo, primero debemos convertir nuestro mlr objeto modelo, logRegModel, en un objeto modelo R usando getLearnerModel (). A continuación, pasamos este objeto de modelo R como argumento de la función coef (), que significa coeficientes (otro término para parámetros), por lo que esta función devuelve el parámetros del modelo. logRegModel &lt;- train(logReg, titanicTask) logRegModel ## Model for learner.id=classif.logreg; learner.class=classif.logreg ## Trained on: task.id = imp$data; obs = 891; features = 5 ## Hyperparameters: model=FALSE Podemos ver que la data posee 891 observaciones y 5 vbariables diferentes y que el tipo de clasificador que se utilizara es el de classif.logreg . Validamos la precisión del modelo La función makeImputeWrapper () envuelve a un alumno (dado como primer argumento) y un método de imputación.Usaremos \"classif.logreg\" por como esta constituida la data lo que se explica mas adelante logRegWrapper &lt;- makeImputeWrapper(&quot;classif.logreg&quot;, cols = list(Age = imputeMean())) logRegWrapper ## Learner classif.logreg.imputed from package stats ## Type: classif ## Name: ; Short name: ## Class: ImputeWrapper ## Properties: twoclass,numerics,factors,prob,weights,missings ## Predict-Type: response ## Hyperparameters: model=FALSE kFold &lt;- makeResampleDesc(method = &quot;RepCV&quot;, folds = 10, reps = 50, stratify = TRUE) Ahora apliquemos una validación cruzada estratificada de diez veces, repetida 50 veces, a nuestro paquete aprendiz. Porque estamos proporcionando nuestro alumno empaquetado a la función resample (), para cada pliegue de la validación cruzada, la media de la variable Edad en el conjunto de entrenamiento será utilizado para imputar los valores perdidos. logRegwithImpute &lt;- resample(logRegWrapper, titanicTask, resampling = kFold, measures = list(acc, fpr, fnr)) ## Resampling: repeated cross-validation ## Measures: acc fpr fnr ## [Resample] iter 1: 0.8314607 0.1470588 0.1818182 ## [Resample] iter 2: 0.7977528 0.4117647 0.0727273 ## [Resample] iter 3: 0.7666667 0.2285714 0.2363636 ## [Resample] iter 4: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 5: 0.7666667 0.3428571 0.1636364 ## [Resample] iter 6: 0.8089888 0.3823529 0.0727273 ## [Resample] iter 7: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 8: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 9: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 10: 0.8068182 0.3529412 0.0925926 ## [Resample] iter 11: 0.7888889 0.2857143 0.1636364 ## [Resample] iter 12: 0.7840909 0.3823529 0.1111111 ## [Resample] iter 13: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 14: 0.7977528 0.4117647 0.0727273 ## [Resample] iter 15: 0.8089888 0.1176471 0.2363636 ## [Resample] iter 16: 0.7415730 0.3529412 0.2000000 ## [Resample] iter 17: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 18: 0.8426966 0.2352941 0.1090909 ## [Resample] iter 19: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 20: 0.7444444 0.2857143 0.2363636 ## [Resample] iter 21: 0.7977528 0.3823529 0.0909091 ## [Resample] iter 22: 0.8666667 0.1714286 0.1090909 ## [Resample] iter 23: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 24: 0.7303371 0.2941176 0.2545455 ## [Resample] iter 25: 0.8522727 0.2352941 0.0925926 ## [Resample] iter 26: 0.7078652 0.4411765 0.2000000 ## [Resample] iter 27: 0.7528090 0.4411765 0.1272727 ## [Resample] iter 28: 0.8222222 0.2571429 0.1272727 ## [Resample] iter 29: 0.8426966 0.2058824 0.1272727 ## [Resample] iter 30: 0.8202247 0.2058824 0.1636364 ## [Resample] iter 31: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 32: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 33: 0.8089888 0.2647059 0.1454545 ## [Resample] iter 34: 0.7415730 0.4705882 0.1272727 ## [Resample] iter 35: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 36: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 37: 0.7528090 0.2647059 0.2363636 ## [Resample] iter 38: 0.8111111 0.3428571 0.0909091 ## [Resample] iter 39: 0.8295455 0.3235294 0.0740741 ## [Resample] iter 40: 0.7777778 0.2571429 0.2000000 ## [Resample] iter 41: 0.8409091 0.2352941 0.1111111 ## [Resample] iter 42: 0.7415730 0.4117647 0.1636364 ## [Resample] iter 43: 0.8539326 0.2352941 0.0909091 ## [Resample] iter 44: 0.7666667 0.2285714 0.2363636 ## [Resample] iter 45: 0.7222222 0.4571429 0.1636364 ## [Resample] iter 46: 0.8764045 0.1764706 0.0909091 ## [Resample] iter 47: 0.7752809 0.2352941 0.2181818 ## [Resample] iter 48: 0.8089888 0.3823529 0.0727273 ## [Resample] iter 49: 0.7528090 0.3823529 0.1636364 ## [Resample] iter 50: 0.8764045 0.2352941 0.0545455 ## [Resample] iter 51: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 52: 0.7865169 0.4117647 0.0909091 ## [Resample] iter 53: 0.7777778 0.2857143 0.1818182 ## [Resample] iter 54: 0.8636364 0.2058824 0.0925926 ## [Resample] iter 55: 0.8202247 0.1764706 0.1818182 ## [Resample] iter 56: 0.7640449 0.3823529 0.1454545 ## [Resample] iter 57: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 58: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 59: 0.7528090 0.3529412 0.1818182 ## [Resample] iter 60: 0.7555556 0.3428571 0.1818182 ## [Resample] iter 61: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 62: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 63: 0.8426966 0.2058824 0.1272727 ## [Resample] iter 64: 0.7159091 0.3529412 0.2407407 ## [Resample] iter 65: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 66: 0.8539326 0.2941176 0.0545455 ## [Resample] iter 67: 0.7666667 0.3714286 0.1454545 ## [Resample] iter 68: 0.8089888 0.2647059 0.1454545 ## [Resample] iter 69: 0.7303371 0.3823529 0.2000000 ## [Resample] iter 70: 0.8222222 0.2571429 0.1272727 ## [Resample] iter 71: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 72: 0.8426966 0.2941176 0.0727273 ## [Resample] iter 73: 0.7752809 0.3823529 0.1272727 ## [Resample] iter 74: 0.8068182 0.2647059 0.1481481 ## [Resample] iter 75: 0.7977528 0.2058824 0.2000000 ## [Resample] iter 76: 0.8539326 0.2647059 0.0727273 ## [Resample] iter 77: 0.7888889 0.2571429 0.1818182 ## [Resample] iter 78: 0.7666667 0.3142857 0.1818182 ## [Resample] iter 79: 0.7752809 0.3823529 0.1272727 ## [Resample] iter 80: 0.7528090 0.3235294 0.2000000 ## [Resample] iter 81: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 82: 0.8539326 0.2647059 0.0727273 ## [Resample] iter 83: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 84: 0.7752809 0.3235294 0.1636364 ## [Resample] iter 85: 0.7528090 0.3823529 0.1636364 ## [Resample] iter 86: 0.8111111 0.1714286 0.2000000 ## [Resample] iter 87: 0.7528090 0.3235294 0.2000000 ## [Resample] iter 88: 0.8089888 0.2857143 0.1296296 ## [Resample] iter 89: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 90: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 91: 0.6741573 0.3823529 0.2909091 ## [Resample] iter 92: 0.7159091 0.3235294 0.2592593 ## [Resample] iter 93: 0.7640449 0.3235294 0.1818182 ## [Resample] iter 94: 0.8426966 0.1764706 0.1454545 ## [Resample] iter 95: 0.8111111 0.3142857 0.1090909 ## [Resample] iter 96: 0.8764045 0.2941176 0.0181818 ## [Resample] iter 97: 0.7888889 0.3428571 0.1272727 ## [Resample] iter 98: 0.7977528 0.3529412 0.1090909 ## [Resample] iter 99: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 100: 0.8202247 0.1764706 0.1818182 ## [Resample] iter 101: 0.7752809 0.3235294 0.1636364 ## [Resample] iter 102: 0.7415730 0.3823529 0.1818182 ## [Resample] iter 103: 0.7159091 0.3823529 0.2222222 ## [Resample] iter 104: 0.7865169 0.4117647 0.0909091 ## [Resample] iter 105: 0.8426966 0.3529412 0.0363636 ## [Resample] iter 106: 0.7555556 0.4000000 0.1454545 ## [Resample] iter 107: 0.7977528 0.1764706 0.2181818 ## [Resample] iter 108: 0.8555556 0.1714286 0.1272727 ## [Resample] iter 109: 0.8876404 0.1764706 0.0727273 ## [Resample] iter 110: 0.8426966 0.1764706 0.1454545 ## [Resample] iter 111: 0.7752809 0.2352941 0.2181818 ## [Resample] iter 112: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 113: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 114: 0.7191011 0.4411765 0.1818182 ## [Resample] iter 115: 0.8222222 0.3142857 0.0909091 ## [Resample] iter 116: 0.7303371 0.3823529 0.2000000 ## [Resample] iter 117: 0.8202247 0.2058824 0.1636364 ## [Resample] iter 118: 0.7888889 0.3714286 0.1090909 ## [Resample] iter 119: 0.8202247 0.2352941 0.1454545 ## [Resample] iter 120: 0.8068182 0.3235294 0.1111111 ## [Resample] iter 121: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 122: 0.7752809 0.3235294 0.1636364 ## [Resample] iter 123: 0.8181818 0.3529412 0.0740741 ## [Resample] iter 124: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 125: 0.8426966 0.2941176 0.0727273 ## [Resample] iter 126: 0.8539326 0.2058824 0.1090909 ## [Resample] iter 127: 0.8444444 0.2285714 0.1090909 ## [Resample] iter 128: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 129: 0.7191011 0.4117647 0.2000000 ## [Resample] iter 130: 0.7666667 0.2571429 0.2181818 ## [Resample] iter 131: 0.7415730 0.4117647 0.1636364 ## [Resample] iter 132: 0.7191011 0.3823529 0.2181818 ## [Resample] iter 133: 0.7752809 0.4117647 0.1090909 ## [Resample] iter 134: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 135: 0.8539326 0.2941176 0.0545455 ## [Resample] iter 136: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 137: 0.7888889 0.2857143 0.1636364 ## [Resample] iter 138: 0.8295455 0.2352941 0.1296296 ## [Resample] iter 139: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 140: 0.8111111 0.2857143 0.1272727 ## [Resample] iter 141: 0.8314607 0.2941176 0.0909091 ## [Resample] iter 142: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 143: 0.8333333 0.2857143 0.0909091 ## [Resample] iter 144: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 145: 0.8089888 0.2352941 0.1636364 ## [Resample] iter 146: 0.7727273 0.3235294 0.1666667 ## [Resample] iter 147: 0.7777778 0.2571429 0.2000000 ## [Resample] iter 148: 0.8314607 0.3235294 0.0727273 ## [Resample] iter 149: 0.7865169 0.3235294 0.1454545 ## [Resample] iter 150: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 151: 0.7528090 0.2941176 0.2181818 ## [Resample] iter 152: 0.6966292 0.4705882 0.2000000 ## [Resample] iter 153: 0.7888889 0.3714286 0.1090909 ## [Resample] iter 154: 0.7415730 0.5882353 0.0545455 ## [Resample] iter 155: 0.8333333 0.2285714 0.1272727 ## [Resample] iter 156: 0.7977528 0.2058824 0.2000000 ## [Resample] iter 157: 0.8426966 0.2352941 0.1090909 ## [Resample] iter 158: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 159: 0.9090909 0.1470588 0.0555556 ## [Resample] iter 160: 0.8089888 0.1470588 0.2181818 ## [Resample] iter 161: 0.7865169 0.3823529 0.1090909 ## [Resample] iter 162: 0.7303371 0.3823529 0.2000000 ## [Resample] iter 163: 0.7752809 0.2941176 0.1818182 ## [Resample] iter 164: 0.8444444 0.2571429 0.0909091 ## [Resample] iter 165: 0.8089888 0.3529412 0.0909091 ## [Resample] iter 166: 0.8333333 0.1714286 0.1636364 ## [Resample] iter 167: 0.8426966 0.2352941 0.1090909 ## [Resample] iter 168: 0.7727273 0.3529412 0.1481481 ## [Resample] iter 169: 0.7752809 0.3235294 0.1636364 ## [Resample] iter 170: 0.8202247 0.2352941 0.1454545 ## [Resample] iter 171: 0.8111111 0.2285714 0.1636364 ## [Resample] iter 172: 0.7840909 0.1470588 0.2592593 ## [Resample] iter 173: 0.7415730 0.2352941 0.2727273 ## [Resample] iter 174: 0.7640449 0.4117647 0.1272727 ## [Resample] iter 175: 0.8111111 0.3142857 0.1090909 ## [Resample] iter 176: 0.7640449 0.3823529 0.1454545 ## [Resample] iter 177: 0.7640449 0.2647059 0.2181818 ## [Resample] iter 178: 0.8539326 0.3235294 0.0363636 ## [Resample] iter 179: 0.8202247 0.3823529 0.0545455 ## [Resample] iter 180: 0.8202247 0.2352941 0.1454545 ## [Resample] iter 181: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 182: 0.8444444 0.2000000 0.1272727 ## [Resample] iter 183: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 184: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 185: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 186: 0.7444444 0.3428571 0.2000000 ## [Resample] iter 187: 0.7415730 0.4117647 0.1636364 ## [Resample] iter 188: 0.8636364 0.2058824 0.0925926 ## [Resample] iter 189: 0.7415730 0.4411765 0.1454545 ## [Resample] iter 190: 0.7977528 0.2352941 0.1818182 ## [Resample] iter 191: 0.8539326 0.2352941 0.0909091 ## [Resample] iter 192: 0.8409091 0.2352941 0.1111111 ## [Resample] iter 193: 0.7444444 0.3714286 0.1818182 ## [Resample] iter 194: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 195: 0.7415730 0.5294118 0.0909091 ## [Resample] iter 196: 0.7752809 0.2941176 0.1818182 ## [Resample] iter 197: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 198: 0.7777778 0.3142857 0.1636364 ## [Resample] iter 199: 0.7865169 0.2058824 0.2181818 ## [Resample] iter 200: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 201: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 202: 0.7444444 0.2857143 0.2363636 ## [Resample] iter 203: 0.7752809 0.3823529 0.1272727 ## [Resample] iter 204: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 205: 0.7954545 0.2941176 0.1481481 ## [Resample] iter 206: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 207: 0.8888889 0.2000000 0.0545455 ## [Resample] iter 208: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 209: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 210: 0.7865169 0.3235294 0.1454545 ## [Resample] iter 211: 0.7752809 0.2352941 0.2181818 ## [Resample] iter 212: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 213: 0.8314607 0.2058824 0.1454545 ## [Resample] iter 214: 0.8426966 0.3235294 0.0545455 ## [Resample] iter 215: 0.8000000 0.3142857 0.1272727 ## [Resample] iter 216: 0.7840909 0.3235294 0.1481481 ## [Resample] iter 217: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 218: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 219: 0.8222222 0.2571429 0.1272727 ## [Resample] iter 220: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 221: 0.8636364 0.2352941 0.0740741 ## [Resample] iter 222: 0.8666667 0.0857143 0.1636364 ## [Resample] iter 223: 0.8426966 0.2352941 0.1090909 ## [Resample] iter 224: 0.7415730 0.3235294 0.2181818 ## [Resample] iter 225: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 226: 0.7752809 0.3823529 0.1272727 ## [Resample] iter 227: 0.7666667 0.3142857 0.1818182 ## [Resample] iter 228: 0.7977528 0.2352941 0.1818182 ## [Resample] iter 229: 0.6853933 0.4411765 0.2363636 ## [Resample] iter 230: 0.7640449 0.5294118 0.0545455 ## [Resample] iter 231: 0.7666667 0.3714286 0.1454545 ## [Resample] iter 232: 0.7640449 0.3235294 0.1818182 ## [Resample] iter 233: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 234: 0.8426966 0.3235294 0.0545455 ## [Resample] iter 235: 0.8314607 0.1470588 0.1818182 ## [Resample] iter 236: 0.8222222 0.2857143 0.1090909 ## [Resample] iter 237: 0.8522727 0.1764706 0.1296296 ## [Resample] iter 238: 0.7303371 0.3823529 0.2000000 ## [Resample] iter 239: 0.8202247 0.2352941 0.1454545 ## [Resample] iter 240: 0.8426966 0.3235294 0.0545455 ## [Resample] iter 241: 0.8764045 0.1176471 0.1272727 ## [Resample] iter 242: 0.8539326 0.1470588 0.1454545 ## [Resample] iter 243: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 244: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 245: 0.7415730 0.4117647 0.1636364 ## [Resample] iter 246: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 247: 0.7555556 0.4000000 0.1454545 ## [Resample] iter 248: 0.7640449 0.4571429 0.0925926 ## [Resample] iter 249: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 250: 0.7528090 0.3529412 0.1818182 ## [Resample] iter 251: 0.7191011 0.3235294 0.2545455 ## [Resample] iter 252: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 253: 0.7191011 0.3235294 0.2545455 ## [Resample] iter 254: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 255: 0.8333333 0.2571429 0.1090909 ## [Resample] iter 256: 0.8202247 0.3142857 0.0925926 ## [Resample] iter 257: 0.7752809 0.4117647 0.1090909 ## [Resample] iter 258: 0.8539326 0.1764706 0.1272727 ## [Resample] iter 259: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 260: 0.7752809 0.3235294 0.1636364 ## [Resample] iter 261: 0.8000000 0.3428571 0.1090909 ## [Resample] iter 262: 0.7977528 0.3529412 0.1090909 ## [Resample] iter 263: 0.7666667 0.2857143 0.2000000 ## [Resample] iter 264: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 265: 0.7078652 0.3235294 0.2727273 ## [Resample] iter 266: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 267: 0.8089888 0.2352941 0.1636364 ## [Resample] iter 268: 0.7840909 0.2941176 0.1666667 ## [Resample] iter 269: 0.8539326 0.2058824 0.1090909 ## [Resample] iter 270: 0.7977528 0.3823529 0.0909091 ## [Resample] iter 271: 0.8202247 0.2352941 0.1454545 ## [Resample] iter 272: 0.7727273 0.3823529 0.1296296 ## [Resample] iter 273: 0.8202247 0.2058824 0.1636364 ## [Resample] iter 274: 0.8089888 0.2058824 0.1818182 ## [Resample] iter 275: 0.7415730 0.3529412 0.2000000 ## [Resample] iter 276: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 277: 0.8111111 0.2857143 0.1272727 ## [Resample] iter 278: 0.8555556 0.2857143 0.0545455 ## [Resample] iter 279: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 280: 0.7303371 0.4117647 0.1818182 ## [Resample] iter 281: 0.7888889 0.3142857 0.1454545 ## [Resample] iter 282: 0.7777778 0.3714286 0.1272727 ## [Resample] iter 283: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 284: 0.8314607 0.3235294 0.0727273 ## [Resample] iter 285: 0.7303371 0.4411765 0.1636364 ## [Resample] iter 286: 0.8409091 0.2352941 0.1111111 ## [Resample] iter 287: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 288: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 289: 0.7977528 0.2058824 0.2000000 ## [Resample] iter 290: 0.7865169 0.2647059 0.1818182 ## [Resample] iter 291: 0.7415730 0.2352941 0.2727273 ## [Resample] iter 292: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 293: 0.7865169 0.2647059 0.1818182 ## [Resample] iter 294: 0.8555556 0.2285714 0.0909091 ## [Resample] iter 295: 0.7865169 0.3235294 0.1454545 ## [Resample] iter 296: 0.8314607 0.3235294 0.0727273 ## [Resample] iter 297: 0.8314607 0.1764706 0.1636364 ## [Resample] iter 298: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 299: 0.7977528 0.3529412 0.1090909 ## [Resample] iter 300: 0.8089888 0.3428571 0.0925926 ## [Resample] iter 301: 0.7865169 0.2647059 0.1818182 ## [Resample] iter 302: 0.8202247 0.3235294 0.0909091 ## [Resample] iter 303: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 304: 0.8111111 0.3714286 0.0727273 ## [Resample] iter 305: 0.7640449 0.2647059 0.2181818 ## [Resample] iter 306: 0.7727273 0.4117647 0.1111111 ## [Resample] iter 307: 0.8000000 0.2285714 0.1818182 ## [Resample] iter 308: 0.7865169 0.3235294 0.1454545 ## [Resample] iter 309: 0.8089888 0.2058824 0.1818182 ## [Resample] iter 310: 0.8539326 0.2941176 0.0545455 ## [Resample] iter 311: 0.8750000 0.2352941 0.0555556 ## [Resample] iter 312: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 313: 0.8764045 0.2352941 0.0545455 ## [Resample] iter 314: 0.8333333 0.2000000 0.1454545 ## [Resample] iter 315: 0.7555556 0.4571429 0.1090909 ## [Resample] iter 316: 0.8314607 0.1764706 0.1636364 ## [Resample] iter 317: 0.7528090 0.3529412 0.1818182 ## [Resample] iter 318: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 319: 0.7303371 0.4117647 0.1818182 ## [Resample] iter 320: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 321: 0.8000000 0.3428571 0.1090909 ## [Resample] iter 322: 0.7954545 0.2941176 0.1481481 ## [Resample] iter 323: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 324: 0.8089888 0.2647059 0.1454545 ## [Resample] iter 325: 0.8333333 0.2285714 0.1272727 ## [Resample] iter 326: 0.7191011 0.2647059 0.2909091 ## [Resample] iter 327: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 328: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 329: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 330: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 331: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 332: 0.7640449 0.3235294 0.1818182 ## [Resample] iter 333: 0.8181818 0.3823529 0.0555556 ## [Resample] iter 334: 0.8539326 0.1470588 0.1454545 ## [Resample] iter 335: 0.7415730 0.4411765 0.1454545 ## [Resample] iter 336: 0.7222222 0.2571429 0.2909091 ## [Resample] iter 337: 0.8222222 0.2571429 0.1272727 ## [Resample] iter 338: 0.8089888 0.3529412 0.0909091 ## [Resample] iter 339: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 340: 0.7865169 0.3529412 0.1272727 ## [Resample] iter 341: 0.8202247 0.3529412 0.0727273 ## [Resample] iter 342: 0.7666667 0.3142857 0.1818182 ## [Resample] iter 343: 0.8089888 0.2285714 0.1666667 ## [Resample] iter 344: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 345: 0.7415730 0.2941176 0.2363636 ## [Resample] iter 346: 0.7865169 0.3823529 0.1090909 ## [Resample] iter 347: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 348: 0.7752809 0.1764706 0.2545455 ## [Resample] iter 349: 0.8089888 0.3823529 0.0727273 ## [Resample] iter 350: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 351: 0.7977528 0.2058824 0.2000000 ## [Resample] iter 352: 0.8202247 0.2941176 0.1090909 ## [Resample] iter 353: 0.7555556 0.3428571 0.1818182 ## [Resample] iter 354: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 355: 0.7303371 0.4117647 0.1818182 ## [Resample] iter 356: 0.7191011 0.4411765 0.1818182 ## [Resample] iter 357: 0.8522727 0.2058824 0.1111111 ## [Resample] iter 358: 0.8651685 0.2352941 0.0727273 ## [Resample] iter 359: 0.8222222 0.3142857 0.0909091 ## [Resample] iter 360: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 361: 0.8539326 0.2058824 0.1090909 ## [Resample] iter 362: 0.8666667 0.2285714 0.0727273 ## [Resample] iter 363: 0.7555556 0.3428571 0.1818182 ## [Resample] iter 364: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 365: 0.7528090 0.3823529 0.1636364 ## [Resample] iter 366: 0.8295455 0.2647059 0.1111111 ## [Resample] iter 367: 0.8651685 0.2941176 0.0363636 ## [Resample] iter 368: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 369: 0.7415730 0.2647059 0.2545455 ## [Resample] iter 370: 0.7078652 0.3529412 0.2545455 ## [Resample] iter 371: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 372: 0.7666667 0.3142857 0.1818182 ## [Resample] iter 373: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 374: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 375: 0.7954545 0.2647059 0.1666667 ## [Resample] iter 376: 0.8651685 0.2941176 0.0363636 ## [Resample] iter 377: 0.7666667 0.3714286 0.1454545 ## [Resample] iter 378: 0.8426966 0.1764706 0.1454545 ## [Resample] iter 379: 0.7415730 0.3235294 0.2181818 ## [Resample] iter 380: 0.7977528 0.3823529 0.0909091 ## [Resample] iter 381: 0.7078652 0.4705882 0.1818182 ## [Resample] iter 382: 0.8111111 0.3428571 0.0909091 ## [Resample] iter 383: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 384: 0.7865169 0.2352941 0.2000000 ## [Resample] iter 385: 0.8539326 0.2352941 0.0909091 ## [Resample] iter 386: 0.8000000 0.2857143 0.1454545 ## [Resample] iter 387: 0.7977528 0.2058824 0.2000000 ## [Resample] iter 388: 0.7727273 0.3529412 0.1481481 ## [Resample] iter 389: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 390: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 391: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 392: 0.8426966 0.2647059 0.0909091 ## [Resample] iter 393: 0.7415730 0.4000000 0.1666667 ## [Resample] iter 394: 0.7977528 0.2647059 0.1636364 ## [Resample] iter 395: 0.7528090 0.2941176 0.2181818 ## [Resample] iter 396: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 397: 0.7415730 0.3529412 0.2000000 ## [Resample] iter 398: 0.8314607 0.2352941 0.1272727 ## [Resample] iter 399: 0.8222222 0.2285714 0.1454545 ## [Resample] iter 400: 0.8539326 0.2647059 0.0727273 ## [Resample] iter 401: 0.7613636 0.2647059 0.2222222 ## [Resample] iter 402: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 403: 0.7888889 0.2857143 0.1636364 ## [Resample] iter 404: 0.7752809 0.4411765 0.0909091 ## [Resample] iter 405: 0.8111111 0.3714286 0.0727273 ## [Resample] iter 406: 0.8089888 0.2941176 0.1272727 ## [Resample] iter 407: 0.8202247 0.3235294 0.0909091 ## [Resample] iter 408: 0.8314607 0.1764706 0.1636364 ## [Resample] iter 409: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 410: 0.7640449 0.2647059 0.2181818 ## [Resample] iter 411: 0.8333333 0.2571429 0.1090909 ## [Resample] iter 412: 0.7977528 0.2941176 0.1454545 ## [Resample] iter 413: 0.8089888 0.4117647 0.0545455 ## [Resample] iter 414: 0.7752809 0.2941176 0.1818182 ## [Resample] iter 415: 0.7415730 0.4117647 0.1636364 ## [Resample] iter 416: 0.8539326 0.2058824 0.1090909 ## [Resample] iter 417: 0.7666667 0.4285714 0.1090909 ## [Resample] iter 418: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 419: 0.8426966 0.1764706 0.1454545 ## [Resample] iter 420: 0.7954545 0.2647059 0.1666667 ## [Resample] iter 421: 0.7640449 0.3823529 0.1454545 ## [Resample] iter 422: 0.8202247 0.2058824 0.1636364 ## [Resample] iter 423: 0.7303371 0.2941176 0.2545455 ## [Resample] iter 424: 0.7528090 0.2647059 0.2363636 ## [Resample] iter 425: 0.8202247 0.3529412 0.0727273 ## [Resample] iter 426: 0.8000000 0.3714286 0.0909091 ## [Resample] iter 427: 0.7666667 0.2857143 0.2000000 ## [Resample] iter 428: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 429: 0.8314607 0.3235294 0.0727273 ## [Resample] iter 430: 0.8750000 0.1764706 0.0925926 ## [Resample] iter 431: 0.8089888 0.2647059 0.1454545 ## [Resample] iter 432: 0.8426966 0.2647059 0.0909091 ## [Resample] iter 433: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 434: 0.7640449 0.3529412 0.1636364 ## [Resample] iter 435: 0.8181818 0.2352941 0.1481481 ## [Resample] iter 436: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 437: 0.7977528 0.3529412 0.1090909 ## [Resample] iter 438: 0.7444444 0.5142857 0.0909091 ## [Resample] iter 439: 0.8000000 0.2571429 0.1636364 ## [Resample] iter 440: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 441: 0.8202247 0.2647059 0.1272727 ## [Resample] iter 442: 0.7752809 0.2941176 0.1818182 ## [Resample] iter 443: 0.7415730 0.3714286 0.1851852 ## [Resample] iter 444: 0.7078652 0.2941176 0.2909091 ## [Resample] iter 445: 0.8426966 0.2647059 0.0909091 ## [Resample] iter 446: 0.7752809 0.3529412 0.1454545 ## [Resample] iter 447: 0.8089888 0.2058824 0.1818182 ## [Resample] iter 448: 0.8333333 0.1714286 0.1636364 ## [Resample] iter 449: 0.7865169 0.3823529 0.1090909 ## [Resample] iter 450: 0.8202247 0.3823529 0.0545455 ## [Resample] iter 451: 0.7865169 0.2941176 0.1636364 ## [Resample] iter 452: 0.8444444 0.2571429 0.0909091 ## [Resample] iter 453: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 454: 0.7191011 0.5000000 0.1454545 ## [Resample] iter 455: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 456: 0.7865169 0.2352941 0.2000000 ## [Resample] iter 457: 0.7977528 0.2352941 0.1818182 ## [Resample] iter 458: 0.8333333 0.2285714 0.1272727 ## [Resample] iter 459: 0.8409091 0.2352941 0.1111111 ## [Resample] iter 460: 0.7415730 0.3823529 0.1818182 ## [Resample] iter 461: 0.7977528 0.4411765 0.0545455 ## [Resample] iter 462: 0.8202247 0.2058824 0.1636364 ## [Resample] iter 463: 0.8222222 0.2857143 0.1090909 ## [Resample] iter 464: 0.7977528 0.1764706 0.2181818 ## [Resample] iter 465: 0.7640449 0.3823529 0.1454545 ## [Resample] iter 466: 0.7640449 0.3823529 0.1454545 ## [Resample] iter 467: 0.8314607 0.2647059 0.1090909 ## [Resample] iter 468: 0.7954545 0.3529412 0.1111111 ## [Resample] iter 469: 0.7415730 0.2647059 0.2545455 ## [Resample] iter 470: 0.8000000 0.2857143 0.1454545 ## [Resample] iter 471: 0.8089888 0.1470588 0.2181818 ## [Resample] iter 472: 0.7640449 0.2941176 0.2000000 ## [Resample] iter 473: 0.7752809 0.2352941 0.2181818 ## [Resample] iter 474: 0.8068182 0.3529412 0.0925926 ## [Resample] iter 475: 0.8089888 0.3823529 0.0727273 ## [Resample] iter 476: 0.7444444 0.3428571 0.2000000 ## [Resample] iter 477: 0.8539326 0.2058824 0.1090909 ## [Resample] iter 478: 0.7777778 0.4285714 0.0909091 ## [Resample] iter 479: 0.7415730 0.4705882 0.1272727 ## [Resample] iter 480: 0.8651685 0.1470588 0.1272727 ## [Resample] iter 481: 0.7865169 0.4117647 0.0909091 ## [Resample] iter 482: 0.7640449 0.2285714 0.2407407 ## [Resample] iter 483: 0.8089888 0.3235294 0.1090909 ## [Resample] iter 484: 0.8089888 0.2352941 0.1636364 ## [Resample] iter 485: 0.7752809 0.3823529 0.1272727 ## [Resample] iter 486: 0.7303371 0.5588235 0.0909091 ## [Resample] iter 487: 0.8888889 0.0857143 0.1272727 ## [Resample] iter 488: 0.7528090 0.3529412 0.1818182 ## [Resample] iter 489: 0.7865169 0.2058824 0.2181818 ## [Resample] iter 490: 0.8651685 0.1470588 0.1272727 ## [Resample] iter 491: 0.7078652 0.3529412 0.2545455 ## [Resample] iter 492: 0.7977528 0.3235294 0.1272727 ## [Resample] iter 493: 0.8651685 0.2352941 0.0727273 ## [Resample] iter 494: 0.7752809 0.2941176 0.1818182 ## [Resample] iter 495: 0.7977528 0.3529412 0.1090909 ## [Resample] iter 496: 0.7752809 0.2647059 0.2000000 ## [Resample] iter 497: 0.7303371 0.4411765 0.1636364 ## [Resample] iter 498: 0.8426966 0.2571429 0.0925926 ## [Resample] iter 499: 0.8089888 0.2647059 0.1454545 ## [Resample] iter 500: 0.8777778 0.1428571 0.1090909 ## ## Aggregated Result: acc.test.mean=0.7964598,fpr.test.mean=0.2988017,fnr.test.mean=0.1442040 ## logRegwithImpute ## Resample Result ## Task: imp$data ## Learner: classif.logreg.imputed ## Aggr perf: acc.test.mean=0.7964598,fpr.test.mean=0.2988017,fnr.test.mean=0.1442040 ## Runtime: 8.11083 Como se trata de un problema de clasificación de dos clases, tenemos acceso a algunos resultados adicionales métricas, como la tasa de falsos positivos (fpr) y la tasa de falsos negativos (fnr). Podemos ver que aunque en promedio entre las repeticiones, el modelo clasificó correctamente al 79,6% de los pasajeros, clasificó incorrectamente 29,9% de los pasajeros que murieron por haber sobrevivido (falsos positivos) e incorrectamente clasificaron al 14,4% de los pasajeros que sobrevivieron como muertos (falsos negativos). Extraemos los Odd Ratios Para extraer los parámetros del modelo, primero debemos convertir nuestro mlr objeto modelo, logRegModel, en un objeto modelo R usando getLearnerModel ()A continuación, pasamos este objeto de modelo R como argumento de la función coef (), que significa coeficientes (otro término para parámetros), por lo que esta función devuelve el parámetros del modelo. logRegModelData &lt;- getLearnerModel(logRegModel) coef(logRegModelData) ## (Intercept) Pclass2 Pclass3 Sexmale Age Fare FamSize ## 3.809661697 -1.000344806 -2.132428850 -2.775928255 -0.038822458 0.003218432 -0.243029114 La intersección es el registro de probabilidades de sobrevivir al desastre del Titanic cuando todas las variables continuas son 0 y los factores están en sus niveles de referencia. Tendemos a estar más interesados en las pendientes que la intersección con el eje y, pero estos valores están en unidades logarítmicas de probabilidades, que son difíciles interpretar. En cambio, la gente comúnmente los convierte en razones de probabilidades. Una razón de probabilidades es, bueno, una razón de probabilidades. Por ejemplo, si las probabilidades de sobrevivir al Titanic si eres mujer tienes entre 7 y 10, y las probabilidades de sobrevivir si eres hombre son De 2 a 10, la razón de posibilidades de sobrevivir si eres mujer es de 3,5. En otras palabras, si tu si fuera mujer, habría tenido 3,5 veces más probabilidades de sobrevivir que si fuera masculino. Las razones de probabilidad son una forma muy popular de interpretar el impacto de los predictores en un resultado, porque se entienden fácilmente. Hacemos predicciones sobre nueva data Hemos construido, validado e interpretado nuestro modelo, y ahora sería bueno utilice el modelo para hacer predicciones sobre nuevos datos. Este escenario es un poco inusual en que hemos creado un modelo basado en un evento histórico, por lo que (¡con suerte!) no utilizaremos para predecir la supervivencia de otro desastre del Titanic. Sin embargo, quiero ilustrar cómo hacer predicciones con un modelo de regresión logística, lo mismo que puede para cualquier otro algoritmo supervisado. Carguemos algunos datos de pasajeros sin etiquetar, límpielos listo para la predicción y pasarlo a través de nuestro modelo. data(titanic_test, package = &quot;titanic&quot;) titanicNew &lt;- as_tibble(titanic_test) str(titanicNew) ## tibble [418 x 11] (S3: tbl_df/tbl/data.frame) ## $ PassengerId: int [1:418] 892 893 894 895 896 897 898 899 900 901 ... ## $ Pclass : int [1:418] 3 3 2 3 3 3 3 2 3 3 ... ## $ Name : chr [1:418] &quot;Kelly, Mr. James&quot; &quot;Wilkes, Mrs. James (Ellen Needs)&quot; &quot;Myles, Mr. Thomas Francis&quot; &quot;Wirz, Mr. Albert&quot; ... ## $ Sex : chr [1:418] &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; ... ## $ Age : num [1:418] 34.5 47 62 27 22 14 30 26 18 21 ... ## $ SibSp : int [1:418] 0 1 0 0 1 0 0 1 0 2 ... ## $ Parch : int [1:418] 0 0 0 0 1 0 0 1 0 0 ... ## $ Ticket : chr [1:418] &quot;330911&quot; &quot;363272&quot; &quot;240276&quot; &quot;315154&quot; ... ## $ Fare : num [1:418] 7.83 7 9.69 8.66 12.29 ... ## $ Cabin : chr [1:418] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Embarked : chr [1:418] &quot;Q&quot; &quot;S&quot; &quot;Q&quot; &quot;S&quot; ... titanicNewClean &lt;- titanicNew %&gt;% mutate_at(.vars = c(&quot;Sex&quot;, &quot;Pclass&quot;), .funs = factor) %&gt;% mutate(FamSize = SibSp + Parch) %&gt;% select(Pclass, Sex, Age, Fare, FamSize) predict(logRegModel, newdata = titanicNewClean) ## Warning in predict.WrappedModel(logRegModel, newdata = titanicNewClean): Provided data for prediction is not a pure data.frame but from class ## tbl_df, hence it will be converted. ## Prediction: 418 observations ## predict.type: prob ## threshold: 0=0.50,1=0.50 ## time: 0.00 ## prob.0 prob.1 response ## 1 0.9178036 0.08219636 0 ## 2 0.5909570 0.40904305 0 ## 3 0.9123303 0.08766974 0 ## 4 0.8927383 0.10726167 0 ## 5 0.4069407 0.59305933 1 ## 6 0.8337609 0.16623907 0 ## ... (#rows: 418, #cols: 3) a 6.1.2.1 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos Soporta nota classif.logreg stats x x x prob, dos clase Delegados a con . Establecemos model en FALSE de forma predeterminada para ahorrar memoria. glm family = binomial(link = 'logit') classif. LiblineaRL1LogReg LiblineaR x dos clase, multiclase, prob, class.weights classif. LiblineaRL2LogReg LiblineaR x dos clase, multiclase, prob, class.weights type = 0 (el valor predeterminado) es primario y es doble problema.type = 7 6.1.3 Modelo de máquina de soporte vectorial (SVM) Las Máquinas de Vectores Soporte fue creado por Vladimir Vapnik, es un método basado en aprendizaje para la resolución de problemas de clasificación y regresión. En ambos casos, esta resolución se basa en una primera fase de entrenamiento en donde se les informa con múltiples ejemplos ya resueltos y una segunda fase de uso para la resolución de problemas. En ella, las SVM se convierten en una caja que proporciona una respuesta (salida) a un problema dado (entrada), como lo muestra en la siguiente imagen. Requisitos install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) #Bibliotecas library(datasets) library(tidyverse) library(XML) library(kernlab) library(mlr) library(parallelMap) library(parallel) Cargaremos base de datos Para demostrar las Máquinas de Vectores Soporte usaremos la base de datos spam,la cual es un conjunto de datos recopilados en Hewlett-Packard Labs, que clasifica 4601 correos electrónicos como spam o no spam. Además de esta etiqueta de clase, hay 57 variables que indican la frecuencia de ciertas palabras y caracteres en el correo electrónico. Las primeras 48 variables contienen la frecuencia del nombre de la variable (por ejemplo, empresa) en el correo electrónico. Si el nombre de la variable comienza con num (por ejemplo, num650), indica la frecuencia del número correspondiente (por ejemplo, 650). Las variables 49-54 indican la frecuencia de los caracteres ;, (, [, !,  $ Y  #. Las variables 55-57 contienen el promedio, el más largo y el total largo de las letras mayúsculas La variable 58 indica el tipo de correo y es nonspamo spam, es decir, correo electrónico comercial no solicitado. A continuación llamaremos la base de datos. #Obtenemos los datos data(spam, package = &quot;kernlab&quot;) spam ## Warning in instance$preRenderHook(instance): It seems your data is too big for client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html Pasamos la base spam la convertimos en as_tibble creamos una nuevo data frame con el nombre de spamTib. spamTib &lt;- as_tibble(spam) spamTib ## # A tibble: 4,601 x 58 ## make address all num3d our over remove internet order mail receive will people report addresses free business email you credit ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 ## 2 0.21 0.28 0.5 0 0.14 0.28 0.21 0.07 0 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0 ## 3 0.06 0 0.71 0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0 1.75 0.06 0.06 1.03 1.36 0.32 ## 4 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0 0 0.31 0 0 3.18 0 ## 5 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0 0 0.31 0 0 3.18 0 ## 6 0 0 0 0 1.85 0 0 1.85 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 0 0 1.92 0 0 0 0 0.64 0.96 1.28 0 0 0 0.96 0 0.32 3.85 0 ## 8 0 0 0 0 1.88 0 0 1.88 0 0 0 0 0 0 0 0 0 0 0 0 ## 9 0.15 0 0.46 0 0.61 0 0.3 0 0.92 0.76 0.76 0.92 0 0 0 0 0 0.15 1.23 3.53 ## 10 0.06 0.12 0.77 0 0.19 0.32 0.38 0 0.06 0 0 0.64 0.25 0 0.12 0 0 0.12 1.67 0.06 ## # ... with 4,591 more rows, and 38 more variables: your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;, hp &lt;dbl&gt;, hpl &lt;dbl&gt;, george &lt;dbl&gt;, ## # num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;, telnet &lt;dbl&gt;, num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;, technology &lt;dbl&gt;, num1999 &lt;dbl&gt;, ## # parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;, cs &lt;dbl&gt;, meeting &lt;dbl&gt;, original &lt;dbl&gt;, project &lt;dbl&gt;, re &lt;dbl&gt;, edu &lt;dbl&gt;, table &lt;dbl&gt;, ## # conference &lt;dbl&gt;, charSemicolon &lt;dbl&gt;, charRoundbracket &lt;dbl&gt;, charSquarebracket &lt;dbl&gt;, charExclamation &lt;dbl&gt;, charDollar &lt;dbl&gt;, ## # charHash &lt;dbl&gt;, capitalAve &lt;dbl&gt;, capitalLong &lt;dbl&gt;, capitalTotal &lt;dbl&gt;, type &lt;fct&gt; Ahora veremos las variables y como estan constituidas con str(). str(spamTib) ## tibble [4,601 x 58] (S3: tbl_df/tbl/data.frame) ## $ make : num [1:4601] 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ... ## $ address : num [1:4601] 0.64 0.28 0 0 0 0 0 0 0 0.12 ... ## $ all : num [1:4601] 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ... ## $ num3d : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ our : num [1:4601] 0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ... ## $ over : num [1:4601] 0 0.28 0.19 0 0 0 0 0 0 0.32 ... ## $ remove : num [1:4601] 0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ... ## $ internet : num [1:4601] 0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ... ## $ order : num [1:4601] 0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ... ## $ mail : num [1:4601] 0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ... ## $ receive : num [1:4601] 0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ... ## $ will : num [1:4601] 0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ... ## $ people : num [1:4601] 0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ... ## $ report : num [1:4601] 0 0.21 0 0 0 0 0 0 0 0 ... ## $ addresses : num [1:4601] 0 0.14 1.75 0 0 0 0 0 0 0.12 ... ## $ free : num [1:4601] 0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ... ## $ business : num [1:4601] 0 0.07 0.06 0 0 0 0 0 0 0 ... ## $ email : num [1:4601] 1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ... ## $ you : num [1:4601] 1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ... ## $ credit : num [1:4601] 0 0 0.32 0 0 0 0 0 3.53 0.06 ... ## $ your : num [1:4601] 0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ... ## $ font : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num000 : num [1:4601] 0 0.43 1.16 0 0 0 0 0 0 0.19 ... ## $ money : num [1:4601] 0 0.43 0.06 0 0 0 0 0 0.15 0 ... ## $ hp : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ hpl : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ george : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num650 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ lab : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ labs : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ telnet : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num857 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ data : num [1:4601] 0 0 0 0 0 0 0 0 0.15 0 ... ## $ num415 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num85 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ technology : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num1999 : num [1:4601] 0 0.07 0 0 0 0 0 0 0 0 ... ## $ parts : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ pm : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ direct : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ cs : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ meeting : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ original : num [1:4601] 0 0 0.12 0 0 0 0 0 0.3 0 ... ## $ project : num [1:4601] 0 0 0 0 0 0 0 0 0 0.06 ... ## $ re : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ edu : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ table : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ conference : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ charSemicolon : num [1:4601] 0 0 0.01 0 0 0 0 0 0 0.04 ... ## $ charRoundbracket : num [1:4601] 0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ... ## $ charSquarebracket: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ charExclamation : num [1:4601] 0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ... ## $ charDollar : num [1:4601] 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ... ## $ charHash : num [1:4601] 0 0.048 0.01 0 0 0 0 0 0.022 0 ... ## $ capitalAve : num [1:4601] 3.76 5.11 9.82 3.54 3.54 ... ## $ capitalLong : num [1:4601] 61 101 485 40 40 15 4 11 445 43 ... ## $ capitalTotal : num [1:4601] 278 1028 2259 191 191 ... ## $ type : Factor w/ 2 levels &quot;nonspam&quot;,&quot;spam&quot;: 2 2 2 2 2 2 2 2 2 2 ... Creamos la tarea de clasificación spamTask &lt;- makeClassifTask(data = spamTib, target = &quot;type&quot;) ## Warning in makeTask(type = type, data = data, weights = weights, blocking = blocking, : Provided data is not a pure data.frame but from class ## tbl_df, hence it will be converted. spamTask ## Supervised task: spamTib ## Type: classif ## Target: type ## Observations: 4601 ## Features: ## numerics factors ordered functionals ## 57 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ## nonspam spam ## 2788 1813 ## Positive class: nonspam El conjunto de datos contiene 2788 correos electrónicos clasificados como nonspamy 1813 clasificados como spam. El concepto de spam es diverso: anuncios de productos / sitios web, esquemas para ganar dinero rápidamente, cartas en cadena, pornografía. Creamos el clasificador Vamos a definir la tarea y aprendiz. Esta vez, suministramos classif. SVM como el argumento de makeLearner () para especificar que vamos a utilizar SVM svm &lt;- makeLearner(&quot;classif.svm&quot;) svm ## Learner classif.svm from package e1071 ## Type: classif ## Name: Support Vector Machines (libsvm); Short name: svm ## Class: classif.svm ## Properties: twoclass,multiclass,numerics,factors,prob,class.weights ## Predict-Type: response ## Hyperparameters: Hiperparametros del modelo Antes de entrenar el modelo, es necesario ajustar nuestros hiperparámetros. Para saber qué hiperparámetros se puede utilizar para un algoritmo, se debe usar getParamSet(). El primer argumento utilizado es el nombre de la hiperparámetro propuesta por getParamSet ( classif. svm  ) , entre comillas. getParamSet(&quot;classif.svm&quot;) ## Type len Def Constr Req Tunable Trafo ## type discrete - C-classifica... C-classification,nu-classification - TRUE - ## cost numeric - 1 0 to Inf Y TRUE - ## nu numeric - 0.5 -Inf to Inf Y TRUE - ## class.weights numericvector &lt;NA&gt; - 0 to Inf - TRUE - ## kernel discrete - radial linear,polynomial,radial,sigmoid - TRUE - ## degree integer - 3 1 to Inf Y TRUE - ## coef0 numeric - 0 -Inf to Inf Y TRUE - ## gamma numeric - - 0 to Inf Y TRUE - ## cachesize numeric - 40 -Inf to Inf - TRUE - ## tolerance numeric - 0.001 0 to Inf - TRUE - ## shrinking logical - TRUE - - TRUE - ## cross integer - 0 0 to Inf - FALSE - ## fitted logical - TRUE - - FALSE - ## scale logicalvector &lt;NA&gt; TRUE - - TRUE - Es importante tener en cuenta que algoritmo SVM es sensible a las variables que se encuentran en diferentes escalas, por lo que generalmente es una buena idea escalar los predictores primero. Tipos de Kernesls kernels &lt;- c(&quot;polynomial&quot;, &quot;radial&quot;, &quot;sigmoid&quot;) Se utiliza la función makeParamSet () para definir el espacio de hiperparámetros que deseamos sintonizar. Para la makeParamSet (), obtenemos la información necesaria para definir cada hiperparámetro que deseamos sintonizar, separados por columnas. kernel hiperparámetro toma discretos valores ( el nombre del kernel función ) , por lo que utilizar el makeDiscreteParam () función para definir sus valores como el vector de kernels que creamos. degree hiperparámetro toma valores enteros ( números enteros ) , por lo que utilizamos la función makeIntegerParam () y definir sus valores superior e inferior que deseamos sintonizar cost y gamma hiperparámetros toman valores numéricos ( cualquier número entre cero y el infinito ) , por lo que utilizar la función makeNumericParam () para poder definir los valores superior e inferior que deseamos sintonizar. svmParamSpace &lt;- makeParamSet( makeDiscreteParam(&quot;kernel&quot;, values = kernels), makeIntegerParam(&quot;degree&quot;, lower = 1, upper = 3), makeNumericParam(&quot;cost&quot;, lower = 0.1, upper = 10), makeNumericParam(&quot;gamma&quot;, lower = 0.1, 10)) svmParamSpace ## Type len Def Constr Req Tunable Trafo ## kernel discrete - - polynomial,radial,sigmoid - TRUE - ## degree integer - - 1 to 3 - TRUE - ## cost numeric - - 0.1 to 10 - TRUE - ## gamma numeric - - 0.1 to 10 - TRUE - Digamos que queríamos probar los valores de la hiperparámetros cost y gamma en pasos de 0.1 , que es de 100 valores de cada uno. Tenemos tres funciones del kernel y tres valores del hiperparámetro de degree. Para realizar una búsqueda de rejilla sobre Si queremos entrenar con grandes numeros podemos emplear una técnica llamada búsqueda aleatoria . En lugar de intentar todas las combinaciones posibles de los parámetros, búsqueda aleatoria procede de la siguiente manera: Seleccionar al azar una combinación de valores de hiperparámetros. Uso validación cruzada para entrenar y evaluar un modelo usando los valores de hiperparámetros. Registro la métrica de rendimiento de la modelo ( por lo general significa la clasificación errónea error para tareas de clasificación ) Repetir ( Iterate ) los pasos 1 a 3 tantas veces como su presupuesto computacional permite Seleccione la combinación de los valores hiperparámetro que le dio la mejor modelo de desempeño A diferencia de rejilla de búsqueda, búsqueda aleatoria no está garantizado para encontrar el mejor conjunto de valores de hiperparámetros. Sin embargo, con suficientes iteraciones, se puede Suele encontrar una buena combinación que funcione bien. Mediante el uso de azar de búsqueda, podemos ejecutar 500 combinaciones de valores hiperparámetro. Vamos a definir nuestra búsqueda al azar utilizando la función makeTuneControlRandom (). Decimos la función de la cantidad de iteraciones de la aleatoria que queremos usar la búsqueda, con el maxit argumento. Es recomendables establecer un numero de iteracione lo mas alta posible que dispositivo lo permitan, pero en este ejemplo vamos a repetir a 40 con el fin de evitar que el ejemplo de tomar demasiado tiempo. A continuación, describo nuestra validación cruzada procedimiento. Por lo general prefieren k veces validación cruzada para retención de salida transversal de validación ( como el primero da estimaciones más estables de modelo rendimiento ) , a menos que el procedimiento de formación es computacionalmente costoso. Bueno, esto es computacionalmente caro, así que estoy comprometer utilizando en su lugar la validación cruzada de retención. randSearch &lt;- makeTuneControlRandom(maxit = 40) la busqueda de la mejor realización de la combinación de hiperparámetros, uno enfoque consiste en utilizar modelos de trenes en todas las combinaciones de hiperparámetros, de forma exhaustiva. Tenemos entonces evaluar el desempeño de cada uno de estos modelos y elija el de mejor rendimiento. Este es llamada búsqueda de cuadrícula. Se utilizó el procedimiento de búsqueda cuadrícula para tratar cada valor de k que definimos, durante el ajuste. Esto es lo que el método de búsqueda de rejilla hace: intenta todas las combinaciones de la hiperparámetro espacio se define, y encuentra el mejor rendimiento combinación. cvForTuning &lt;- makeResampleDesc(&quot;Holdout&quot;, split = 2/3) Para ejecutar un proceso de MLR en paralelo, ponemos su código entre la parallelStartSocket () y parallelStop () funciones del paquete paralelMap. Para iniciar nuestra hiperparámetro sintonía proceso, que llamamos los tuneParams () función y suministro como argumentos: el primer argumento es el nombre del alumno (\"classif.svm\"). task = el nombre de la tarea. resampling = el procedimiento de validación cruzada ( definido en la lista de códigos 5 ). par. set = el espacio de hiperparámetros ( definido en el listado de código 4 ). control = el procedimiento de búsqueda ( búsqueda aleatoria, definida en la lista de códigos 5 ). Realización del ajuste de hiperparámetros library(parallelMap) library(parallel) parallelStartSocket(cpus = detectCores()) ## Starting parallelization in mode=socket with cpus=4. tunedSvmPars &lt;- tuneParams(&quot;classif.svm&quot;, task = spamTask, resampling = cvForTuning, par.set = svmParamSpace, control = randSearch) ## [Tune] Started tuning learner classif.svm for parameter set: ## Type len Def Constr Req Tunable Trafo ## kernel discrete - - polynomial,radial,sigmoid - TRUE - ## degree integer - - 1 to 3 - TRUE - ## cost numeric - - 0.1 to 10 - TRUE - ## gamma numeric - - 0.1 to 10 - TRUE - ## With control class: TuneControlRandom ## Imputation value: 1 ## Exporting objects to slaves for mode socket: .mlr.slave.options ## Mapping in parallel: mode = socket; level = mlr.tuneParams; cpus = 4; elements = 40. ## [Tune] Result: kernel=polynomial; degree=1; cost=7.78; gamma=7.88 : mmce.test.mean=0.0723598 parallelStop() ## Stopped parallelization. All cleaned up. El hiperparámetro grado sólo se aplica al polinomio núcleo la función y el hiperparámetro gamma no se aplica a los lineales núcleo. Extraer los valores de los hiperparámetros ganadores del ajuste #Revisamos los parÃ¡metros tunedSvmPars ## Tune result: ## Op. pars: kernel=polynomial; degree=1; cost=7.78; gamma=7.88 ## mmce.test.mean=0.0723598 Puede imprimir el mejor valor del rendimiento del hiperparámetro y el rendimiento del modelo construido. llamando tunedSvm podemos ver que la primera polinomio núcleo grado de función ( equivalente al núcleo lineal función ) , con un cost de 2.23 y gamma de 7.99 , dio el modelo de mejor rendimiento. Y como un mmce de 0.0651890. Entrenamiento del modelo con hiperparámetros ajustados Ahora hemos afinado los hiperparámetros, para luego construir el modelo de combinación con mejor rendimiento. Utilizamos las funciones setHyperPars ()para combinar un alumno con un conjunto de pre-definido valores de hiperparámetros,con el primer argumento es el alumno que queremos uso, y el par.vals El argumento es el objeto que contiene nuestros valores de hiperparámetros ajustados. tunedSvm &lt;- setHyperPars(makeLearner(&quot;classif.svm&quot;), par.vals = tunedSvmPars$x) Luego entrenar un modelo utilizando nuestro tunedSvm alumno con el train (). tunedSvmModel &lt;- train(tunedSvm, spamTask) tunedSvmModel ## Model for learner.id=classif.svm; learner.class=classif.svm ## Trained on: task.id = spamTib; obs = 4601; features = 57 ## Hyperparameters: kernel=polynomial,degree=1,cost=7.78,gamma=7.88 6.1.3.1 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos Soporta nota classif.clusterSVM SwarmSVM, LiblineaR x x dos clase centers establecido de forma predeterminada. classif.dcSVM SwarmSVM, e1071 x dos clase classif.gaterSVM SwarmSVM x dos clase m establecido y establecido en de forma predeterminada.3 max.iter1 classif.ksvm kernlab x x dos clase, multiclase, prob, class.weights Los parámetros del núcleo deben pasarse directamente y no utilizando la lista en . Tenga en cuenta que se ha establecido de forma predeterminada para la velocidad. kpar ksvmf it FALSE classif.lssvm kernlab x x dos clase, multiclase, fitted se ha establecido en por defecto para la velocidad.FALSE classif.svm e1071 x x x dos clase, multiclase, prob, class.weights regr.ksvm kernlab x x Los parámetros del núcleo deben pasarse directamente y no utilizando la lista en . Tenga en cuenta que se ha establecido de forma predeterminada para la velocidad.kpar ksvm fit FALSE regr.svm ke1071 x x x featimp Todos los ajustes se pasan directamente, en lugar de a través del argumento s. se ha establecido en y en por defecto. xgboost params nrounds 1 verbose 0 . 6.2 Algoritmo no supervisados ** los modelos de aprendizaje no supervisado** son aquellos en los que no estamos interesados en ajustar pares como las entrada y salida, sino mas bien en aumentar el conocimiento estructural de los datos disponibles y/o posibles datos futuros que provengan del mismo fenómeno, un ejemplo claro es la agrupación de los datos según su similaridad lo que es llmado clustering, simplificando las estructura de los mismos manteniendo sus características fundamentales como en los procesos de reducción de la dimensionalidad, o extrayendo la estructura interna con la que se distribuyen los datos en su espacio original. 6.2.1 K-Means install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) library(mlr) library(tidyverse) library(datasets) library(XML) Cargamos la data La data que utilizaremos ** GvHD ** que es la enfermedad de injerto contra huésped de Brinkman et al. (2007). Dos muestras de estos datos de citometría de flujo, una de un paciente con EICH y la otra de un paciente de control. Las muestras de GvHD positivas y de control constan de 9083 y 6809 observaciones, respectivamente. Ambas muestras incluyen cuatro variables de biomarcadores, a saber, CD4, CD8b, CD3 y CD8. El objetivo del análisis es identificar subpoblaciones de células CD3 + CD4 + CD8b + presentes en la muestra positiva de EICH. Usaremos mclust es un paquete R contribuido para el agrupamiento, la clasificación y la estimación de densidad basados en modelos basados en el modelado de mezclas normales finitas. data(GvHD, package = &quot;mclust&quot;) La data GvHD posee lo siguiente: GvHD.pos (paciente positivo) es un marco de datos con 9083 observaciones sobre las siguientes 4 variables, que son mediciones de biomarcadores. CD4 CD8b CD3 CD8 GvHD.control (paciente de control) es un marco de datos con 6809 observaciones sobre las siguientes 4 variables, que son mediciones de biomarcadores. CD4 CD8b CD3 CD8 Usaremos este ultimo. Ahora carguemos los datos, que están integrados en el paquete mclust, conviértalos en tibble con as_tibble () . gvhdTib &lt;- as_tibble(GvHD.control) gvhdTib summary(gvhdTib) ## CD4 CD8b CD3 CD8 ## Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. : 1.0 ## 1st Qu.:143.0 1st Qu.:149.0 1st Qu.: 71.0 1st Qu.:142.0 ## Median :277.0 Median :341.0 Median :123.0 Median :195.0 ## Mean :258.6 Mean :292.2 Mean :161.8 Mean :204.9 ## 3rd Qu.:351.0 3rd Qu.:411.0 3rd Qu.:185.0 3rd Qu.:241.0 ## Max. :670.0 Max. :781.0 Max. :740.0 Max. :778.0 Escalonamiento Debido a que los algoritmos de k-medias usan una métrica de distancia para asignar casos a grupos, es importante que nuestras variables se escalen para que las variables en diferentes escalas reciban la misma peso. Todas nuestras variables son continuas, por lo que simplemente podemos canalizar todo nuestro tibble en la función scale (). De esta manera centrará y escalará cada variable por restando la media y dividiendo por la desviación estándar. gvhdScaled &lt;- gvhdTib %&gt;% scale() summary(gvhdScaled) ## CD4 CD8b CD3 CD8 ## Min. :-1.8897 Min. :-1.9934 Min. :-1.1683 Min. :-1.76768 ## 1st Qu.:-0.8481 1st Qu.:-0.9801 1st Qu.:-0.6598 1st Qu.:-0.54508 ## Median : 0.1348 Median : 0.3344 Median :-0.2820 Median :-0.08552 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6776 3rd Qu.: 0.8136 3rd Qu.: 0.1684 3rd Qu.: 0.31334 ## Max. : 3.0175 Max. : 3.3468 Max. : 4.2004 Max. : 4.96961 library(GGally) ** Visualización de densidad ** El gráfico de cada variable frente a cualquier otra variable en nuestro conjunto de datos de GvHD. Los diagramas de dispersión se muestran debajo de la diagonal, los diagramas de densidad 2D se muestran sobre la diagonal y los diagramas de densidad 1D se dibujan en la diagonal. Parece como si hubiera varios clústeres en los datos. ggpairs(GvHD.control, upper = list(continuous = &quot;density&quot;), lower = list(continuous = wrap(&quot;points&quot;, size = 0.5)), diag = list(continuous = &quot;densityDiag&quot;)) + theme_bw() ## plot: [1,1] [======&gt;------------------------------------------------------------------------------------------------------------] 6% est: 0s ## plot: [1,2] [=============&gt;-----------------------------------------------------------------------------------------------------] 12% est: 0s ## plot: [1,3] [=====================&gt;---------------------------------------------------------------------------------------------] 19% est: 2s ## plot: [1,4] [============================&gt;--------------------------------------------------------------------------------------] 25% est: 3s ## plot: [2,1] [===================================&gt;-------------------------------------------------------------------------------] 31% est: 3s ## plot: [2,2] [==========================================&gt;------------------------------------------------------------------------] 38% est: 3s ## plot: [2,3] [=================================================&gt;-----------------------------------------------------------------] 44% est: 2s ## plot: [2,4] [=========================================================&gt;---------------------------------------------------------] 50% est: 2s ## plot: [3,1] [================================================================&gt;--------------------------------------------------] 56% est: 2s ## plot: [3,2] [=======================================================================&gt;-------------------------------------------] 62% est: 1s ## plot: [3,3] [==============================================================================&gt;------------------------------------] 69% est: 1s ## plot: [3,4] [=====================================================================================&gt;-----------------------------] 75% est: 1s ## plot: [4,1] [============================================================================================&gt;----------------------] 81% est: 1s ## plot: [4,2] [====================================================================================================&gt;--------------] 88% est: 0s ## plot: [4,3] [===========================================================================================================&gt;-------] 94% est: 0s ## plot: [4,4] [===================================================================================================================]100% est: 0s Creamos la tarea de clasificación Con mlr, creamos una tarea de agrupamiento usando la función makeClusterTask (). gvhdTask &lt;- makeClusterTask(data = as.data.frame(gvhdScaled)) gvhdTask ## Unsupervised task: as.data.frame(gvhdScaled) ## Type: cluster ## Observations: 6809 ## Features: ## numerics factors ordered functionals ## 4 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE Es importante Tener en cuenta que, a diferencia de la creación de una tarea de aprendizaje supervisada , ya no es necesario proporcionar el argumento objetivo. Esto se debe a que en una tarea de aprendizaje no supervisada, no hay variables de etiquetas para utilizar como objetivo. Que tipos de clusters existen en MLR Usemos la función listLearners () para ver qué algoritmos ha implementado el paquete mlr hasta ahora. listLearners(&quot;cluster&quot;)$class ## [1] &quot;cluster.cmeans&quot; &quot;cluster.Cobweb&quot; &quot;cluster.dbscan&quot; &quot;cluster.EM&quot; &quot;cluster.FarthestFirst&quot; ## [6] &quot;cluster.kkmeans&quot; &quot;cluster.kmeans&quot; &quot;cluster.MiniBatchKmeans&quot; &quot;cluster.SimpleKMeans&quot; &quot;cluster.XMeans&quot; Ahora definamos nuestro alumno de k-medias. Hacemos esto usando la función makeLearner (), esta vez proporcionando cluster.kmeans como el nombre del alumno. Usamos el argumento par.vals para proporcionar dos argumentos al alumno, con el argumento iter.max establece un límite superior para el número de veces que el algoritmo recorrerá los datos en este caso asignaremos 100 y con el argumento nstart controla cuántas veces la función inicializará aleatoriamente los centros que asignamos 10. kMeans &lt;- makeLearner(&quot;cluster.kmeans&quot;, par.vals = list(iter.max = 100, nstart = 10)) es importante tener en cuenta que los centros iniciales generalmente se inicializan aleatoriamente en algún lugar del espacio de características, lo cual puede tener un impacto en las posiciones finales del centroide y al establecer el argumento nstart más alto que el valor predeterminado de 1 inicializará aleatoriamente este número de centros. Para cada juego de los centros iniciales, los casos se asignan al grupo de su centro más cercano en cada conjunto, y el conjunto con la menor suma de error cuadrado dentro del clúster se utiliza para el resto del algoritmo de agrupamiento. De esta forma, el algoritmo selecciona el conjunto de centros que ya es más similar a los centroides de clúster reales en los datos. Tuning k and the algorithm choice for our k-means model Podemos usar la función getParamSet(kMeans) para encontrar todos los hiperparámetros disponibles para nosotros. getParamSet(kMeans) ## Type len Def Constr Req Tunable Trafo ## centers untyped - - - - TRUE - ## iter.max integer - 10 1 to Inf - TRUE - ## nstart integer - 1 1 to Inf - TRUE - ## algorithm discrete - Hartigan-Wong Hartigan-Wong,Lloyd,Forgy,MacQueen - TRUE - ## trace logical - - - - FALSE - Los hiperparámetros disponibles son: Hartigan-Wong: Lloyd: Forgy: MacQuee: a continuación usaremos la función makeParamSet(). Definimos dos hiperparámetros discretos sobre los cuales buscar valores: centros, que es el número de grupos que el algoritmo buscará (k), y el algoritmo, que especifica cuál de los tres algoritmos utilizará para adaptarse al modelo. kMeansParamSpace &lt;- makeParamSet( makeDiscreteParam(&quot;centers&quot;, values = 3:8), makeDiscreteParam(&quot;algorithm&quot;, values = c(&quot;Hartigan-Wong&quot;, &quot;Lloyd&quot;, &quot;MacQueen&quot;))) gridSearch &lt;- makeTuneControlGrid() gridSearch ## Tune control: TuneControlGrid ## Same resampling instance: TRUE ## Imputation value: &lt;worst&gt; ## Start: &lt;NULL&gt; ## ## Tune threshold: FALSE ## Further arguments: resolution=10 Definimos nuestro enfoque de validación cruzada como 10 veces. kFold &lt;- makeResampleDesc(&quot;CV&quot;, iters = 10) install.packages(&quot;clusterSim&quot;) ## Error in install.packages : Updating loaded packages listMeasures(&quot;cluster&quot;) ## [1] &quot;featperc&quot; &quot;db&quot; &quot;timeboth&quot; &quot;timetrain&quot; &quot;timepredict&quot; &quot;silhouette&quot; &quot;G1&quot; &quot;G2&quot; Realizar ajustes Para realizar el ajuste, usamos la función tuneParams()con los siguentes argumentos: kMeanses el primer argumento que es el nombre del alumno. El argumento task es el nombre de la tarea de agrupamiento. El argumentoresampling es el nombre de nuestra estrategia de validación cruzada. El argumento par.set es nuestro espacio de búsqueda de hiperparámetros. El argumento control es nuestro método de búsqueda. El argumento measures nos permite definir qué medidas de desempeño queremos,con índice (db), índice de Dunn (dunn) y pseudo estadístico F (G1), en ese orden. tunedK &lt;- tuneParams(kMeans, task = gvhdTask, resampling = kFold, par.set = kMeansParamSpace, control = gridSearch, measures = list(db, G1)) ## [Tune] Started tuning learner cluster.kmeans for parameter set: ## Type len Def Constr Req Tunable Trafo ## centers discrete - - 3,4,5,6,7,8 - TRUE - ## algorithm discrete - - Hartigan-Wong,Lloyd,MacQueen - TRUE - ## With control class: TuneControlGrid ## Imputation value: InfImputation value: -0 ## [Tune-x] 1: centers=3; algorithm=Hartigan-Wong ## [Tune-y] 1: db.test.mean=1.1706155,G1.test.mean=341.4513344; time: 0.0 min ## [Tune-x] 2: centers=4; algorithm=Hartigan-Wong ## [Tune-y] 2: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 3: centers=5; algorithm=Hartigan-Wong ## [Tune-y] 3: db.test.mean=1.1440635,G1.test.mean=437.1214563; time: 0.0 min ## [Tune-x] 4: centers=6; algorithm=Hartigan-Wong ## [Tune-y] 4: db.test.mean=1.1910863,G1.test.mean=394.5595074; time: 0.0 min ## [Tune-x] 5: centers=7; algorithm=Hartigan-Wong ## [Tune-y] 5: db.test.mean=1.1691958,G1.test.mean=377.6782850; time: 0.0 min ## [Tune-x] 6: centers=8; algorithm=Hartigan-Wong ## [Tune-y] 6: db.test.mean=1.2175333,G1.test.mean=362.0658320; time: 0.0 min ## [Tune-x] 7: centers=3; algorithm=Lloyd ## [Tune-y] 7: db.test.mean=1.1706155,G1.test.mean=341.4513344; time: 0.0 min ## [Tune-x] 8: centers=4; algorithm=Lloyd ## [Tune-y] 8: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 9: centers=5; algorithm=Lloyd ## [Tune-y] 9: db.test.mean=1.1447321,G1.test.mean=437.0755098; time: 0.0 min ## [Tune-x] 10: centers=6; algorithm=Lloyd ## [Tune-y] 10: db.test.mean=1.2037249,G1.test.mean=392.8459968; time: 0.0 min ## [Tune-x] 11: centers=7; algorithm=Lloyd ## [Tune-y] 11: db.test.mean=1.1789063,G1.test.mean=376.8527292; time: 0.0 min ## [Tune-x] 12: centers=8; algorithm=Lloyd ## Warning: did not converge in 100 iterations ## [Tune-y] 12: db.test.mean=1.2167544,G1.test.mean=361.9683803; time: 0.0 min ## [Tune-x] 13: centers=3; algorithm=MacQueen ## [Tune-y] 13: db.test.mean=1.1708359,G1.test.mean=341.4686930; time: 0.0 min ## [Tune-x] 14: centers=4; algorithm=MacQueen ## [Tune-y] 14: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 15: centers=5; algorithm=MacQueen ## [Tune-y] 15: db.test.mean=1.1447565,G1.test.mean=437.0926618; time: 0.0 min ## [Tune-x] 16: centers=6; algorithm=MacQueen ## [Tune-y] 16: db.test.mean=1.2066667,G1.test.mean=394.0274437; time: 0.0 min ## [Tune-x] 17: centers=7; algorithm=MacQueen ## [Tune-y] 17: db.test.mean=1.1834367,G1.test.mean=376.5984665; time: 0.0 min ## [Tune-x] 18: centers=8; algorithm=MacQueen ## [Tune-y] 18: db.test.mean=1.2103025,G1.test.mean=361.8921627; time: 0.0 min ## [Tune] Result: centers=4; algorithm=Hartigan-Wong : db.test.mean=0.8066902,G1.test.mean=486.3999518 tunedK ## Tune result: ## Op. pars: centers=4; algorithm=Hartigan-Wong ## db.test.mean=0.8066902,G1.test.mean=486.3999518 necesitamos extraer los datos de ajuste de nuestro resultado de ajuste con la función generateHyperParsEffectData(). Llame al componente ($)data desde kMeansTuningData. Recopilamos los datos de manera que el El nombre de cada métrica de rendimiento está en una columna y el valor de la métrica está en otra columna. Hacemos esto usando la función gather(), nombrando la columna clave Metric y la columna de valor Value. kMeansTuningData &lt;- generateHyperParsEffectData(tunedK) gatheredTuningData &lt;- gather(kMeansTuningData$data, key = &quot;Metric&quot;, value = &quot;Value&quot;, c(-centers, -iteration, -algorithm)) gatheredTuningData ## centers algorithm iteration Metric Value ## 1 3 Hartigan-Wong 1 db.test.mean 1.1706155 ## 2 4 Hartigan-Wong 2 db.test.mean 0.8066902 ## 3 5 Hartigan-Wong 3 db.test.mean 1.1440635 ## 4 6 Hartigan-Wong 4 db.test.mean 1.1910863 ## 5 7 Hartigan-Wong 5 db.test.mean 1.1691958 ## 6 8 Hartigan-Wong 6 db.test.mean 1.2175333 ## 7 3 Lloyd 7 db.test.mean 1.1706155 ## 8 4 Lloyd 8 db.test.mean 0.8066902 ## 9 5 Lloyd 9 db.test.mean 1.1447321 ## 10 6 Lloyd 10 db.test.mean 1.2037249 ## 11 7 Lloyd 11 db.test.mean 1.1789063 ## 12 8 Lloyd 12 db.test.mean 1.2167544 ## 13 3 MacQueen 13 db.test.mean 1.1708359 ## 14 4 MacQueen 14 db.test.mean 0.8066902 ## 15 5 MacQueen 15 db.test.mean 1.1447565 ## 16 6 MacQueen 16 db.test.mean 1.2066667 ## 17 7 MacQueen 17 db.test.mean 1.1834367 ## 18 8 MacQueen 18 db.test.mean 1.2103025 ## 19 3 Hartigan-Wong 1 G1.test.mean 341.4513344 ## 20 4 Hartigan-Wong 2 G1.test.mean 486.3999518 ## 21 5 Hartigan-Wong 3 G1.test.mean 437.1214563 ## 22 6 Hartigan-Wong 4 G1.test.mean 394.5595074 ## 23 7 Hartigan-Wong 5 G1.test.mean 377.6782850 ## 24 8 Hartigan-Wong 6 G1.test.mean 362.0658320 ## 25 3 Lloyd 7 G1.test.mean 341.4513344 ## 26 4 Lloyd 8 G1.test.mean 486.3999518 ## 27 5 Lloyd 9 G1.test.mean 437.0755098 ## 28 6 Lloyd 10 G1.test.mean 392.8459968 ## 29 7 Lloyd 11 G1.test.mean 376.8527292 ## 30 8 Lloyd 12 G1.test.mean 361.9683803 ## 31 3 MacQueen 13 G1.test.mean 341.4686930 ## 32 4 MacQueen 14 G1.test.mean 486.3999518 ## 33 5 MacQueen 15 G1.test.mean 437.0926618 ## 34 6 MacQueen 16 G1.test.mean 394.0274437 ## 35 7 MacQueen 17 G1.test.mean 376.5984665 ## 36 8 MacQueen 18 G1.test.mean 361.8921627 ## 37 3 Hartigan-Wong 1 exec.time 0.4800000 ## 38 4 Hartigan-Wong 2 exec.time 0.5800000 ## 39 5 Hartigan-Wong 3 exec.time 0.8900000 ## 40 6 Hartigan-Wong 4 exec.time 0.9700000 ## 41 7 Hartigan-Wong 5 exec.time 1.4100000 ## 42 8 Hartigan-Wong 6 exec.time 1.3400000 ## 43 3 Lloyd 7 exec.time 0.4500000 ## 44 4 Lloyd 8 exec.time 0.5300000 ## 45 5 Lloyd 9 exec.time 1.1400000 ## 46 6 Lloyd 10 exec.time 1.1900000 ## 47 7 Lloyd 11 exec.time 1.5900000 ## 48 8 Lloyd 12 exec.time 1.8400000 ## 49 3 MacQueen 13 exec.time 0.3700000 ## 50 4 MacQueen 14 exec.time 0.4600000 ## 51 5 MacQueen 15 exec.time 0.6700000 ## 52 6 MacQueen 16 exec.time 0.7900000 ## 53 7 MacQueen 17 exec.time 0.9600000 ## 54 8 MacQueen 18 exec.time 1.0600000 Para graficar los datos anterior usaremos la función ggplot(), mapeando centros (el número de clusters) y Valor a la estética xey, respectivamente. Al mapear el algoritmo a las capas estéticas col, geom_line() y geom_point() separadas se dibujarán para cada algoritmo (con diferentes colores). ggplot(gatheredTuningData, aes(centers, Value, col = algorithm)) + facet_wrap(~ Metric, scales = &quot;free_y&quot;) + geom_line() + geom_point() + theme_bw() La mayor diferencia entre los algoritmos es su tiempo de entrenamiento. Darse cuenta de El algoritmo de MacQueen es consistentemente más rápido que cualquiera de los otros. Esto es debido a el algoritmo actualiza sus centroides con más frecuencia que Lloyds y tiene que vuelve a calcular las distancias con menos frecuencia que Hartigan-Wong. El algoritmo de Hartigan-Wong Construyendo su primer modelo 395 de k-means parece ser el más intenso computacionalmente en números bajos de clústeres, pero supera Algoritmo de Lloyds a medida que el número de grupos aumenta más allá de siete. Entrenamiento del modelo final de k-medias ajustado Ahora usaremos nuestros hiperparámetros ajustados para entrenar nuestro modelo de agrupamiento final.Ademas utilizaremos la validación cruzada anidada para validar de forma cruzada Todo el proceso de construcción de modelos. primero comensaremos creando un alumno de k-means que use nuestros valores de hiperparámetros ajustados, usando la función setHyperPars(). tunedKMeans &lt;- setHyperPars(kMeans, par.vals = tunedK$x) tunedKMeans ## Learner cluster.kmeans from package stats,clue ## Type: cluster ## Name: K-Means; Short name: kmeans ## Class: cluster.kmeans ## Properties: numerics,prob ## Predict-Type: response ## Hyperparameters: centers=4,iter.max=100,nstart=10,algorithm=Hartigan-Wong Luego entrenamos este modelo ajustado en nuestra gvhdTask usando la función train() y usamos la función getLearnerModel() para extraer los datos del modelo para que podamos trazar los conglomerados. tunedKMeansModel &lt;- train(tunedKMeans, gvhdTask) tunedKMeansModel ## Model for learner.id=cluster.kmeans; learner.class=cluster.kmeans ## Trained on: task.id = as.data.frame(gvhdScaled); obs = 6809; features = 4 ## Hyperparameters: centers=4,iter.max=100,nstart=10,algorithm=Hartigan-Wong Imprima los datos del modelo llamando a kMeansModelData y examine la salida; Contiene una gran cantidad de información útil. kMeansModelData &lt;- getLearnerModel(tunedKMeansModel) kMeansModelData ## K-means clustering with 4 clusters of sizes 428, 4227, 1488, 666 ## ## Cluster means: ## CD4 CD8b CD3 CD8 ## 1 -0.9164416 0.5912660 1.6843512 2.796499034 ## 2 0.2904422 0.5955009 -0.3417141 -0.216151841 ## 3 -1.2790155 -1.2397409 -0.4350552 -0.004331523 ## 4 1.6031727 -1.3896543 2.0583862 -0.415589260 ## ## Clustering vector: ## [1] 2 2 3 3 3 2 1 2 2 2 2 2 2 3 3 1 3 2 3 3 2 3 2 2 4 2 3 2 2 4 2 2 2 2 2 3 2 1 3 2 4 1 4 3 4 2 2 2 1 2 2 3 2 2 2 2 3 4 2 2 3 2 3 2 3 4 2 2 2 ## [70] 2 4 2 1 2 3 2 2 2 2 2 4 2 3 3 1 4 2 3 2 3 2 4 2 1 4 2 2 2 2 3 2 2 2 3 3 2 2 2 2 4 4 2 2 2 4 3 2 2 2 3 3 2 2 3 2 2 3 4 2 2 2 2 2 2 2 4 3 2 ## [139] 3 3 2 2 3 2 2 3 2 3 2 2 2 2 2 3 3 3 2 3 2 2 3 1 2 2 1 2 2 4 3 2 2 4 2 2 4 2 3 2 3 3 2 2 3 2 2 3 2 2 2 3 3 3 2 3 2 2 1 2 2 2 2 4 2 2 3 3 3 ## [208] 2 2 3 2 4 3 3 3 2 2 3 3 2 2 3 2 2 3 2 2 3 2 2 3 2 4 2 3 3 4 2 2 3 2 2 2 2 1 2 2 2 2 2 3 4 3 3 3 3 3 2 1 3 2 3 3 2 1 3 2 3 2 2 2 3 2 3 3 2 ## [277] 2 3 3 2 2 2 2 3 3 2 2 2 3 1 3 2 2 2 3 2 2 3 2 3 2 1 2 3 3 2 4 2 3 3 1 2 2 2 3 3 4 2 2 2 2 3 4 4 2 4 2 4 3 2 2 2 2 2 2 2 3 4 2 2 2 1 2 1 2 ## [346] 2 4 2 4 4 2 2 2 2 3 3 2 4 2 2 2 2 4 2 3 4 2 2 4 1 2 3 2 2 2 2 3 2 2 3 3 2 3 2 2 2 4 2 3 2 3 2 2 2 2 2 2 3 4 2 1 3 2 3 2 2 3 3 2 2 4 3 4 2 ## [415] 2 2 2 4 2 2 3 4 2 2 2 2 2 2 3 2 3 2 2 2 2 3 2 2 2 2 2 2 2 1 3 2 1 1 2 1 2 2 2 4 3 3 2 2 2 2 2 2 2 2 2 4 3 2 2 2 3 2 3 3 2 2 2 2 2 2 2 2 2 ## [484] 3 2 3 4 2 3 1 2 2 4 4 2 2 4 1 2 2 2 2 2 2 3 3 2 2 3 2 3 2 2 2 2 2 1 2 4 2 4 2 2 2 2 3 2 3 1 2 4 2 2 2 4 2 2 1 2 2 3 3 3 2 3 3 2 2 2 2 2 2 ## [553] 2 2 2 2 2 3 2 2 1 4 2 3 3 2 3 4 1 3 2 2 1 2 3 1 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 3 3 3 2 4 2 2 2 4 2 2 2 2 2 2 2 3 2 3 2 2 2 3 1 3 4 4 2 2 2 ## [622] 1 2 2 2 2 4 3 4 2 2 3 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2 4 4 3 2 2 3 3 2 2 4 2 2 4 3 2 1 2 3 2 3 2 1 3 3 3 2 1 3 2 2 ## [691] 2 2 3 3 2 3 2 3 2 2 3 2 3 1 2 2 3 2 1 2 2 2 4 2 4 1 2 3 2 2 2 2 4 2 2 3 2 2 2 2 2 2 3 2 1 2 2 2 3 3 2 2 3 3 4 3 2 4 2 2 2 2 1 3 2 2 2 2 2 ## [760] 4 2 2 3 3 1 4 3 2 2 2 2 2 3 4 3 2 2 2 2 2 2 1 4 3 2 2 2 2 4 2 2 3 2 2 3 2 2 2 3 3 4 4 3 2 2 4 2 2 2 2 4 2 2 3 3 2 4 3 1 3 2 2 3 2 2 2 3 3 ## [829] 2 2 2 2 4 2 2 2 3 2 2 2 3 1 4 2 2 3 3 2 2 2 2 3 4 3 2 3 4 2 3 2 2 4 3 2 2 2 2 2 2 4 2 4 2 2 2 4 2 2 2 1 2 3 3 2 3 1 3 2 2 4 2 3 3 2 2 2 3 ## [898] 3 3 2 2 2 3 4 2 3 3 3 3 2 1 2 2 2 3 2 2 1 4 4 2 3 2 2 3 4 2 3 4 1 3 1 2 2 2 1 2 2 2 2 4 2 1 4 3 2 2 2 2 2 2 4 2 2 3 2 2 3 2 2 2 4 1 2 2 2 ## [967] 2 2 2 2 2 3 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 4 2 2 3 2 1 2 2 2 4 2 4 4 ## [ reached getOption(&quot;max.print&quot;) -- omitted 5809 entries ] ## ## Within cluster sum of squares by cluster: ## [1] 1402.9972 4239.4711 2113.1202 921.7474 ## (between_SS / total_SS = 68.1 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Usando nuestro modelo para predecir grupos de nuevos datos Utilizaremos un caso practico, tomaremos nuevos datos y generar los clústeres a los que se acercan más los nuevos casos. Como por ejemplo posea los siguientes datos: CD4 = 1 CD8b = 26 CD3 = 1 CD8 = 122 Pero primero comencemos por crear un tibble que contenga los datos anteriores, incluido un valor para cada variable del conjunto de datos en el que entrenamos el modelo. Una ves resliazado lo anterior escalamos los datos de entrenamiento. La forma más sencilla de hacerlo es utilizar el función attr() para extraer el centro y los atributos de escala de los datos escalados. Debido a que la función scale() devuelve un objeto de matriz de clase (y la función predict () la función arrojará un error si le damos una matriz), necesitamos canalizar los datos escalados en la función `as_tibble() para convertirlo de nuevo en tibble. Para predecir a qué grupo pertenece el nuevo caso, simplemente llamamos al predict() función, proporcionando el modelo como primer argumento y el nuevo caso como newdata argumento. newCell &lt;- tibble(CD4 = 1, CD8b = 26, CD3 = 1, CD8 = 122) %&gt;% scale(center = attr(gvhdScaled, &quot;scaled:center&quot;), scale = attr(gvhdScaled, &quot;scaled:scale&quot;)) %&gt;% as_tibble() newCell ## # A tibble: 1 x 4 ## CD4 CD8b CD3 CD8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.89 -1.82 -1.17 -0.718 predict(tunedKMeansModel, newdata = newCell) ## Warning in predict.WrappedModel(tunedKMeansModel, newdata = newCell): Provided data for prediction is not a pure data.frame but from class ## tbl_df, hence it will be converted. ## Prediction: 1 observations ## predict.type: response ## threshold: ## time: 0.01 ## response ## 1 3 Podemos ver en la salida que este nuevo caso está más cerca del centroide de grupo 1. Para saber la caracterirsticas principales que posee el grupo 1 es importante ver las observaciones que pertener a este grupo y encontrar puntos en comun con el fin de entender mejor la clasificación obtenida. clase paquete Num Fac Ord Nas Pesos Soporta nota cluster.kkmeans kknn x x x prob, dos clase Delegados a con . Establecemos model en FALSE de forma predeterminada para ahorrar memoria. glm family = binomial(link = 'logit') classif. LiblineaRL1LogReg LiblineaR x dos clase, multiclase, prob, class.weights classif. LiblineaRL2LogReg LiblineaR x dos clase, multiclase, prob, class.weights type = 0 (el valor predeterminado) es primario y es doble problema.type = 7 6.2.2 PCA https://bookdown.org/dparedesi/data-science-con-r/aprendizaje-no-supervisado.html "]]
