[["machine-learning.html", "6 Machine Learning 6.1 Algoritmo Supervisados 6.2 Algoritmo no supervisados", " 6 Machine Learning Machine Learning o también conocido como Aprendizaje automático es una rama de la inteligencia artificial, que tiene como objetivo desarrollar técnicas que permitan que las computadoras aprendan, en otras palabras, es la capacidad de una máquina o software para aprender mediante algoritmos de su programación respecto a cierta entrada de datos en su sistema. Un sistema informático de Machine Lenarning (ML) utiliza la experiencias y evidencias en forma de datos a partir de un gran número de ejemplos de una situación, con los que obtiene por sí mismo patrones o comportamientos. De este modo, se pueden elaborar predicciones en diferentes escenarios o iniciar operaciones que son la solución para una tarea específica. El Machine Lenarning (ML) posee dos diferentes tipos el Aprendizaje Supervisado y No Supervisado, que veremos con más detalle a continuación. 6.1 Algoritmo Supervisados Los modelos de aprendizajes supervisados es una técnica para deducir una función a partir de datos de entrenamiento, en donde el entrenamiento actúa como una guía para enseñar al algoritmo los resultados a las que debe llegar, es decir la salida del algoritmo ya es conocida. Esto requiere que los posibles resultados del algoritmo ya sean conocidos y que los datos utilizados para entrenar el algoritmo ya estén etiquetados labels con las respuestas correctas. Un ejemplo es la clasificación de los correos entrante entre Spam o no. Entre las diversas características que queremos entrenar deberemos incluir si es correo basura o no con un 1 o un 0. Otro ejemplo es si queremos predecir unos valores numéricos como el precio de vivienda a partir de sus características. Es importante tener en cuenta lo siguiente; se consideran como un modelo de regresión, si la salida es un valor de un espacio continuo y de lo contrario si la salida es un valor categórico (por ejemplo, una enumeración, o un conjunto finito de clases) se considerará un modelo de clasificación, en otras palabras, cuando la variable sea discreta los llamaremos clasificación. En la siguiente imagen se muestra los diferentes tipos de algoritmos supervisados: 6.1.1 Vecinos cercanos (KNN) K-Nearest-Neighbor (Vecinos cercanos) es un método que busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de los datos que le rodean. Por ejemplo, Imaginemos que tenemos dos variables como input y como output nos da si es Clase A(naranjo) o Clase B (azul). Esta data es nuestra data de entrenamiento. Una vez que tenemos nuestra data de entrenamiento empezaremos a usar la data de test. Ahora como queremos predecir la clase output, veremos cómo uno de estos datos se vería visualmente y lo pintaremos de color amarillo. Para ello a continuación, calculamos la distancia entre este punto y los demás datos. Hemos trazado solo algunas distancias, pero podríamos hacerlo con todos. Para este ejemplo tomaremos los k = 4 vecinos más cercanos. Podemos observar que si nos enfocamos solo en los 3 vecinos más cercanos hay más naranjos que azules, entonces nuestra predicción será que este punto (demarcado con ?) ha de ser clase naranja. Al calcular la distancia en un plano cartesiano es podría ser más sencillo, solo tenemos variables como input: en el eje x y eje y. Sin embargo, la misma lógica se puede llevar a más variables. Ahora veremos lo anterior en el caso de múltiples variables con la data data frame iris. El famoso conjunto de datos de Ronald Fisher, que se encuentra incluido en todas las instalaciones de R. El dataset Iris se compone de 150 observaciones de flores de la planta iris. La cual existen tres tipos de clases de flores iris: virginica, setosa y versicolor(Hay 50 observaciones de cada una). Las variables o atributos que se miden de cada flor son: El tipo de flor como variable categórica. El largo y el ancho del pétalo en cm como variables numéricas. El largo y el ancho del sépalo en cm como variables numéricas. 6.1.1.1 Requisitos install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) #Bibliotecas library(datasets) library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.0.4 library(mlr) ## Warning: package &#39;mlr&#39; was built under R version 4.0.4 library(XML) Una vez instalada las librerías, se procede a llamar la base de datos iris, para luego ver a través del código str() las variables que la componen. 6.1.1.2 Analizar el dataset IRIS iris str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Se observa que posee 5 columnas de las cuales 4 son numéricas y una pertenece a una variable factor que corresponde a la especie, esta posee 3 niveles diferentes. para luego crear un as_tibble llamado iristib con la función as_tibble() que convierte un objeto existente, como un marco de datos o una matriz, en un llamado tibble, un marco de datos con clase tbl_df. irisTib &lt;- as_tibble(iris) Luego se realiza una summary() para ver la media, máxima, mínima etc. Estos nos sirves para ver si extiende datos fuera de rango o datos que ensucie la data. summary(irisTib) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Es importante tener en cuenta de que la base de datos debe estar balanceada, en el caso de dataset de iris esta se encuentra balanceada, ya que poseemos de 50 observaciones de cada especie, esto se realiza para tener mejor rendimiento del modelo. Y si se presenta el caso de que la base de datos no se encuentra balanceada, esta se debe balacear tomado cantidades de observaciones iguales de cada tipo de la variable que se utilizara como variable dependiente. 6.1.1.3 Visualizamos la data Veremos los datos con un gráfico para ver cómo se comportan los datos. ggplot(irisTib, aes(Sepal.Width, Sepal.Length, col=Species )) + geom_point() + theme_bw() realizamos un agrupar elementos por colores ggplot(irisTib, aes(Petal.Length, Petal.Width, col=Species)) + geom_point() + theme_bw() Se puede observar que entre las especies presentan distintas asociaciones, una forma rápida de visualizarlo es coloreando los puntos según el nivel del factor Species. Además, los datos presentan una estructura de asociación entre el largo de los sépalos y el de los pétalos es decir que a mayor largo de sépalos, mayor largo de pétalos. 6.1.1.4 Crear el clasificador irisTask &lt;- makeClassifTask(id=&quot;iris&quot;, data = irisTib, target = &quot;Species&quot;) ## Warning in makeTask(type = type, data = data, weights = weights, blocking = ## blocking, : Provided data is not a pure data.frame but from class tbl_df, hence ## it will be converted. irisTask ## Supervised task: iris ## Type: classif ## Target: Species ## Observations: 150 ## Features: ## numerics factors ordered functionals ## 4 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 3 ## setosa versicolor virginica ## 50 50 50 ## Positive class: NA 6.1.1.5 Entrenamiento y pruebas Un modelo generalmente se entrena en un subconjunto de datos, y el resto de la data se utiliza para evaluar su desempeño. n=getTaskSize(irisTask) n ## [1] 150 Primero obtenemos a través de getTaskSize() el número de observaciones en la tarea. En este caso n = 150. ratio=3/4 Luego llamaremos set.seed() para que estos resultados sean replicables. Esto es para generar números aleatorios replicables. set.seed(1234) train = sample(1:n, n*ratio) train ## [1] 28 80 101 111 137 133 144 132 98 103 90 70 79 116 14 126 62 4 ## [19] 143 40 93 122 5 66 135 47 131 123 84 48 108 3 87 41 115 100 ## [37] 72 32 42 43 2 138 54 49 102 56 51 6 107 130 96 106 57 8 ## [55] 26 17 63 97 22 35 117 149 119 86 142 10 55 92 25 88 50 139 ## [73] 20 140 94 71 61 104 109 27 121 60 65 36 150 19 9 134 30 52 ## [91] 95 38 83 141 21 105 113 13 69 110 118 73 16 11 67 91 146 46 ## [109] 129 59 89 64 Revisamos la compasión de train y podemos ver que está compuesto por números enteros y son 112 observaciones. str(train) ## int [1:112] 28 80 101 111 137 133 144 132 98 103 ... Para luego calcular la diferencia de conjuntos (no simétricos) de subconjuntos de un espacio de probabilidad. test = setdiff(1:n, train) test ## [1] 1 7 12 15 18 23 24 29 31 33 34 37 39 44 45 53 58 68 74 ## [20] 75 76 77 78 81 82 85 99 112 114 120 124 125 127 128 136 145 147 148 6.1.1.6 Determinamos el Clasificador Es impórtate tener en cuenta antes de escoger el clasificador la siguiente tabla que muestra los tipos de classif existen. clase paquete Num Fac Ord Nas Pesos Soporta classif.kknn kknn x x prob, dos clase, clase multiclase classif.knn clase x dos clase, multiclase Por ejemplo, si tenemos un data que posee tantos variables numéricas y factor usaremos el clasificador classif.kknn , pero si es caso de que nuestras variables son numéricas usaremos el clasificador classif.knn. Según las características de nuestra base de datos Iris usaremos classif.knn. Crearemos un clasificador que busque los 3 más cercanos al punto con K =3, esto lo hacemos con makeLearner(\"classif.knn\", k = 3). Se debe Tener en cuenta que classif.knn se llama desde el classpaquete a través de mlr. knn = makeLearner(&quot;classif.knn&quot;, k=3) knn ## Learner classif.knn from package class ## Type: classif ## Name: k-Nearest Neighbor; Short name: knn ## Class: classif.knn ## Properties: twoclass,multiclass,numerics ## Predict-Type: response ## Hyperparameters: k=3 Luego creamos el modelo con el dataset de entrenamiento con train() primero colocamos en clasificador creado knn, luego usamos iristask y por último train que fue creado anteriormente. mod = train(knn, irisTask, subset = train) mod ## Model for learner.id=classif.knn; learner.class=classif.knn ## Trained on: task.id = iris; obs = 112; features = 4 ## Hyperparameters: k=3 Podemos ver que tenemos 112 observaciones con 4 variables con un valor de k=3. 6.1.1.7 Predicciones preds = predict(mod, irisTask, subset = test) preds ## Prediction: 38 observations ## predict.type: response ## threshold: ## time: 0.00 ## id truth response ## 1 1 setosa setosa ## 7 7 setosa setosa ## 12 12 setosa setosa ## 15 15 setosa setosa ## 18 18 setosa setosa ## 23 23 setosa setosa ## ... (#rows: 38, #cols: 3) Evaluemos las predicciones Se calcula varias medidas de rendimiento a la vez como: * acc: define la precisión general como la probabilidad de correspondencia entre una decisión positiva y una condición verdadera (es decir, la proporción de decisiones de clasificación correctas o de casos). * mmce(Error medio de clasificación errónea). ms = list(&quot;mmce&quot; = mmce, &quot;acc&quot; = acc, &quot;timetrain&quot; = timetrain) La función performance() mide la calidad de una predicción y está compuesta por los siguientes argumentos; pred como el objeto de predicción; measures es la Medida(s) de rendimiento a evaluar, es decir el valor predeterminado es la medida predeterminada para la tarea; task es la tarea de aprendizaje, podría solicitarse por medida de rendimiento, por lo general no es necesario, excepto para la agrupación en clústeres o la supervivencia; model Modelo basado en datos de entrenamiento, podría solicitarse por medida de rendimiento, por lo general no es necesario, excepto para la supervivencia. performance(preds, measures = ms, task = irisTask, mod) ## mmce acc timetrain ## 0.1052632 0.8947368 0.0000000 Con un error de clasificación errónea media de 0.1052632 (el más bajo) y acc de 0.8947368. 6.1.1.8 Matriz de confusión Ahora realizaremos una matriz de confusión, para ver cuántos valores predichos fueron iguales a los reales utilizando la función calculateConfusionMatrix(). Esta función nos permite visualizar el rendimiento de un algoritmo, usado generalmente en un aprendizaje supervisado (en cuanto aprendizaje no supervisado generalmente se denomina matriz coincidente). Como se observa en el siguiente código cada fila de la matriz representa las instancias en una clase predicha, mientras que cada columna representa las instancias en una clase real (o viceversa). El nombre se deriva del hecho de que hace que sea fácil ver si el sistema confunde dos clases (etiquetar incorrectamente una como otra). calculateConfusionMatrix(preds, relative = FALSE) ## predicted ## true setosa versicolor virginica -err.- ## setosa 15 0 0 0 ## versicolor 0 11 1 1 ## virginica 0 3 8 3 ## -err.- 0 3 1 4 Interpretemos celda a celda este resultado: El modelo kNN predijo 15 valores como especie setosa y resulta que en nuestra prueba el valor real, output era también setosa. Además el modelo predijo 12 como especie versicolor. Sin embargo, en la data real-test, de esos 12, solo 11 son versicolor y 1 son virginica. El modelo predijo 12 como especie virginica. Sin embargo, en la data real-test, de esos 16, solo 8 son virginica y 3 versicolor. 6.1.2 Regresiones logística Es un método estadístico para predecir clases binarias, el resultado es llamado como variable objetivo y es de naturaleza dicotómica, esto quiere decir que solo hay dos clases posibles. La Regresión Logística es uno de los algoritmos de Machine Learning más simples y utilizados para la clasificación de dos clases, esta describe y estima la relación entre una variable binaria dependiente y las variables independientes. 6.1.2.1 requisitos install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) library(mlr) library(tidyverse) 6.1.2.2 Analizar el dataset titanic Queremos saber si los factores socioeconómicos influyeron en la probabilidad de una persona de sobrevivir al desastre. Por lo tanto, el objetivo es construir un modelo de regresión logística binomial para predecir si un pasajero sobreviví al desastre del Titanic, según datos como su género y cómo Cuánto pagaron por su boleto. A continuación, instalaremos la base de datos del Titanic . install.packages(&quot;titanic&quot;) data(titanic_train, package = &quot;titanic&quot;) Ahora carguemos los datos, que están integrados en el paquete Titanic, convertiremos la base con la función as_tibble (). titanicTib &lt;- as_tibble(titanic_train) Ahora veremos la visualización de la data Titanic. Tenemos un tibble que contiene 891 casos (observaciones) y que contiene 12 variables. El objetivo de realizar este modelo es para predecir si un pasajero tiene posibilidad de sobrevivir al desastre utilizando la información en estas variables. str(titanicTib) ## tibble [891 x 12] (S3: tbl_df/tbl/data.frame) ## $ PassengerId: int [1:891] 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : int [1:891] 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : int [1:891] 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : chr [1:891] &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ... ## $ Sex : chr [1:891] &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Age : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ... ## $ SibSp : int [1:891] 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int [1:891] 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : chr [1:891] &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ... ## $ Fare : num [1:891] 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr [1:891] &quot;&quot; &quot;C85&quot; &quot;&quot; &quot;C123&quot; ... ## $ Embarked : chr [1:891] &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ... la base contiene las siguientes variables: Variable descripción PassengerId Un número arbitrario único para cada pasajero. Sobrevivido Un número entero que denota supervivencia (1 = sobrevivió, 0 = murió) Pclass Si el pasajero estaba alojado en primera, segunda o tercera clase Nombre Un vector de caracteres de los nombres de los pasajeros Sexo Un vector de caracteres que contiene masculino y femenino Edad La edad del pasajero SibSp El número combinado de hermanos y cónyuges a bordo Parch El número combinado de padres e hijos a bordo Boleto Un vector de caracteres con el número de boleto de cada pasajero arifa La cantidad de dinero que cada pasajero pagó por su boleto Cabina Un vector de caracteres del número de cabina de cada pasajero Embarked Un vector de caracteres del que los pasajeros del puerto se embarcaron Convertir datos a factores Ahora necesitamos limpiar la data antes de que podamos pasarlo al algoritmo de regresión logística. Lo primero que haremos es cambiar las variables Survived, Sex y Pclass a factor como se muestra a continuación. fctrs &lt;- c(&quot;Survived&quot;, &quot;Sex&quot;, &quot;Pclass&quot;) Para convertirlas usaremos función mutate_at(), que nos permite mutar varios factores a la vez. Suministramos las variables existentes como vector de caracteres al argumento .vars que en este caso es la anterior base creada con las columna llamada fctrsy para luego utilizar el argumento .funs para que las trasforme a factor. Posteriormente utilizamos la función mutate() para poder crear una nueva variable llamada FamSize que sería la suma SibSp (el número combinado de hermanos y cónyuges a bordo) y Parch (el número combinado de padres e hijos a bordo). Para finalizar con la función select() el cual nos ayuda a seleccionar solo las variables que creemos pueden tener algún valor predictivo para nuestro modelo, las cuales son las variables Survived, Pclass, Sex, Age, Fare y FamSize. titanicClean &lt;- titanicTib %&gt;% mutate_at(.vars = fctrs, .funs = factor) %&gt;% mutate(FamSize = SibSp + Parch) %&gt;% select(Survived, Pclass, Sex, Age, Fare, FamSize) Ahora visualizamos el cambio realizado anteriormente. Y utilizamos str() para ver cómo están constituidas las variables de la data, y como se observas las variables cambiaron a factor . str(titanicClean) ## tibble [891 x 6] (S3: tbl_df/tbl/data.frame) ## $ Survived: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 1 1 1 2 2 ... ## $ Pclass : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ Age : num [1:891] 22 38 26 35 35 NA 54 2 27 14 ... ## $ Fare : num [1:891] 7.25 71.28 7.92 53.1 8.05 ... ## $ FamSize : int [1:891] 1 1 0 1 0 0 0 4 2 1 ... Además, tenemos nuestra nueva variable, FamSize y hemos eliminado las variables irrelevantes. 6.1.2.3 Graficamos algunos datos Ahora para poder graficar usaremos los siguientes códigos. titanicUntidy &lt;- gather(titanicClean, key = &quot;Variable&quot;, value = &quot;Value&quot;, -Survived) ## Warning: attributes are not identical across measure variables; ## they will be dropped titanicUntidy %&gt;% filter(Variable != &quot;Pclass&quot; &amp; Variable != &quot;Sex&quot;) %&gt;% ggplot(aes(Survived, as.numeric(Value))) + facet_wrap(~ Variable, scales = &quot;free_y&quot;) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + theme_bw() ## Warning: Removed 177 rows containing non-finite values (stat_ydensity). ## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): ## collapsing to unique &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): ## collapsing to unique &#39;x&#39; values ## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): ## collapsing to unique &#39;x&#39; values titanicUntidy %&gt;% filter(Variable == &quot;Pclass&quot; | Variable == &quot;Sex&quot;) %&gt;% ggplot(aes(Value, fill = Survived)) + facet_wrap(~ Variable, scales = &quot;free_x&quot;) + geom_bar(position = &quot;fill&quot;) + theme_bw() Si desea imputar NAs todas las características de números enteros por la media. imp &lt;- impute(titanicClean, cols = list(Age = imputeMean())) impute() devuelve una list() que contiene el conjunto de datos imputados. Esto quiere decir de de la anterior data set titanicClean cambian los datos vacíos por el promedio de las edades para no afectar al a data. Crear una tarea De clasificación en la cual la tarea encapsula los datos y específica, a través de sus subclases, el tipo de tarea que en este caso es de clasificación usaremos la función makeClassifTask. Para la clasificación de etiquetas múltiples, asumimos que la presencia de etiquetas se codifica mediante columnas lógicas en data. El nombre de la columna especifica el nombre de la etiqueta. target . titanicTask &lt;- makeClassifTask(data = imp$data, target = &quot;Survived&quot;) Ahora verificamos si los cambios realizados anteriormente fueron realizados. imp$data$Age ## [1] 22.00000 38.00000 26.00000 35.00000 35.00000 29.69912 54.00000 2.00000 ## [9] 27.00000 14.00000 4.00000 58.00000 20.00000 39.00000 14.00000 55.00000 ## [17] 2.00000 29.69912 31.00000 29.69912 35.00000 34.00000 15.00000 28.00000 ## [25] 8.00000 38.00000 29.69912 19.00000 29.69912 29.69912 40.00000 29.69912 ## [33] 29.69912 66.00000 28.00000 42.00000 29.69912 21.00000 18.00000 14.00000 ## [41] 40.00000 27.00000 29.69912 3.00000 19.00000 29.69912 29.69912 29.69912 ## [49] 29.69912 18.00000 7.00000 21.00000 49.00000 29.00000 65.00000 29.69912 ## [57] 21.00000 28.50000 5.00000 11.00000 22.00000 38.00000 45.00000 4.00000 ## [65] 29.69912 29.69912 29.00000 19.00000 17.00000 26.00000 32.00000 16.00000 ## [73] 21.00000 26.00000 32.00000 25.00000 29.69912 29.69912 0.83000 30.00000 ## [81] 22.00000 29.00000 29.69912 28.00000 17.00000 33.00000 16.00000 29.69912 ## [89] 23.00000 24.00000 29.00000 20.00000 46.00000 26.00000 59.00000 29.69912 ## [97] 71.00000 23.00000 34.00000 34.00000 28.00000 29.69912 21.00000 33.00000 ## [105] 37.00000 28.00000 21.00000 29.69912 38.00000 29.69912 47.00000 14.50000 ## [113] 22.00000 20.00000 17.00000 21.00000 70.50000 29.00000 24.00000 2.00000 ## [121] 21.00000 29.69912 32.50000 32.50000 54.00000 12.00000 29.69912 24.00000 ## [129] 29.69912 45.00000 33.00000 20.00000 47.00000 29.00000 25.00000 23.00000 ## [137] 19.00000 37.00000 16.00000 24.00000 29.69912 22.00000 24.00000 19.00000 ## [145] 18.00000 19.00000 27.00000 9.00000 36.50000 42.00000 51.00000 22.00000 ## [153] 55.50000 40.50000 29.69912 51.00000 16.00000 30.00000 29.69912 29.69912 ## [161] 44.00000 40.00000 26.00000 17.00000 1.00000 9.00000 29.69912 45.00000 ## [169] 29.69912 28.00000 61.00000 4.00000 1.00000 21.00000 56.00000 18.00000 ## [177] 29.69912 50.00000 30.00000 36.00000 29.69912 29.69912 9.00000 1.00000 ## [185] 4.00000 29.69912 29.69912 45.00000 40.00000 36.00000 32.00000 19.00000 ## [193] 19.00000 3.00000 44.00000 58.00000 29.69912 42.00000 29.69912 24.00000 ## [201] 28.00000 29.69912 34.00000 45.50000 18.00000 2.00000 32.00000 26.00000 ## [209] 16.00000 40.00000 24.00000 35.00000 22.00000 30.00000 29.69912 31.00000 ## [217] 27.00000 42.00000 32.00000 30.00000 16.00000 27.00000 51.00000 29.69912 ## [225] 38.00000 22.00000 19.00000 20.50000 18.00000 29.69912 35.00000 29.00000 ## [233] 59.00000 5.00000 24.00000 29.69912 44.00000 8.00000 19.00000 33.00000 ## [241] 29.69912 29.69912 29.00000 22.00000 30.00000 44.00000 25.00000 24.00000 ## [249] 37.00000 54.00000 29.69912 29.00000 62.00000 30.00000 41.00000 29.00000 ## [257] 29.69912 30.00000 35.00000 50.00000 29.69912 3.00000 52.00000 40.00000 ## [265] 29.69912 36.00000 16.00000 25.00000 58.00000 35.00000 29.69912 25.00000 ## [273] 41.00000 37.00000 29.69912 63.00000 45.00000 29.69912 7.00000 35.00000 ## [281] 65.00000 28.00000 16.00000 19.00000 29.69912 33.00000 30.00000 22.00000 ## [289] 42.00000 22.00000 26.00000 19.00000 36.00000 24.00000 24.00000 29.69912 ## [297] 23.50000 2.00000 29.69912 50.00000 29.69912 29.69912 19.00000 29.69912 ## [305] 29.69912 0.92000 29.69912 17.00000 30.00000 30.00000 24.00000 18.00000 ## [313] 26.00000 28.00000 43.00000 26.00000 24.00000 54.00000 31.00000 40.00000 ## [321] 22.00000 27.00000 30.00000 22.00000 29.69912 36.00000 61.00000 36.00000 ## [329] 31.00000 16.00000 29.69912 45.50000 38.00000 16.00000 29.69912 29.69912 ## [337] 29.00000 41.00000 45.00000 45.00000 2.00000 24.00000 28.00000 25.00000 ## [345] 36.00000 24.00000 40.00000 29.69912 3.00000 42.00000 23.00000 29.69912 ## [353] 15.00000 25.00000 29.69912 28.00000 22.00000 38.00000 29.69912 29.69912 ## [361] 40.00000 29.00000 45.00000 35.00000 29.69912 30.00000 60.00000 29.69912 ## [369] 29.69912 24.00000 25.00000 18.00000 19.00000 22.00000 3.00000 29.69912 ## [377] 22.00000 27.00000 20.00000 19.00000 42.00000 1.00000 32.00000 35.00000 ## [385] 29.69912 18.00000 1.00000 36.00000 29.69912 17.00000 36.00000 21.00000 ## [393] 28.00000 23.00000 24.00000 22.00000 31.00000 46.00000 23.00000 28.00000 ## [401] 39.00000 26.00000 21.00000 28.00000 20.00000 34.00000 51.00000 3.00000 ## [409] 21.00000 29.69912 29.69912 29.69912 33.00000 29.69912 44.00000 29.69912 ## [417] 34.00000 18.00000 30.00000 10.00000 29.69912 21.00000 29.00000 28.00000 ## [425] 18.00000 29.69912 28.00000 19.00000 29.69912 32.00000 28.00000 29.69912 ## [433] 42.00000 17.00000 50.00000 14.00000 21.00000 24.00000 64.00000 31.00000 ## [441] 45.00000 20.00000 25.00000 28.00000 29.69912 4.00000 13.00000 34.00000 ## [449] 5.00000 52.00000 36.00000 29.69912 30.00000 49.00000 29.69912 29.00000 ## [457] 65.00000 29.69912 50.00000 29.69912 48.00000 34.00000 47.00000 48.00000 ## [465] 29.69912 38.00000 29.69912 56.00000 29.69912 0.75000 29.69912 38.00000 ## [473] 33.00000 23.00000 22.00000 29.69912 34.00000 29.00000 22.00000 2.00000 ## [481] 9.00000 29.69912 50.00000 63.00000 25.00000 29.69912 35.00000 58.00000 ## [489] 30.00000 9.00000 29.69912 21.00000 55.00000 71.00000 21.00000 29.69912 ## [497] 54.00000 29.69912 25.00000 24.00000 17.00000 21.00000 29.69912 37.00000 ## [505] 16.00000 18.00000 33.00000 29.69912 28.00000 26.00000 29.00000 29.69912 ## [513] 36.00000 54.00000 24.00000 47.00000 34.00000 29.69912 36.00000 32.00000 ## [521] 30.00000 22.00000 29.69912 44.00000 29.69912 40.50000 50.00000 29.69912 ## [529] 39.00000 23.00000 2.00000 29.69912 17.00000 29.69912 30.00000 7.00000 ## [537] 45.00000 30.00000 29.69912 22.00000 36.00000 9.00000 11.00000 32.00000 ## [545] 50.00000 64.00000 19.00000 29.69912 33.00000 8.00000 17.00000 27.00000 ## [553] 29.69912 22.00000 22.00000 62.00000 48.00000 29.69912 39.00000 36.00000 ## [561] 29.69912 40.00000 28.00000 29.69912 29.69912 24.00000 19.00000 29.00000 ## [569] 29.69912 32.00000 62.00000 53.00000 36.00000 29.69912 16.00000 19.00000 ## [577] 34.00000 39.00000 29.69912 32.00000 25.00000 39.00000 54.00000 36.00000 ## [585] 29.69912 18.00000 47.00000 60.00000 22.00000 29.69912 35.00000 52.00000 ## [593] 47.00000 29.69912 37.00000 36.00000 29.69912 49.00000 29.69912 49.00000 ## [601] 24.00000 29.69912 29.69912 44.00000 35.00000 36.00000 30.00000 27.00000 ## [609] 22.00000 40.00000 39.00000 29.69912 29.69912 29.69912 35.00000 24.00000 ## [617] 34.00000 26.00000 4.00000 26.00000 27.00000 42.00000 20.00000 21.00000 ## [625] 21.00000 61.00000 57.00000 21.00000 26.00000 29.69912 80.00000 51.00000 ## [633] 32.00000 29.69912 9.00000 28.00000 32.00000 31.00000 41.00000 29.69912 ## [641] 20.00000 24.00000 2.00000 29.69912 0.75000 48.00000 19.00000 56.00000 ## [649] 29.69912 23.00000 29.69912 18.00000 21.00000 29.69912 18.00000 24.00000 ## [657] 29.69912 32.00000 23.00000 58.00000 50.00000 40.00000 47.00000 36.00000 ## [665] 20.00000 32.00000 25.00000 29.69912 43.00000 29.69912 40.00000 31.00000 ## [673] 70.00000 31.00000 29.69912 18.00000 24.50000 18.00000 43.00000 36.00000 ## [681] 29.69912 27.00000 20.00000 14.00000 60.00000 25.00000 14.00000 19.00000 ## [689] 18.00000 15.00000 31.00000 4.00000 29.69912 25.00000 60.00000 52.00000 ## [697] 44.00000 29.69912 49.00000 42.00000 18.00000 35.00000 18.00000 25.00000 ## [705] 26.00000 39.00000 45.00000 42.00000 22.00000 29.69912 24.00000 29.69912 ## [713] 48.00000 29.00000 52.00000 19.00000 38.00000 27.00000 29.69912 33.00000 ## [721] 6.00000 17.00000 34.00000 50.00000 27.00000 20.00000 30.00000 29.69912 ## [729] 25.00000 25.00000 29.00000 11.00000 29.69912 23.00000 23.00000 28.50000 ## [737] 48.00000 35.00000 29.69912 29.69912 29.69912 36.00000 21.00000 24.00000 ## [745] 31.00000 70.00000 16.00000 30.00000 19.00000 31.00000 4.00000 6.00000 ## [753] 33.00000 23.00000 48.00000 0.67000 28.00000 18.00000 34.00000 33.00000 ## [761] 29.69912 41.00000 20.00000 36.00000 16.00000 51.00000 29.69912 30.50000 ## [769] 29.69912 32.00000 24.00000 48.00000 57.00000 29.69912 54.00000 18.00000 ## [777] 29.69912 5.00000 29.69912 43.00000 13.00000 17.00000 29.00000 29.69912 ## [785] 25.00000 25.00000 18.00000 8.00000 1.00000 46.00000 29.69912 16.00000 ## [793] 29.69912 29.69912 25.00000 39.00000 49.00000 31.00000 30.00000 30.00000 ## [801] 34.00000 31.00000 11.00000 0.42000 27.00000 31.00000 39.00000 18.00000 ## [809] 39.00000 33.00000 26.00000 39.00000 35.00000 6.00000 30.50000 29.69912 ## [817] 23.00000 31.00000 43.00000 10.00000 52.00000 27.00000 38.00000 27.00000 ## [825] 2.00000 29.69912 29.69912 1.00000 29.69912 62.00000 15.00000 0.83000 ## [833] 29.69912 23.00000 18.00000 39.00000 21.00000 29.69912 32.00000 29.69912 ## [841] 20.00000 16.00000 30.00000 34.50000 17.00000 42.00000 29.69912 35.00000 ## [849] 28.00000 29.69912 4.00000 74.00000 9.00000 16.00000 44.00000 18.00000 ## [857] 45.00000 51.00000 24.00000 29.69912 41.00000 21.00000 48.00000 29.69912 ## [865] 24.00000 42.00000 27.00000 31.00000 29.69912 4.00000 26.00000 47.00000 ## [873] 33.00000 47.00000 28.00000 15.00000 20.00000 19.00000 29.69912 56.00000 ## [881] 25.00000 33.00000 22.00000 28.00000 25.00000 39.00000 27.00000 19.00000 ## [889] 29.69912 26.00000 32.00000 titanicTask ## Supervised task: imp$data ## Type: classif ## Target: Survived ## Observations: 891 ## Features: ## numerics factors ordered functionals ## 3 2 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ## 0 1 ## 549 342 ## Positive class: 0 Podemos observar que hay 5 variables de las cuales 3 son numéricas y dos son factor, también podemos observar que la data no presenta ningún dato vacío o perdido. A continuación se crea el modelo logístico, la base de data la llamaremos logReg . logReg &lt;- makeLearner(&quot;classif.logreg&quot;, predict.type = &quot;prob&quot;) 6.1.2.4 Entrenamos el modelo Para extraer los parámetros del modelo, primero debemos convertir nuestro mlr objeto modelo, logRegModel, en un objeto modelo usando getLearnerModel (). A continuación, pasamos este objeto de modelo como argumento de la función coef (), que significa coeficientes (otro término para parámetros), por lo que esta función devuelve los parámetros del modelo. logRegModel &lt;- train(logReg, titanicTask) logRegModel ## Model for learner.id=classif.logreg; learner.class=classif.logreg ## Trained on: task.id = imp$data; obs = 891; features = 5 ## Hyperparameters: model=FALSE Podemos ver que la data posee 891 observaciones y 5 variables diferentes, y que el tipo de clasificador que se utilizara en este caso es el de classif.logreg . 6.1.2.5 Validamos la precisión del modelo La función makeImputeWrapper () envuelve a un alumno y un método de imputación.Usaremos \"classif.logreg\" por cómo está constituida la data lo que se explica más adelante . logRegWrapper &lt;- makeImputeWrapper(&quot;classif.logreg&quot;, cols = list(Age = imputeMean())) kFold &lt;- makeResampleDesc(method = &quot;RepCV&quot;, folds = 10, reps = 50, stratify = TRUE) Ahora apliquemos una validación cruzada estratificada de diez veces, repetida 50 veces, a nuestro paquete aprendiz. Porque estamos proporcionando nuestro alumno empaquetado a la función resample (), para cada pliegue de la validación cruzada, la media de la variable Edad en el conjunto de entrenamiento será utilizado para imputar los valores perdidos. logRegwithImpute &lt;- resample(logRegWrapper, titanicTask, resampling = kFold, measures = list(acc, fpr, fnr)) Como se trata de un problema de clasificación de dos clases, tenemos acceso a algunos resultados adicionales métricas, como la tasa de falsos positivos (fpr) y la tasa de falsos negativos (fnr). Podemos ver que, aunque en promedio entre las repeticiones, el modelo clasificó correctamente al 79,6% de los pasajeros, clasificó incorrectamente 29,9% de los pasajeros que murieron y los pasó como sobrevivientes (falsos positivos) e incorrectamente clasificaron al 14,4% de los pasajeros que sobrevivieron como muertos (falsos negativos). 6.1.2.6 Extraemos los Odd Ratios Para extraer los parámetros del modelo, primero debemos convertir nuestro mlr objeto modelo, logRegModel, en un objeto modelo R usando función getLearnerModel (). A continuación, pasamos este objeto de modelo R como argumento de la función coef (), que significa coeficientes (otro término para parámetros), por lo que esta función devuelve los parámetros del modelo. logRegModelData &lt;- getLearnerModel(logRegModel) coef(logRegModelData) ## (Intercept) Pclass2 Pclass3 Sexmale Age Fare ## 3.809661697 -1.000344806 -2.132428850 -2.775928255 -0.038822458 0.003218432 ## FamSize ## -0.243029114 La intersección es el registro de probabilidades de sobrevivir al desastre del Titanic cuando todas las variables continuas son 0 y los factores están en sus niveles de referencia. Tendemos a estar más interesados en las pendientes que la intersección con el eje y, pero estos valores están en unidades logarítmicas de probabilidades, que son difíciles de interpretar. Por ejemplo, si las probabilidades de sobrevivir al Titanic, en el caso de que seas mujer son entre 7 y 10, en cuanto a si eres hombre son De 2 a 10, por lo que la razón de posibilidades de sobrevivir si eres mujer es de 3,5. En otras palabras, si fueras mujer, habrías tenido 3,5 veces más probabilidades de sobrevivir que si fuera hombre. Las razones de probabilidad son una forma muy popular de interpretar el impacto de los predictores en un resultado, porque se entienden fácilmente. Hacemos predicciones sobre nueva data Ahora realizaremos predicciones sobre una nueva data, primero carguemos algunos datos de pasajeros sin etiquetar, luego realizaremos una limpieza para dejar lista la data para la predicción y lo pasaremos a través de nuestro modelo. data(titanic_test, package = &quot;titanic&quot;) titanicNew &lt;- as_tibble(titanic_test) str(titanicNew) ## tibble [418 x 11] (S3: tbl_df/tbl/data.frame) ## $ PassengerId: int [1:418] 892 893 894 895 896 897 898 899 900 901 ... ## $ Pclass : int [1:418] 3 3 2 3 3 3 3 2 3 3 ... ## $ Name : chr [1:418] &quot;Kelly, Mr. James&quot; &quot;Wilkes, Mrs. James (Ellen Needs)&quot; &quot;Myles, Mr. Thomas Francis&quot; &quot;Wirz, Mr. Albert&quot; ... ## $ Sex : chr [1:418] &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; ... ## $ Age : num [1:418] 34.5 47 62 27 22 14 30 26 18 21 ... ## $ SibSp : int [1:418] 0 1 0 0 1 0 0 1 0 2 ... ## $ Parch : int [1:418] 0 0 0 0 1 0 0 1 0 0 ... ## $ Ticket : chr [1:418] &quot;330911&quot; &quot;363272&quot; &quot;240276&quot; &quot;315154&quot; ... ## $ Fare : num [1:418] 7.83 7 9.69 8.66 12.29 ... ## $ Cabin : chr [1:418] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Embarked : chr [1:418] &quot;Q&quot; &quot;S&quot; &quot;Q&quot; &quot;S&quot; ... titanicNewClean &lt;- titanicNew %&gt;% mutate_at(.vars = c(&quot;Sex&quot;, &quot;Pclass&quot;), .funs = factor) %&gt;% mutate(FamSize = SibSp + Parch) %&gt;% select(Pclass, Sex, Age, Fare, FamSize) predict(logRegModel, newdata = titanicNewClean) ## Warning in predict.WrappedModel(logRegModel, newdata = titanicNewClean): ## Provided data for prediction is not a pure data.frame but from class tbl_df, ## hence it will be converted. ## Prediction: 418 observations ## predict.type: prob ## threshold: 0=0.50,1=0.50 ## time: 0.00 ## prob.0 prob.1 response ## 1 0.9178036 0.08219636 0 ## 2 0.5909570 0.40904305 0 ## 3 0.9123303 0.08766974 0 ## 4 0.8927383 0.10726167 0 ## 5 0.4069407 0.59305933 1 ## 6 0.8337609 0.16623907 0 ## ... (#rows: 418, #cols: 3) 6.1.2.7 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos Soporta nota classif.logreg stats x x x prob, dos clase Delegados a con . Establecemos model en FALSE de forma predeterminada para ahorrar memoria. glm family = binomial(link = 'logit') classif. LiblineaRL1LogReg LiblineaR x dos clase, multiclase, prob, class.weights classif. LiblineaRL2LogReg LiblineaR x dos clase, multiclase, prob, class.weights type = 0 (el valor predeterminado) es primario y es doble problema.type = 7 6.1.3 Modelo de máquina de soporte vectorial (SVM) El Modelo de máquina de soporte vectorial fue creado por Vladimir Vapnik, es un método basado en aprendizaje para la resolución de problemas de clasificación y regresión. En ambos casos, esta resolución se basa en una primera fase de entrenamiento en donde se les informa con múltiples ejemplos ya resueltos y una segunda fase de uso para la resolución de problemas. En ella, las SVM se convierten en una caja que proporciona una respuesta (salida) a un problema dado (entrada), como lo muestra en la siguiente imagen. 6.1.3.1 Requisitos install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) #Bibliotecas library(datasets) library(tidyverse) library(XML) library(kernlab) library(mlr) library(parallelMap) library(parallel) 6.1.3.2 Cargaremos base de datos Para demostrar las Máquinas de Vectores Soporte usaremos la base de datos spam, la cual es un conjunto de datos recopilados en Hewlett-Packard Labs, que clasifica 4601 correos electrónicos como spam o no spam. Además de esta etiqueta de clase, hay 57 variables que indican la frecuencia de ciertas palabras y caracteres en el correo electrónico. Las primeras 48 variables contienen la frecuencia del nombre de la variable (por ejemplo, empresa) en el correo electrónico. Si el nombre de la variable comienza con num (por ejemplo, num650), indica la frecuencia del número correspondiente (por ejemplo, 650). Las variables 49-54 indican la frecuencia de los caracteres ; (, [, !  $ Y  #. Las variables 55-57 contienen el promedio, el más largo y el total largo de las letras mayúsculas La variable 58 indica el tipo de correo y es nonspamo spam, es decir, correo electrónico comercial no solicitado. A continuación, llamaremos la base de datos. #Obtenemos los datos data(spam, package = &quot;kernlab&quot;) spam ## Warning in instance$preRenderHook(instance): It seems your data is too big ## for client-side DataTables. You may consider server-side processing: https:// ## rstudio.github.io/DT/server.html Pasamos la base spam la convertimos en as_tibble creamos una nueva data frame con el nombre de spamTib. spamTib &lt;- as_tibble(spam) spamTib ## # A tibble: 4,601 x 58 ## make address all num3d our over remove internet order mail receive ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.64 0.64 0 0.32 0 0 0 0 0 0 ## 2 0.21 0.28 0.5 0 0.14 0.28 0.21 0.07 0 0.94 0.21 ## 3 0.06 0 0.71 0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 ## 4 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 ## 5 0 0 0 0 0.63 0 0.31 0.63 0.31 0.63 0.31 ## 6 0 0 0 0 1.85 0 0 1.85 0 0 0 ## 7 0 0 0 0 1.92 0 0 0 0 0.64 0.96 ## 8 0 0 0 0 1.88 0 0 1.88 0 0 0 ## 9 0.15 0 0.46 0 0.61 0 0.3 0 0.92 0.76 0.76 ## 10 0.06 0.12 0.77 0 0.19 0.32 0.38 0 0.06 0 0 ## # ... with 4,591 more rows, and 47 more variables: will &lt;dbl&gt;, people &lt;dbl&gt;, ## # report &lt;dbl&gt;, addresses &lt;dbl&gt;, free &lt;dbl&gt;, business &lt;dbl&gt;, email &lt;dbl&gt;, ## # you &lt;dbl&gt;, credit &lt;dbl&gt;, your &lt;dbl&gt;, font &lt;dbl&gt;, num000 &lt;dbl&gt;, money &lt;dbl&gt;, ## # hp &lt;dbl&gt;, hpl &lt;dbl&gt;, george &lt;dbl&gt;, num650 &lt;dbl&gt;, lab &lt;dbl&gt;, labs &lt;dbl&gt;, ## # telnet &lt;dbl&gt;, num857 &lt;dbl&gt;, data &lt;dbl&gt;, num415 &lt;dbl&gt;, num85 &lt;dbl&gt;, ## # technology &lt;dbl&gt;, num1999 &lt;dbl&gt;, parts &lt;dbl&gt;, pm &lt;dbl&gt;, direct &lt;dbl&gt;, ## # cs &lt;dbl&gt;, meeting &lt;dbl&gt;, original &lt;dbl&gt;, project &lt;dbl&gt;, re &lt;dbl&gt;, ## # edu &lt;dbl&gt;, table &lt;dbl&gt;, conference &lt;dbl&gt;, charSemicolon &lt;dbl&gt;, ## # charRoundbracket &lt;dbl&gt;, charSquarebracket &lt;dbl&gt;, charExclamation &lt;dbl&gt;, ## # charDollar &lt;dbl&gt;, charHash &lt;dbl&gt;, capitalAve &lt;dbl&gt;, capitalLong &lt;dbl&gt;, ## # capitalTotal &lt;dbl&gt;, type &lt;fct&gt; Ahora veremos las variables y como estan constituidas con str(). str(spamTib) ## tibble [4,601 x 58] (S3: tbl_df/tbl/data.frame) ## $ make : num [1:4601] 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ... ## $ address : num [1:4601] 0.64 0.28 0 0 0 0 0 0 0 0.12 ... ## $ all : num [1:4601] 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ... ## $ num3d : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ our : num [1:4601] 0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ... ## $ over : num [1:4601] 0 0.28 0.19 0 0 0 0 0 0 0.32 ... ## $ remove : num [1:4601] 0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ... ## $ internet : num [1:4601] 0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ... ## $ order : num [1:4601] 0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ... ## $ mail : num [1:4601] 0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ... ## $ receive : num [1:4601] 0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ... ## $ will : num [1:4601] 0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ... ## $ people : num [1:4601] 0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ... ## $ report : num [1:4601] 0 0.21 0 0 0 0 0 0 0 0 ... ## $ addresses : num [1:4601] 0 0.14 1.75 0 0 0 0 0 0 0.12 ... ## $ free : num [1:4601] 0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ... ## $ business : num [1:4601] 0 0.07 0.06 0 0 0 0 0 0 0 ... ## $ email : num [1:4601] 1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ... ## $ you : num [1:4601] 1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ... ## $ credit : num [1:4601] 0 0 0.32 0 0 0 0 0 3.53 0.06 ... ## $ your : num [1:4601] 0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ... ## $ font : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num000 : num [1:4601] 0 0.43 1.16 0 0 0 0 0 0 0.19 ... ## $ money : num [1:4601] 0 0.43 0.06 0 0 0 0 0 0.15 0 ... ## $ hp : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ hpl : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ george : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num650 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ lab : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ labs : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ telnet : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num857 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ data : num [1:4601] 0 0 0 0 0 0 0 0 0.15 0 ... ## $ num415 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num85 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ technology : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ num1999 : num [1:4601] 0 0.07 0 0 0 0 0 0 0 0 ... ## $ parts : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ pm : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ direct : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ cs : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ meeting : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ original : num [1:4601] 0 0 0.12 0 0 0 0 0 0.3 0 ... ## $ project : num [1:4601] 0 0 0 0 0 0 0 0 0 0.06 ... ## $ re : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ edu : num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ... ## $ table : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ conference : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ charSemicolon : num [1:4601] 0 0 0.01 0 0 0 0 0 0 0.04 ... ## $ charRoundbracket : num [1:4601] 0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ... ## $ charSquarebracket: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ... ## $ charExclamation : num [1:4601] 0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ... ## $ charDollar : num [1:4601] 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ... ## $ charHash : num [1:4601] 0 0.048 0.01 0 0 0 0 0 0.022 0 ... ## $ capitalAve : num [1:4601] 3.76 5.11 9.82 3.54 3.54 ... ## $ capitalLong : num [1:4601] 61 101 485 40 40 15 4 11 445 43 ... ## $ capitalTotal : num [1:4601] 278 1028 2259 191 191 ... ## $ type : Factor w/ 2 levels &quot;nonspam&quot;,&quot;spam&quot;: 2 2 2 2 2 2 2 2 2 2 ... Creamos la tarea de clasificación spamTask &lt;- makeClassifTask(data = spamTib, target = &quot;type&quot;) ## Warning in makeTask(type = type, data = data, weights = weights, blocking = ## blocking, : Provided data is not a pure data.frame but from class tbl_df, hence ## it will be converted. spamTask ## Supervised task: spamTib ## Type: classif ## Target: type ## Observations: 4601 ## Features: ## numerics factors ordered functionals ## 57 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE ## Classes: 2 ## nonspam spam ## 2788 1813 ## Positive class: nonspam El conjunto de datos contiene 2788 correos electrónicos clasificados como nonspamy 1813 clasificados como spam. El concepto de spam es diverso por lo que lo determinaremos como anuncios de productos / sitios web, esquemas para ganar dinero rápidamente, cartas en cadena, pornografía. 6.1.3.3 Creamos el clasificador Vamos a definir la tarea y aprendiz. Esta vez, suministramos classif. SVM como el argumento de makeLearner () para especificar que vamos a utilizar SVM. svm &lt;- makeLearner(&quot;classif.svm&quot;) svm ## Learner classif.svm from package e1071 ## Type: classif ## Name: Support Vector Machines (libsvm); Short name: svm ## Class: classif.svm ## Properties: twoclass,multiclass,numerics,factors,prob,class.weights ## Predict-Type: response ## Hyperparameters: 6.1.3.4 Hiperparametros del modelo Antes de entrenar el modelo, es necesario ajustar nuestros hiperparámetros. Para saber qué hiperparámetros se puede utilizar para un algoritmo, se debe usar getParamSet(). El primer argumento utilizado es el nombre de la hiperparámetro propuesta por getParamSet ( classif. svm  ) , entre comillas. getParamSet(&quot;classif.svm&quot;) ## Type len Def ## type discrete - C-classifica... ## cost numeric - 1 ## nu numeric - 0.5 ## class.weights numericvector &lt;NA&gt; - ## kernel discrete - radial ## degree integer - 3 ## coef0 numeric - 0 ## gamma numeric - - ## cachesize numeric - 40 ## tolerance numeric - 0.001 ## shrinking logical - TRUE ## cross integer - 0 ## fitted logical - TRUE ## scale logicalvector &lt;NA&gt; TRUE ## Constr Req Tunable Trafo ## type C-classification,nu-classification - TRUE - ## cost 0 to Inf Y TRUE - ## nu -Inf to Inf Y TRUE - ## class.weights 0 to Inf - TRUE - ## kernel linear,polynomial,radial,sigmoid - TRUE - ## degree 1 to Inf Y TRUE - ## coef0 -Inf to Inf Y TRUE - ## gamma 0 to Inf Y TRUE - ## cachesize -Inf to Inf - TRUE - ## tolerance 0 to Inf - TRUE - ## shrinking - - TRUE - ## cross 0 to Inf - FALSE - ## fitted - - FALSE - ## scale - - TRUE - Es importante tener en cuenta que algoritmo SVM es sensible a las variables que se encuentran en diferentes escalas, por lo que generalmente es una buena idea escalar los predictores primero. 6.1.3.5 Tipos de Kernesls kernels &lt;- c(&quot;polynomial&quot;, &quot;radial&quot;, &quot;sigmoid&quot;) Se utiliza la función makeParamSet () para definir el espacio de hiperparámetros que deseamos recoger, separados por columnas. * kernel hiperparámetro toma discretos valores (el nombre del kernel función) , por lo que utilizar el makeDiscreteParam () función para definir sus valores como el vector de kernels que creamos. * degree hiperparámetro toma valores enteros (números enteros) , por lo que utilizamos la función makeIntegerParam () y definir sus valores superior e inferior que deseamos sintonizar * cost y gamma hiperparámetros toman valores numéricos (cualquier número entre cero y el infinito) , por lo que utilizar la función makeNumericParam () para poder definir los valores superior e inferior que deseamos sintonizar. svmParamSpace &lt;- makeParamSet( makeDiscreteParam(&quot;kernel&quot;, values = kernels), makeIntegerParam(&quot;degree&quot;, lower = 1, upper = 3), makeNumericParam(&quot;cost&quot;, lower = 0.1, upper = 10), makeNumericParam(&quot;gamma&quot;, lower = 0.1, 10)) svmParamSpace ## Type len Def Constr Req Tunable Trafo ## kernel discrete - - polynomial,radial,sigmoid - TRUE - ## degree integer - - 1 to 3 - TRUE - ## cost numeric - - 0.1 to 10 - TRUE - ## gamma numeric - - 0.1 to 10 - TRUE - Digamos que queremos probar los valores de la hiperparámetros cost y gamma en pasos de 0.1 , que es de 100 valores de cada uno. Tenemos tres funciones del kernel y tres valores del hiperparámetro de degree. Si queremos entrenar con grandes números podemos emplear una técnica llamada búsqueda aleatoria. En lugar de intentar todas las combinaciones posibles de los parámetros, la búsqueda aleatoria procedería de la siguiente manera: * Seleccionar al azar una combinación de valores de hiperparámetros. * Usar validación cruzada para entrenar y evaluar un modelo usando los valores de hiperparámetros. * Registrar la métrica de rendimiento del modelo (por lo general significa la clasificación errónea error para tareas de clasificación) * Repetir ( Iterate ) los pasos 1 a 3 tantas veces como su presupuesto computacional permite * Seleccionar la combinación de los valores hiperparámetro que le dio la mejor modelo de desempeño A diferencia de rejilla de búsqueda, búsqueda aleatoria no está garantizado para encontrar el mejor conjunto de valores de hiperparámetros. Sin embargo, con suficientes iteraciones, se puede encontrar una buena combinación que funcione bien. Mediante el uso al azar en la búsqueda, podemos ejecutar 500 combinaciones de valores hiperparámetro. Vamos a definir nuestra búsqueda al azar utilizando la función makeTuneControlRandom (). Con esta determinamos la cantidad de iteraciones aleatorias que queremos usar en la búsqueda, con el argumento maxit. Es recomendable establecer un numero de iteraciones lo más alta posible que el dispositivo utilizado permita, pero en este caso solo usaremos 40 repeticiones con el fin de evitar que el ejemplo de tome demasiado tiempo. A continuación, describiremos el procedimiento de la validación cruzada. Por lo general se prefiere una cantidad de k veces de validación cruzada, para la retención de la salida transversal de validación, a menos que el procedimiento de formación sea computacionalmente costoso. randSearch &lt;- makeTuneControlRandom(maxit = 40) Se debe evaluar el desempeño de cada uno de estos modelos y se debe elegir el de mejor rendimiento. Se utilizó el procedimiento de búsqueda cuadrícula para tratar cada valor de k que definimos, durante el ajuste. Esto es lo que el método de búsqueda de rejilla hace: intenta todas las combinaciones de un hiperparámetro en un espacio definido, y encuentra la mejor combinación de rendimiento. cvForTuning &lt;- makeResampleDesc(&quot;Holdout&quot;, split = 2/3) Para ejecutar un proceso de MLR en paralelo, ponemos el código entre la parallelStartSocket () y parallelStop () que pertenecen al paquete paralelMap. Para iniciar nuestra hiperparámetro , llamamos en a la función tuneParams () y se suministra con los siguientes argumentos: el primer argumento es el nombre del alumno (\"classif.svm\"). task = el nombre de la tarea. resampling = el procedimiento de validación cruzada. par. set = el espacio de hiperparámetros . control = el procedimiento de búsqueda . 6.1.3.6 Realización del ajuste de hiperparámetros library(parallelMap) library(parallel) parallelStartSocket(cpus = detectCores()) ## Starting parallelization in mode=socket with cpus=4. tunedSvmPars &lt;- tuneParams(&quot;classif.svm&quot;, task = spamTask, resampling = cvForTuning, par.set = svmParamSpace, control = randSearch) ## [Tune] Started tuning learner classif.svm for parameter set: ## Type len Def Constr Req Tunable Trafo ## kernel discrete - - polynomial,radial,sigmoid - TRUE - ## degree integer - - 1 to 3 - TRUE - ## cost numeric - - 0.1 to 10 - TRUE - ## gamma numeric - - 0.1 to 10 - TRUE - ## With control class: TuneControlRandom ## Imputation value: 1 ## Exporting objects to slaves for mode socket: .mlr.slave.options ## Mapping in parallel: mode = socket; level = mlr.tuneParams; cpus = 4; elements = 40. ## [Tune] Result: kernel=polynomial; degree=1; cost=7.78; gamma=7.88 : mmce.test.mean=0.0723598 parallelStop() ## Stopped parallelization. All cleaned up. El hiperparámetro grado sólo se aplica al polinomio núcleo la función y el hiperparámetro gamma no se aplica a los lineales núcleos. Ahora extraeremos los valores de los hiperparámetros ganadores del ajuste. tunedSvmPars ## Tune result: ## Op. pars: kernel=polynomial; degree=1; cost=7.78; gamma=7.88 ## mmce.test.mean=0.0723598 Se puede imprimir el mejor valor del rendimiento del hiperparámetro y el rendimiento del modelo construido. llamando tunedSvm en la cual podemos ver que la primera polinomio núcleo grado de función (equivalente al núcleo lineal función) , con un cost de 2.23 y gamma de 7.99 , dio el modelo de mejor rendimiento. Y como un mmce de 0.0651890. 6.1.3.7 Entrenamiento del modelo con hiperparámetros ajustados Ahora hemos afinado los hiperparámetros, para luego construir el modelo de combinación con mejor rendimiento. Utilizamos las funciones setHyperPars ()para combinar un alumno con un conjunto de pre-definido valores de hiperparámetros, con el primer argumento como el alumno que queremos usar, y el par.vals El argumento es el que contiene nuestros valores de hiperparámetros ajustados. tunedSvm &lt;- setHyperPars(makeLearner(&quot;classif.svm&quot;), par.vals = tunedSvmPars$x) Luego entrenar un modelo utilizando nuestro tunedSvm alumno con el train (). tunedSvmModel &lt;- train(tunedSvm, spamTask) tunedSvmModel ## Model for learner.id=classif.svm; learner.class=classif.svm ## Trained on: task.id = spamTib; obs = 4601; features = 57 ## Hyperparameters: kernel=polynomial,degree=1,cost=7.78,gamma=7.88 6.1.3.8 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos Soporta nota classif.clusterSVM SwarmSVM, LiblineaR x x dos clase classif.dcSVM SwarmSVM, e1071 x dos clase classif.gaterSVM SwarmSVM x dos clase classif.ksvm kernlab x x classif.lssvm kernlab x x dos clase, multiclase, classif.svm e1071 x x x regr.ksvm kernlab x x regr.svm ke1071 x x x featimp ` clase paquete nota classif.clusterSVM centers establecido de forma predeterminada. classif.dcSVM Dos clase classif.gaterSVM m establecido y establecido en de forma predeterminada.3 max.iter1 classif.ksvm Dos clase, multiclase, prob, class.weights Los parámetros del núcleo deben pasarse directamente y no utilizando la lista en . Tenga en cuenta que se ha establecido de forma predeterminada para la velocidad. kpar ksvmf it FALSE classif.lssvm Dos clase, multiclase, fitted se ha establecido en por defecto para la velocidad.FALSE classif.svm Dos clase, multiclase, prob, class.weights regr.ksvm Los parámetros del núcleo deben pasarse directamente y no utilizando la lista en . Tenga en cuenta que se ha establecido de forma predeterminada para la velocidad. kpar ksvm fit FALSE regr.svm Todos los ajustes se pasan directamente, en lugar de a través del argumento s. se ha establecido en y en por defecto. xgboost params nrounds 1 verbose 0 . 6.2 Algoritmo no supervisados Los modelos de aprendizaje no supervisado son aquellos en los que no estamos interesados en ajustar pares como las entrada y salida, sino más bien en aumentar el conocimiento estructural de los datos disponibles y/o posibles datos futuros que provengan del mismo fenómeno, un ejemplo claro es la agrupación de los datos según su parentesco lo que es llamado clustering, simplificando las estructura de los mismos manteniendo sus características fundamentales como en los procesos de reducción de la dimensionalidad, o extrayendo la estructura interna con la que se distribuyen los datos en su espacio original. 6.2.1 K-Means install.packages(&quot;mlr&quot;, dependencies = TRUE) install.packages(&quot;datasets&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;kernlab&quot;) install.packages(&quot;XML&quot;, repos = &quot;http://www.omegahat.net/R&quot;) library(mlr) library(tidyverse) library(datasets) library(XML) 6.2.1.1 Cargamos la data La data que utilizaremos ** GvHD ** que es la enfermedad de injerto contra huésped de Brinkman et al. (2007). Dos muestras de estos datos de citometría de flujo, una de un paciente con EICH y la otra de un paciente de control. Las muestras de GvHD positivas y de control constan de 9083 y 6809 observaciones, respectivamente. Ambas muestras incluyen cuatro variables de biomarcadores, a saber, CD4, CD8b, CD3 y CD8. El objetivo del análisis es identificar subpoblaciones de células CD3 + CD4 + CD8b + presentes en la muestra positiva de EICH. Usaremos mclust es un paquete R contribuido para el agrupamiento, la clasificación y la estimación de densidad basados en modelos basados en el modelado de mezclas normales finitas. data(GvHD, package = &quot;mclust&quot;) La data GvHD posee lo siguiente: GvHD.pos (paciente positivo) es un marco de datos con 9083 observaciones sobre las siguientes 4 variables, que son mediciones de biomarcadores. CD4 CD8b CD3 CD8 GvHD.control (paciente de control) es un marco de datos con 6809 observaciones sobre las siguientes 4 variables, que son mediciones de biomarcadores. CD4 CD8b CD3 CD8 Usaremos este último. Ahora carguemos los datos, que están integrados en el paquete mclust, conviértalos en tibble con as_tibble () . gvhdTib &lt;- as_tibble(GvHD.control) gvhdTib summary(gvhdTib) ## CD4 CD8b CD3 CD8 ## Min. : 1.0 Min. : 1.0 Min. : 1.0 Min. : 1.0 ## 1st Qu.:143.0 1st Qu.:149.0 1st Qu.: 71.0 1st Qu.:142.0 ## Median :277.0 Median :341.0 Median :123.0 Median :195.0 ## Mean :258.6 Mean :292.2 Mean :161.8 Mean :204.9 ## 3rd Qu.:351.0 3rd Qu.:411.0 3rd Qu.:185.0 3rd Qu.:241.0 ## Max. :670.0 Max. :781.0 Max. :740.0 Max. :778.0 6.2.1.2 Escalonamiento Debido a que los algoritmos de k-medias usan una métrica de distancia para asignar casos a grupos, es importante que nuestras variables se escalen para que las variables en diferentes escalas reciban el mismo peso. Todas nuestras variables son continuas, por lo que simplemente podemos canalizar todo nuestro tibble en la función scale (). De esta manera centrará y escalará cada variable por restando la media y dividiendo por la desviación estándar. gvhdScaled &lt;- gvhdTib %&gt;% scale() summary(gvhdScaled) ## CD4 CD8b CD3 CD8 ## Min. :-1.8897 Min. :-1.9934 Min. :-1.1683 Min. :-1.76768 ## 1st Qu.:-0.8481 1st Qu.:-0.9801 1st Qu.:-0.6598 1st Qu.:-0.54508 ## Median : 0.1348 Median : 0.3344 Median :-0.2820 Median :-0.08552 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.6776 3rd Qu.: 0.8136 3rd Qu.: 0.1684 3rd Qu.: 0.31334 ## Max. : 3.0175 Max. : 3.3468 Max. : 4.2004 Max. : 4.96961 library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 6.2.1.3 Visualización de densidad El gráfico de cada variable frente a cualquier otra variable en nuestro conjunto de datos de GvHD. Los diagramas de dispersión se muestran debajo de la diagonal, los diagramas de densidad 2D se muestran sobre la diagonal y los diagramas de densidad 1D se dibujan en la diagonal. Parece como si hubiera varios clústeres en los datos. ggpairs(GvHD.control, upper = list(continuous = &quot;density&quot;), lower = list(continuous = wrap(&quot;points&quot;, size = 0.5)), diag = list(continuous = &quot;densityDiag&quot;)) + theme_bw() 6.2.1.4 Creamos la tarea de clasificación Con mlr, creamos una tarea de agrupamiento usando la función makeClusterTask (). gvhdTask &lt;- makeClusterTask(data = as.data.frame(gvhdScaled)) gvhdTask ## Unsupervised task: as.data.frame(gvhdScaled) ## Type: cluster ## Observations: 6809 ## Features: ## numerics factors ordered functionals ## 4 0 0 0 ## Missings: FALSE ## Has weights: FALSE ## Has blocking: FALSE ## Has coordinates: FALSE Es importante Tener en cuenta que, a diferencia de la creación de una tarea de aprendizaje supervisada , ya no es necesario proporcionar el argumento objetivo. Esto se debe a que en una tarea de aprendizaje no supervisado, no hay variables de etiquetas para utilizar como objetivo. 6.2.1.5 Que tipos de clusters existen en MLR Usemos la función listLearners () para ver qué algoritmos ha implementado el paquete mlr hasta ahora. listLearners(&quot;cluster&quot;)$class ## [1] &quot;cluster.cmeans&quot; &quot;cluster.Cobweb&quot; ## [3] &quot;cluster.dbscan&quot; &quot;cluster.EM&quot; ## [5] &quot;cluster.FarthestFirst&quot; &quot;cluster.kkmeans&quot; ## [7] &quot;cluster.kmeans&quot; &quot;cluster.MiniBatchKmeans&quot; ## [9] &quot;cluster.SimpleKMeans&quot; &quot;cluster.XMeans&quot; Ahora definamos nuestro alumno de k-medias. Hacemos esto usando la función makeLearner (), esta vez proporcionando cluster.kmeans como el nombre del alumno. Usamos el argumento par.vals para proporcionar dos argumentos al alumno, con el argumento iter.max establece un límite superior para el número de veces que el algoritmo recorrerá los datos en este caso asignaremos 100 y con el argumento nstart controla cuántas veces la función inicializará aleatoriamente los centros que asignamos 10. kMeans &lt;- makeLearner(&quot;cluster.kmeans&quot;, par.vals = list(iter.max = 100, nstart = 10)) Es importante tener en cuenta que los centros iniciales generalmente se inicializan aleatoriamente en algún lugar del espacio de características, lo cual puede tener un impacto en las posiciones finales del centroide y al establecer el argumento nstart más alto que el valor predeterminado de 1 inicializará aleatoriamente este número de centros. Para cada juego de los centros iniciales, los casos se asignan al grupo de su centro más cercano en cada conjunto, y el conjunto con la menor suma de error cuadrado dentro del clúster se utiliza para el resto del algoritmo de agrupamiento. De esta forma, el algoritmo selecciona el conjunto de centros que ya es más similar a los centroides de clúster reales en los datos. Tuning k and the algorithm choice for our k-means model Podemos usar la función getParamSet(kMeans) para encontrar todos los hiperparámetros disponibles para nosotros. getParamSet(kMeans) ## Type len Def Constr Req ## centers untyped - - - - ## iter.max integer - 10 1 to Inf - ## nstart integer - 1 1 to Inf - ## algorithm discrete - Hartigan-Wong Hartigan-Wong,Lloyd,Forgy,MacQueen - ## trace logical - - - - ## Tunable Trafo ## centers TRUE - ## iter.max TRUE - ## nstart TRUE - ## algorithm TRUE - ## trace FALSE - Los hiperparámetros disponibles son: Hartigan-Wong: Lloyd: Forgy: MacQuee: A continuación, usaremos la función makeParamSet(). Definimos dos hiperparámetros discretos sobre los cuales buscar valores: centros, que es el número de grupos que el algoritmo buscará (k), y el algoritmo, que especifica cuál de los tres algoritmos utilizará para adaptarse al modelo. kMeansParamSpace &lt;- makeParamSet( makeDiscreteParam(&quot;centers&quot;, values = 3:8), makeDiscreteParam(&quot;algorithm&quot;, values = c(&quot;Hartigan-Wong&quot;, &quot;Lloyd&quot;, &quot;MacQueen&quot;))) gridSearch &lt;- makeTuneControlGrid() gridSearch ## Tune control: TuneControlGrid ## Same resampling instance: TRUE ## Imputation value: &lt;worst&gt; ## Start: &lt;NULL&gt; ## ## Tune threshold: FALSE ## Further arguments: resolution=10 Definimos nuestro enfoque de validación cruzada como 10 veces. kFold &lt;- makeResampleDesc(&quot;CV&quot;, iters = 10) #install.packages(&quot;clusterSim&quot;) listMeasures(&quot;cluster&quot;) ## [1] &quot;featperc&quot; &quot;db&quot; &quot;timeboth&quot; &quot;timetrain&quot; &quot;timepredict&quot; ## [6] &quot;silhouette&quot; &quot;G1&quot; &quot;G2&quot; 6.2.1.6 Realizar ajustes Para realizar el ajuste, usamos la función tuneParams()con los siguientes argumentos: kMeanses el primer argumento que es el nombre del alumno. El argumento task es el nombre de la tarea de agrupamiento. El argumento resampling es el nombre de nuestra estrategia de validación cruzada. El argumento par.set es nuestro espacio de búsqueda de hiperparámetros. El argumento control es nuestro método de búsqueda. El argumento measures nos permite definir qué medidas de desempeño queremos, con índice (db), índice de Dunn (dunn) y pseudo estadístico F (G1), en ese orden. tunedK &lt;- tuneParams(kMeans, task = gvhdTask, resampling = kFold, par.set = kMeansParamSpace, control = gridSearch, measures = list(db, G1)) ## [Tune] Started tuning learner cluster.kmeans for parameter set: ## Type len Def Constr Req Tunable Trafo ## centers discrete - - 3,4,5,6,7,8 - TRUE - ## algorithm discrete - - Hartigan-Wong,Lloyd,MacQueen - TRUE - ## With control class: TuneControlGrid ## Imputation value: InfImputation value: -0 ## [Tune-x] 1: centers=3; algorithm=Hartigan-Wong ## Warning in rgl.init(initValue, onlyNULL): Cannot create Freetype font ## Warning in rgl.init(initValue, onlyNULL): font family &quot;sans&quot; not found, using ## &quot;bitmap&quot; ## [Tune-y] 1: db.test.mean=1.1706155,G1.test.mean=341.4513344; time: 0.0 min ## [Tune-x] 2: centers=4; algorithm=Hartigan-Wong ## [Tune-y] 2: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 3: centers=5; algorithm=Hartigan-Wong ## [Tune-y] 3: db.test.mean=1.1440635,G1.test.mean=437.1214563; time: 0.0 min ## [Tune-x] 4: centers=6; algorithm=Hartigan-Wong ## [Tune-y] 4: db.test.mean=1.1910863,G1.test.mean=394.5595074; time: 0.0 min ## [Tune-x] 5: centers=7; algorithm=Hartigan-Wong ## [Tune-y] 5: db.test.mean=1.1691958,G1.test.mean=377.6782850; time: 0.0 min ## [Tune-x] 6: centers=8; algorithm=Hartigan-Wong ## [Tune-y] 6: db.test.mean=1.2175333,G1.test.mean=362.0658320; time: 0.0 min ## [Tune-x] 7: centers=3; algorithm=Lloyd ## [Tune-y] 7: db.test.mean=1.1706155,G1.test.mean=341.4513344; time: 0.0 min ## [Tune-x] 8: centers=4; algorithm=Lloyd ## [Tune-y] 8: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 9: centers=5; algorithm=Lloyd ## [Tune-y] 9: db.test.mean=1.1447321,G1.test.mean=437.0755098; time: 0.0 min ## [Tune-x] 10: centers=6; algorithm=Lloyd ## [Tune-y] 10: db.test.mean=1.2037249,G1.test.mean=392.8459968; time: 0.0 min ## [Tune-x] 11: centers=7; algorithm=Lloyd ## [Tune-y] 11: db.test.mean=1.1789063,G1.test.mean=376.8527292; time: 0.0 min ## [Tune-x] 12: centers=8; algorithm=Lloyd ## Warning: did not converge in 100 iterations ## [Tune-y] 12: db.test.mean=1.2167544,G1.test.mean=361.9683803; time: 0.0 min ## [Tune-x] 13: centers=3; algorithm=MacQueen ## [Tune-y] 13: db.test.mean=1.1708359,G1.test.mean=341.4686930; time: 0.0 min ## [Tune-x] 14: centers=4; algorithm=MacQueen ## [Tune-y] 14: db.test.mean=0.8066902,G1.test.mean=486.3999518; time: 0.0 min ## [Tune-x] 15: centers=5; algorithm=MacQueen ## [Tune-y] 15: db.test.mean=1.1447565,G1.test.mean=437.0926618; time: 0.0 min ## [Tune-x] 16: centers=6; algorithm=MacQueen ## [Tune-y] 16: db.test.mean=1.2066667,G1.test.mean=394.0274437; time: 0.0 min ## [Tune-x] 17: centers=7; algorithm=MacQueen ## [Tune-y] 17: db.test.mean=1.1834367,G1.test.mean=376.5984665; time: 0.0 min ## [Tune-x] 18: centers=8; algorithm=MacQueen ## [Tune-y] 18: db.test.mean=1.2103025,G1.test.mean=361.8921627; time: 0.0 min ## [Tune] Result: centers=4; algorithm=Hartigan-Wong : db.test.mean=0.8066902,G1.test.mean=486.3999518 tunedK ## Tune result: ## Op. pars: centers=4; algorithm=Hartigan-Wong ## db.test.mean=0.8066902,G1.test.mean=486.3999518 Necesitamos extraer los datos de ajuste de nuestro resultado de ajuste con la función generateHyperParsEffectData(). Llame al componente ($)data desde kMeansTuningData. Recopilamos los datos de manera que el nombre de cada métrica de rendimiento está en una columna y el valor de la métrica está en otra columna. Hacemos esto usando la función gather(), nombrando la columna clave Metric y la columna de valor Value. kMeansTuningData &lt;- generateHyperParsEffectData(tunedK) gatheredTuningData &lt;- gather(kMeansTuningData$data, key = &quot;Metric&quot;, value = &quot;Value&quot;, c(-centers, -iteration, -algorithm)) gatheredTuningData ## centers algorithm iteration Metric Value ## 1 3 Hartigan-Wong 1 db.test.mean 1.1706155 ## 2 4 Hartigan-Wong 2 db.test.mean 0.8066902 ## 3 5 Hartigan-Wong 3 db.test.mean 1.1440635 ## 4 6 Hartigan-Wong 4 db.test.mean 1.1910863 ## 5 7 Hartigan-Wong 5 db.test.mean 1.1691958 ## 6 8 Hartigan-Wong 6 db.test.mean 1.2175333 ## 7 3 Lloyd 7 db.test.mean 1.1706155 ## 8 4 Lloyd 8 db.test.mean 0.8066902 ## 9 5 Lloyd 9 db.test.mean 1.1447321 ## 10 6 Lloyd 10 db.test.mean 1.2037249 ## 11 7 Lloyd 11 db.test.mean 1.1789063 ## 12 8 Lloyd 12 db.test.mean 1.2167544 ## 13 3 MacQueen 13 db.test.mean 1.1708359 ## 14 4 MacQueen 14 db.test.mean 0.8066902 ## 15 5 MacQueen 15 db.test.mean 1.1447565 ## 16 6 MacQueen 16 db.test.mean 1.2066667 ## 17 7 MacQueen 17 db.test.mean 1.1834367 ## 18 8 MacQueen 18 db.test.mean 1.2103025 ## 19 3 Hartigan-Wong 1 G1.test.mean 341.4513344 ## 20 4 Hartigan-Wong 2 G1.test.mean 486.3999518 ## 21 5 Hartigan-Wong 3 G1.test.mean 437.1214563 ## 22 6 Hartigan-Wong 4 G1.test.mean 394.5595074 ## 23 7 Hartigan-Wong 5 G1.test.mean 377.6782850 ## 24 8 Hartigan-Wong 6 G1.test.mean 362.0658320 ## 25 3 Lloyd 7 G1.test.mean 341.4513344 ## 26 4 Lloyd 8 G1.test.mean 486.3999518 ## 27 5 Lloyd 9 G1.test.mean 437.0755098 ## 28 6 Lloyd 10 G1.test.mean 392.8459968 ## 29 7 Lloyd 11 G1.test.mean 376.8527292 ## 30 8 Lloyd 12 G1.test.mean 361.9683803 ## 31 3 MacQueen 13 G1.test.mean 341.4686930 ## 32 4 MacQueen 14 G1.test.mean 486.3999518 ## 33 5 MacQueen 15 G1.test.mean 437.0926618 ## 34 6 MacQueen 16 G1.test.mean 394.0274437 ## 35 7 MacQueen 17 G1.test.mean 376.5984665 ## 36 8 MacQueen 18 G1.test.mean 361.8921627 ## 37 3 Hartigan-Wong 1 exec.time 1.2500000 ## 38 4 Hartigan-Wong 2 exec.time 0.5600000 ## 39 5 Hartigan-Wong 3 exec.time 0.8900000 ## 40 6 Hartigan-Wong 4 exec.time 1.0000000 ## 41 7 Hartigan-Wong 5 exec.time 1.1800000 ## 42 8 Hartigan-Wong 6 exec.time 1.3500000 ## 43 3 Lloyd 7 exec.time 0.3700000 ## 44 4 Lloyd 8 exec.time 0.4700000 ## 45 5 Lloyd 9 exec.time 0.8300000 ## 46 6 Lloyd 10 exec.time 0.8700000 ## 47 7 Lloyd 11 exec.time 1.2200000 ## 48 8 Lloyd 12 exec.time 1.2800000 ## 49 3 MacQueen 13 exec.time 0.3400000 ## 50 4 MacQueen 14 exec.time 0.3900000 ## 51 5 MacQueen 15 exec.time 0.5400000 ## 52 6 MacQueen 16 exec.time 0.6000000 ## 53 7 MacQueen 17 exec.time 0.7100000 ## 54 8 MacQueen 18 exec.time 0.7800000 Para graficar los datos anterior usaremos la función ggplot(), mapeando centros (el número de clusters) y Valor a la estética x e y, respectivamente. Al mapear el algoritmo a las capas estéticas col, geom_line() y geom_point() separadas se dibujarán para cada algoritmo (con diferentes colores). ggplot(gatheredTuningData, aes(centers, Value, col = algorithm)) + facet_wrap(~ Metric, scales = &quot;free_y&quot;) + geom_line() + geom_point() + theme_bw() La mayor diferencia entre los algoritmos es su tiempo de entrenamiento. Darse cuenta de que el algoritmo de MacQueen es consistentemente más rápido que cualquiera de los otros. Esto es debido a el algoritmo actualiza sus centroides con más frecuencia que Lloyds y tiene que vuelve a calcular las distancias con menos frecuencia que Hartigan-Wong. 6.2.1.7 Entrenamiento del modelo final de k-medias ajustado Ahora usaremos nuestros hiperparámetros ajustados para entrenar nuestro modelo de agrupamiento final. Además, utilizaremos la validación cruzada anidada para validar de forma cruzada todo el proceso de construcción de modelos. Primero comenzaremos creando un alumno de k-means que use nuestros valores de hiperparámetros ajustados, usando la función setHyperPars(). tunedKMeans &lt;- setHyperPars(kMeans, par.vals = tunedK$x) tunedKMeans ## Learner cluster.kmeans from package stats,clue ## Type: cluster ## Name: K-Means; Short name: kmeans ## Class: cluster.kmeans ## Properties: numerics,prob ## Predict-Type: response ## Hyperparameters: centers=4,iter.max=100,nstart=10,algorithm=Hartigan-Wong Luego entrenamos este modelo ajustado en nuestra gvhdTask usando la función train() y usamos la función getLearnerModel() para extraer los datos del modelo para que podamos trazar los conglomerados. tunedKMeansModel &lt;- train(tunedKMeans, gvhdTask) tunedKMeansModel ## Model for learner.id=cluster.kmeans; learner.class=cluster.kmeans ## Trained on: task.id = as.data.frame(gvhdScaled); obs = 6809; features = 4 ## Hyperparameters: centers=4,iter.max=100,nstart=10,algorithm=Hartigan-Wong Al imprimir los datos del modelo llamando a kMeansModelData y examine la salida; esta contiene una gran cantidad de información útil. kMeansModelData &lt;- getLearnerModel(tunedKMeansModel) kMeansModelData ## K-means clustering with 4 clusters of sizes 428, 4227, 1488, 666 ## ## Cluster means: ## CD4 CD8b CD3 CD8 ## 1 -0.9164416 0.5912660 1.6843512 2.796499034 ## 2 0.2904422 0.5955009 -0.3417141 -0.216151841 ## 3 -1.2790155 -1.2397409 -0.4350552 -0.004331523 ## 4 1.6031727 -1.3896543 2.0583862 -0.415589260 ## ## Clustering vector: ## [1] 2 2 3 3 3 2 1 2 2 2 2 2 2 3 3 1 3 2 3 3 2 3 2 2 4 2 3 2 2 4 2 2 2 2 2 3 2 ## [38] 1 3 2 4 1 4 3 4 2 2 2 1 2 2 3 2 2 2 2 3 4 2 2 3 2 3 2 3 4 2 2 2 2 4 2 1 2 ## [75] 3 2 2 2 2 2 4 2 3 3 1 4 2 3 2 3 2 4 2 1 4 2 2 2 2 3 2 2 2 3 3 2 2 2 2 4 4 ## [112] 2 2 2 4 3 2 2 2 3 3 2 2 3 2 2 3 4 2 2 2 2 2 2 2 4 3 2 3 3 2 2 3 2 2 3 2 3 ## [149] 2 2 2 2 2 3 3 3 2 3 2 2 3 1 2 2 1 2 2 4 3 2 2 4 2 2 4 2 3 2 3 3 2 2 3 2 2 ## [186] 3 2 2 2 3 3 3 2 3 2 2 1 2 2 2 2 4 2 2 3 3 3 2 2 3 2 4 3 3 3 2 2 3 3 2 2 3 ## [223] 2 2 3 2 2 3 2 2 3 2 4 2 3 3 4 2 2 3 2 2 2 2 1 2 2 2 2 2 3 4 3 3 3 3 3 2 1 ## [260] 3 2 3 3 2 1 3 2 3 2 2 2 3 2 3 3 2 2 3 3 2 2 2 2 3 3 2 2 2 3 1 3 2 2 2 3 2 ## [297] 2 3 2 3 2 1 2 3 3 2 4 2 3 3 1 2 2 2 3 3 4 2 2 2 2 3 4 4 2 4 2 4 3 2 2 2 2 ## [334] 2 2 2 3 4 2 2 2 1 2 1 2 2 4 2 4 4 2 2 2 2 3 3 2 4 2 2 2 2 4 2 3 4 2 2 4 1 ## [371] 2 3 2 2 2 2 3 2 2 3 3 2 3 2 2 2 4 2 3 2 3 2 2 2 2 2 2 3 4 2 1 3 2 3 2 2 3 ## [408] 3 2 2 4 3 4 2 2 2 2 4 2 2 3 4 2 2 2 2 2 2 3 2 3 2 2 2 2 3 2 2 2 2 2 2 2 1 ## [445] 3 2 1 1 2 1 2 2 2 4 3 3 2 2 2 2 2 2 2 2 2 4 3 2 2 2 3 2 3 3 2 2 2 2 2 2 2 ## [482] 2 2 3 2 3 4 2 3 1 2 2 4 4 2 2 4 1 2 2 2 2 2 2 3 3 2 2 3 2 3 2 2 2 2 2 1 2 ## [519] 4 2 4 2 2 2 2 3 2 3 1 2 4 2 2 2 4 2 2 1 2 2 3 3 3 2 3 3 2 2 2 2 2 2 2 2 2 ## [556] 2 2 3 2 2 1 4 2 3 3 2 3 4 1 3 2 2 1 2 3 1 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 3 ## [593] 3 3 2 4 2 2 2 4 2 2 2 2 2 2 2 3 2 3 2 2 2 3 1 3 4 4 2 2 2 1 2 2 2 2 4 3 4 ## [630] 2 2 3 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2 4 4 3 2 2 3 ## [667] 3 2 2 4 2 2 4 3 2 1 2 3 2 3 2 1 3 3 3 2 1 3 2 2 2 2 3 3 2 3 2 3 2 2 3 2 3 ## [704] 1 2 2 3 2 1 2 2 2 4 2 4 1 2 3 2 2 2 2 4 2 2 3 2 2 2 2 2 2 3 2 1 2 2 2 3 3 ## [741] 2 2 3 3 4 3 2 4 2 2 2 2 1 3 2 2 2 2 2 4 2 2 3 3 1 4 3 2 2 2 2 2 3 4 3 2 2 ## [778] 2 2 2 2 1 4 3 2 2 2 2 4 2 2 3 2 2 3 2 2 2 3 3 4 4 3 2 2 4 2 2 2 2 4 2 2 3 ## [815] 3 2 4 3 1 3 2 2 3 2 2 2 3 3 2 2 2 2 4 2 2 2 3 2 2 2 3 1 4 2 2 3 3 2 2 2 2 ## [852] 3 4 3 2 3 4 2 3 2 2 4 3 2 2 2 2 2 2 4 2 4 2 2 2 4 2 2 2 1 2 3 3 2 3 1 3 2 ## [889] 2 4 2 3 3 2 2 2 3 3 3 2 2 2 3 4 2 3 3 3 3 2 1 2 2 2 3 2 2 1 4 4 2 3 2 2 3 ## [926] 4 2 3 4 1 3 1 2 2 2 1 2 2 2 2 4 2 1 4 3 2 2 2 2 2 2 4 2 2 3 2 2 3 2 2 2 4 ## [963] 1 2 2 2 2 2 2 2 2 3 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 4 2 2 3 2 1 2 2 2 4 2 4 ## [1000] 4 2 2 3 2 3 1 2 2 2 2 1 4 3 4 2 2 1 4 4 2 4 3 3 3 1 2 1 2 2 4 2 2 2 2 4 2 ## [1037] 4 2 1 1 2 2 2 2 2 2 3 2 4 2 1 2 2 2 2 2 2 2 2 2 2 2 3 2 3 2 2 2 3 1 2 3 2 ## [1074] 3 2 3 2 3 2 2 2 2 3 2 2 2 3 3 2 2 3 2 2 2 2 2 2 2 3 2 3 3 2 2 3 2 2 2 3 3 ## [1111] 2 3 1 3 2 4 2 2 3 2 2 2 2 4 3 2 2 3 2 3 2 2 2 2 3 2 2 3 3 2 2 2 2 3 2 2 2 ## [1148] 2 4 4 2 1 2 2 2 3 3 2 2 3 2 2 2 2 2 2 2 2 2 3 2 2 3 2 3 3 2 4 2 4 3 2 2 2 ## [1185] 2 2 3 4 2 2 1 2 4 2 2 2 3 2 2 2 3 2 4 2 2 3 2 2 2 4 3 2 2 3 4 2 2 2 2 3 2 ## [1222] 2 4 1 2 2 2 2 2 2 4 2 3 2 3 1 2 2 3 4 2 2 1 2 4 2 2 2 2 2 2 4 1 3 3 2 2 3 ## [1259] 2 3 4 2 3 3 3 4 3 2 3 4 2 3 4 3 2 2 1 3 2 2 1 2 3 2 2 2 2 3 3 2 3 4 2 4 3 ## [1296] 2 2 2 3 3 3 3 4 2 2 2 2 1 3 2 4 2 4 4 3 2 2 3 2 4 2 3 2 1 2 2 2 2 3 4 2 1 ## [1333] 4 3 3 2 3 2 2 2 2 2 4 3 2 2 2 2 3 4 2 3 2 2 4 3 3 3 1 3 1 2 2 3 2 2 2 2 4 ## [1370] 2 2 2 3 4 2 2 2 3 3 2 2 2 3 2 3 2 2 2 2 4 2 3 2 2 2 2 2 2 2 2 2 3 3 2 3 3 ## [1407] 1 3 4 3 4 4 2 2 3 2 4 2 2 2 2 2 2 2 2 2 3 2 2 2 3 2 2 1 2 2 2 1 3 3 4 2 3 ## [1444] 2 2 1 4 3 2 2 2 2 2 2 2 3 2 2 2 2 3 2 2 2 4 3 2 2 2 2 3 2 2 2 3 2 4 4 4 2 ## [1481] 2 3 2 2 1 3 2 2 2 4 2 2 2 1 4 2 2 2 3 3 2 1 2 3 2 2 2 4 4 2 2 2 3 3 3 2 2 ## [1518] 2 1 2 4 1 2 2 2 1 1 2 2 3 2 2 2 2 2 4 2 2 2 3 2 1 1 2 2 3 2 3 2 2 2 2 4 3 ## [1555] 2 2 2 3 1 2 3 2 2 3 2 3 2 1 2 3 4 2 3 2 1 3 3 2 1 2 1 2 3 2 2 2 2 4 1 2 2 ## [1592] 2 2 2 2 1 2 2 3 2 2 2 2 2 3 3 2 2 2 3 2 2 3 3 2 2 3 2 2 3 3 3 3 2 3 2 2 4 ## [1629] 2 2 2 4 2 4 1 3 2 3 2 2 3 3 2 2 1 2 2 4 3 2 4 2 2 2 2 3 2 3 3 3 4 2 1 1 2 ## [1666] 2 2 3 2 4 3 2 1 3 3 4 2 1 2 3 2 3 2 1 2 4 2 2 2 2 3 2 2 2 2 1 2 3 2 4 2 4 ## [1703] 1 2 2 1 2 2 2 2 2 2 2 2 2 3 3 2 2 3 2 3 2 1 2 2 3 3 2 2 2 2 2 3 2 2 1 4 2 ## [1740] 2 2 3 2 2 2 1 2 4 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 4 2 1 2 2 2 2 2 2 2 1 2 ## [1777] 4 2 2 3 3 2 2 3 2 4 2 2 2 2 2 2 3 3 3 2 2 1 3 2 2 3 2 2 3 4 2 1 4 2 2 2 2 ## [1814] 2 2 2 2 3 4 2 2 3 3 2 2 3 2 2 2 3 2 3 2 3 2 2 3 2 2 2 2 4 4 4 2 2 2 3 2 2 ## [1851] 3 2 3 1 3 3 2 3 3 2 3 1 4 2 2 2 3 2 2 2 1 4 2 2 2 2 2 3 1 2 1 3 4 3 2 3 3 ## [1888] 4 2 2 4 2 4 2 2 1 2 2 2 4 3 2 2 2 2 3 4 2 1 2 2 2 4 2 4 3 2 2 4 2 2 2 2 3 ## [1925] 2 2 2 2 2 3 2 2 4 3 2 3 2 1 2 2 2 2 2 3 2 2 3 1 2 4 3 2 4 2 2 2 1 3 3 3 2 ## [1962] 2 2 2 2 3 2 2 2 2 2 1 2 2 2 3 1 1 3 2 3 2 3 2 2 2 2 2 2 2 2 3 4 2 2 2 3 2 ## [1999] 2 3 2 2 2 1 2 1 2 2 2 2 2 2 2 4 2 3 2 3 2 2 2 2 2 2 2 3 2 4 2 4 4 2 3 4 2 ## [2036] 2 2 2 3 2 2 2 3 4 2 2 2 2 2 1 2 1 4 2 2 2 4 2 2 1 2 2 3 2 3 2 2 2 3 3 2 1 ## [2073] 2 2 2 2 4 4 3 2 2 3 2 3 3 2 2 2 4 3 2 2 4 3 4 1 2 2 3 2 2 4 2 2 4 3 3 3 3 ## [2110] 4 2 2 2 2 2 4 1 2 3 2 2 2 2 2 3 3 2 2 2 3 2 3 2 3 2 2 2 2 2 2 3 1 2 1 3 2 ## [2147] 4 3 3 3 2 2 3 2 2 2 4 2 2 3 1 2 2 2 1 2 3 3 3 2 2 2 4 3 2 4 4 2 3 2 2 3 3 ## [2184] 2 2 2 2 2 1 3 2 2 2 2 2 3 2 1 2 2 2 2 2 2 4 1 2 3 1 3 4 2 3 2 2 3 2 1 2 2 ## [2221] 2 3 2 4 2 2 2 2 2 3 2 3 2 4 2 4 2 2 4 3 1 2 1 2 2 4 1 4 4 2 1 2 2 1 2 2 3 ## [2258] 4 2 2 3 3 2 1 3 1 1 2 2 2 2 2 2 2 3 4 2 3 2 3 1 2 2 3 2 3 2 3 2 3 2 3 3 4 ## [2295] 2 3 3 2 3 2 4 2 2 1 2 2 2 2 2 3 2 2 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 3 2 3 ## [2332] 1 2 2 3 3 2 3 2 2 3 2 2 3 2 2 2 3 3 2 2 2 3 4 3 1 2 3 3 2 2 3 2 3 2 2 3 2 ## [2369] 2 2 2 2 3 2 3 2 3 2 2 4 2 4 3 2 2 2 2 4 2 2 1 2 3 3 2 4 2 1 2 3 2 2 2 1 3 ## [2406] 2 2 2 2 2 2 2 3 2 2 3 3 3 2 1 2 3 2 3 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 ## [2443] 2 3 2 3 2 2 3 2 2 2 2 4 2 4 2 2 2 2 4 2 4 3 3 3 2 2 2 1 2 2 3 3 4 3 2 4 2 ## [2480] 2 2 3 4 3 2 3 3 4 1 2 2 2 2 3 2 2 3 2 2 4 3 2 2 2 2 2 2 2 2 4 3 2 2 2 3 2 ## [2517] 3 2 2 2 2 3 4 1 2 4 2 2 2 2 3 1 2 1 2 3 3 3 2 2 2 3 2 2 3 1 3 2 2 2 4 1 4 ## [2554] 2 2 2 2 1 2 2 2 2 2 3 2 2 2 2 1 1 2 2 3 2 2 2 2 2 2 2 2 2 1 2 2 3 2 3 2 2 ## [2591] 4 3 2 3 2 2 2 3 2 1 2 2 2 4 2 2 4 2 3 2 2 2 2 2 2 2 2 2 4 4 4 2 4 2 2 2 3 ## [2628] 2 3 2 2 4 2 2 4 4 2 3 2 4 3 2 3 3 2 2 1 2 2 2 2 4 4 2 2 3 2 3 2 2 2 2 2 1 ## [2665] 2 2 2 3 2 2 3 2 3 2 2 2 2 2 4 2 3 2 1 2 2 4 2 2 2 2 3 3 2 1 2 2 3 3 1 3 2 ## [2702] 3 4 2 3 2 2 3 2 2 2 2 3 2 2 2 2 3 1 2 2 2 3 3 2 2 1 4 3 2 4 2 2 3 2 2 3 3 ## [2739] 2 2 3 2 2 2 3 2 2 2 2 2 2 1 3 2 2 2 2 2 2 2 2 4 2 4 2 2 2 2 3 2 2 3 3 2 4 ## [2776] 2 2 2 2 4 2 2 2 3 2 4 2 3 3 4 2 2 3 4 2 3 3 2 2 4 3 3 3 2 2 2 1 2 2 2 2 4 ## [2813] 2 2 2 2 2 2 2 1 3 2 2 2 3 2 2 3 2 3 2 2 2 4 2 2 3 3 2 3 2 2 2 3 2 2 3 3 2 ## [2850] 2 2 2 2 3 1 2 4 2 2 2 1 2 2 2 1 2 4 4 2 2 2 2 2 2 2 2 2 3 2 3 4 2 3 3 2 3 ## [2887] 2 2 4 2 3 2 4 2 2 2 2 2 2 3 2 2 4 3 2 4 4 3 2 1 2 2 2 2 1 2 2 4 2 2 2 3 4 ## [2924] 4 2 2 3 4 2 3 2 1 2 4 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 4 4 2 2 2 3 2 3 ## [2961] 2 2 2 2 3 2 2 4 2 2 2 3 1 2 3 2 3 3 2 2 3 3 2 2 4 2 2 3 1 2 2 2 4 2 2 2 3 ## [2998] 2 2 2 3 2 3 2 2 2 2 2 2 3 2 2 2 4 1 2 3 2 2 2 1 3 2 2 2 2 2 2 2 2 2 2 2 3 ## [3035] 2 3 3 2 2 2 3 3 2 2 2 2 2 4 2 2 2 2 2 1 2 3 2 1 2 2 1 2 2 3 3 2 4 3 2 4 2 ## [3072] 2 2 2 1 2 2 2 3 2 2 2 3 2 3 2 2 2 2 2 4 2 2 2 4 3 2 3 3 2 2 2 4 1 2 2 1 2 ## [3109] 2 2 3 2 2 2 2 2 2 2 4 2 2 1 1 2 2 2 2 2 3 2 3 2 2 1 2 1 3 4 2 3 2 2 2 2 2 ## [3146] 4 3 1 2 2 4 2 2 2 3 2 2 3 2 2 2 3 2 2 2 3 4 2 3 2 2 2 2 3 2 2 2 1 2 2 3 1 ## [3183] 2 2 3 3 2 2 2 2 2 3 2 1 2 2 2 2 2 2 2 2 1 2 4 2 2 1 2 2 4 2 1 2 4 3 2 3 2 ## [3220] 2 2 3 3 2 2 4 1 2 2 2 2 3 2 1 2 3 2 2 3 4 2 3 2 4 2 2 2 2 2 2 2 2 1 3 2 2 ## [3257] 2 2 2 2 2 3 3 1 2 2 1 2 2 2 2 4 4 2 2 3 2 3 2 3 3 2 2 4 3 3 2 2 2 2 2 2 2 ## [3294] 3 2 2 2 2 2 4 2 3 3 3 3 3 2 2 4 2 2 3 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [3331] 2 1 2 2 2 1 2 2 4 2 3 2 2 2 4 2 2 2 3 2 2 2 3 2 2 3 3 2 3 2 2 3 2 2 2 2 2 ## [3368] 2 3 2 2 4 2 2 2 2 1 2 3 4 2 3 2 3 1 2 4 4 2 4 3 2 2 2 2 2 4 2 4 4 1 2 2 2 ## [3405] 3 4 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 1 3 2 3 2 4 2 2 2 2 2 3 3 1 3 2 3 2 2 2 ## [3442] 2 2 3 3 2 3 2 4 4 2 2 2 2 2 3 2 2 2 2 1 2 2 2 2 4 2 2 2 3 2 1 3 3 3 2 2 3 ## [3479] 4 2 2 2 2 2 2 2 2 2 3 2 2 2 3 3 2 2 1 3 4 2 3 2 2 2 3 3 3 3 4 2 2 2 3 2 2 ## [3516] 2 3 2 2 4 3 2 1 3 2 2 2 2 1 2 2 2 2 2 1 4 3 2 2 3 2 2 2 2 3 3 2 3 2 4 2 2 ## [3553] 2 2 2 1 3 2 2 2 2 3 2 2 4 2 4 2 2 2 3 2 3 2 4 2 3 2 2 2 1 2 2 2 2 2 2 3 1 ## [3590] 2 2 4 2 3 2 4 2 2 4 2 2 4 2 1 4 3 3 3 3 2 3 2 2 2 1 4 1 2 3 3 3 2 4 2 2 2 ## [3627] 2 3 3 3 2 2 3 2 2 2 2 2 1 3 2 3 2 2 3 3 2 2 2 2 2 2 2 3 2 1 4 2 2 4 2 3 2 ## [3664] 4 2 4 2 2 2 2 4 3 3 2 3 2 2 3 2 4 2 2 2 2 2 2 3 2 2 2 4 2 3 4 2 2 2 3 3 2 ## [3701] 2 2 2 2 1 2 2 2 3 2 2 2 4 2 2 2 2 2 3 3 3 2 2 3 2 2 2 1 2 2 2 3 2 3 2 2 2 ## [3738] 2 2 2 2 3 2 2 4 2 2 2 2 2 2 2 2 3 3 4 2 3 3 2 2 2 3 2 2 2 2 2 2 2 2 4 3 2 ## [3775] 3 4 3 2 2 3 2 4 2 4 2 2 2 4 3 2 2 2 4 3 2 4 2 1 2 2 2 2 4 2 2 2 2 3 4 3 2 ## [3812] 2 2 4 2 2 2 2 3 4 3 2 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 1 4 4 2 2 2 1 2 2 3 ## [3849] 4 2 2 3 2 2 4 2 3 2 2 3 2 2 2 3 2 2 4 2 2 3 3 4 1 2 2 2 3 2 1 2 2 2 2 3 2 ## [3886] 2 3 1 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 3 3 2 3 2 2 2 4 3 3 2 3 2 2 1 2 3 2 ## [3923] 2 3 3 1 4 3 2 2 2 3 3 2 2 2 2 3 2 4 3 2 2 3 3 2 3 1 1 4 4 3 2 2 4 3 4 2 2 ## [3960] 2 3 2 1 2 2 3 4 2 2 2 2 2 2 2 2 3 2 2 3 1 2 2 2 3 2 2 2 2 2 3 2 2 2 2 3 2 ## [3997] 2 3 3 2 2 4 3 2 4 2 2 3 4 3 2 2 3 2 2 2 1 2 3 2 1 2 2 2 2 4 2 2 2 4 3 3 2 ## [4034] 2 2 2 2 2 3 2 4 3 2 2 3 4 2 2 2 3 1 2 2 2 1 3 3 3 2 2 4 2 3 3 2 1 2 4 2 2 ## [4071] 2 2 2 2 2 2 2 2 1 2 4 2 3 1 2 2 3 2 4 2 4 2 2 2 2 2 2 2 3 2 2 2 2 2 1 2 4 ## [4108] 2 4 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 3 2 2 3 2 2 2 1 2 2 2 2 2 2 2 3 2 2 1 3 ## [4145] 3 1 2 2 2 3 1 2 2 2 2 4 2 2 2 2 3 2 2 2 2 2 2 3 3 2 2 2 4 2 2 2 2 3 2 2 3 ## [4182] 2 2 2 2 2 4 2 3 4 4 3 2 1 3 2 2 2 3 3 2 2 2 2 2 2 2 3 2 3 2 2 1 2 1 2 4 3 ## [4219] 2 2 2 2 2 4 3 2 2 3 2 2 1 2 2 3 2 3 3 2 4 2 2 2 4 2 3 2 3 3 2 2 1 2 2 3 4 ## [4256] 2 4 2 1 2 2 1 2 4 3 3 2 2 2 2 2 2 3 4 3 2 2 2 2 2 2 2 3 4 2 3 2 1 2 3 4 3 ## [4293] 4 2 4 2 3 3 3 2 4 2 3 3 3 4 2 2 3 3 2 2 2 2 3 3 3 2 2 2 2 2 2 2 1 1 3 2 2 ## [4330] 2 2 4 2 2 2 3 3 2 2 1 2 2 3 2 2 4 2 3 1 2 2 2 2 3 4 2 3 2 2 2 2 2 2 2 2 2 ## [4367] 2 2 2 2 2 4 3 2 2 2 2 2 3 2 2 2 2 2 2 3 1 3 2 2 3 1 3 3 2 2 2 3 4 2 4 2 2 ## [4404] 2 2 3 2 3 3 3 2 2 3 3 2 2 2 2 2 2 3 2 3 2 2 2 2 2 2 2 1 2 2 2 2 2 4 3 2 2 ## [4441] 2 3 3 2 2 4 2 4 2 2 1 2 3 2 2 2 2 3 2 3 1 2 2 2 2 2 2 2 2 2 2 4 2 2 1 2 2 ## [4478] 4 2 2 1 3 2 2 2 2 2 2 2 2 2 3 3 2 3 2 2 2 3 3 2 3 1 3 3 4 2 2 2 2 1 2 2 2 ## [4515] 2 2 2 3 2 4 4 2 2 3 2 2 2 3 2 2 2 2 2 2 2 1 2 2 3 2 2 2 3 2 2 2 3 1 3 2 3 ## [4552] 4 2 2 2 2 2 2 2 3 2 4 2 3 4 3 2 3 2 2 4 3 3 3 2 3 2 3 3 3 2 2 2 2 2 3 3 3 ## [4589] 2 2 2 4 2 2 2 2 3 2 2 2 1 4 2 4 2 4 2 2 4 2 3 3 2 2 3 2 2 2 2 2 3 2 4 3 4 ## [4626] 2 1 3 2 2 4 2 2 2 2 2 2 2 2 2 2 2 3 2 3 4 2 2 4 4 1 3 2 2 3 3 2 2 2 2 4 2 ## [4663] 3 3 2 2 2 2 2 2 2 3 2 3 2 2 2 2 3 1 2 2 1 2 3 2 3 2 2 3 3 2 4 3 2 3 3 2 3 ## [4700] 2 3 2 2 2 3 2 2 2 2 2 1 2 2 3 2 2 2 3 2 3 1 2 2 3 2 2 2 2 2 1 2 2 2 3 2 3 ## [4737] 2 2 3 1 2 2 2 3 4 2 2 2 4 2 1 3 1 3 2 3 2 1 2 2 2 2 2 3 1 2 1 3 1 2 3 2 2 ## [4774] 3 3 2 2 4 2 3 2 3 4 4 2 1 2 3 2 2 3 1 3 2 2 4 3 3 2 4 3 4 2 2 2 2 2 2 3 2 ## [4811] 2 2 2 2 3 1 2 2 2 4 2 4 3 2 3 2 3 2 2 2 4 2 2 3 2 2 2 3 3 2 3 2 2 2 4 2 3 ## [4848] 4 2 4 3 2 2 3 2 2 2 2 2 2 2 4 2 3 2 2 4 2 2 2 2 2 2 2 1 2 2 2 2 3 2 3 2 2 ## [4885] 4 2 2 2 4 2 2 2 4 2 2 2 3 2 4 2 3 2 2 4 2 3 2 4 2 1 2 2 1 2 3 2 2 2 3 2 3 ## [4922] 2 2 2 2 4 2 2 2 2 3 2 4 2 2 2 2 4 2 2 3 3 2 2 2 2 2 3 2 2 2 2 2 3 2 2 1 2 ## [4959] 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 4 3 2 3 2 4 2 2 4 4 3 2 2 2 2 3 3 2 2 4 2 3 ## [4996] 1 2 3 2 2 2 2 2 3 2 2 4 3 2 4 2 2 2 2 3 4 2 3 2 4 2 2 2 2 3 2 2 2 3 2 2 2 ## [5033] 1 2 2 2 2 2 4 2 2 2 2 2 3 3 2 2 2 2 3 3 4 2 2 2 4 3 2 2 2 2 2 3 2 2 2 2 2 ## [5070] 1 2 2 2 2 2 2 2 2 4 2 4 2 2 2 3 4 2 4 2 3 3 2 2 3 2 3 3 2 2 2 2 2 3 2 2 2 ## [5107] 3 2 2 3 3 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2 3 3 4 3 2 2 1 2 4 2 2 2 2 2 2 4 3 ## [5144] 3 2 2 2 3 4 2 2 3 4 2 2 2 2 2 2 2 2 2 3 1 2 2 3 2 2 2 3 2 2 3 1 4 2 3 2 4 ## [5181] 2 2 2 2 1 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 3 2 3 3 1 2 3 2 2 1 2 ## [5218] 2 4 2 3 2 2 3 2 1 2 2 2 4 2 4 3 2 3 2 4 2 2 3 2 3 1 2 1 2 1 2 2 2 2 2 2 4 ## [5255] 1 3 3 2 1 3 2 2 2 2 2 2 2 2 3 2 1 3 1 4 3 3 2 2 3 2 2 3 2 2 3 2 1 2 2 2 2 ## [5292] 3 1 2 2 2 3 2 2 3 2 4 2 1 2 4 2 4 4 2 2 3 3 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 ## [5329] 2 2 2 3 2 2 3 2 2 2 2 1 2 3 4 2 2 3 3 4 3 4 4 4 2 2 3 2 2 2 2 2 2 4 2 4 2 ## [5366] 2 2 2 3 3 3 3 2 2 2 3 2 2 2 2 3 2 2 2 3 2 2 2 2 2 1 2 2 2 4 3 2 3 2 2 1 2 ## [5403] 3 2 4 3 4 1 3 4 1 2 4 3 3 4 2 3 2 2 2 4 2 2 2 3 3 3 4 3 4 2 3 3 2 2 2 2 1 ## [5440] 1 1 2 3 3 2 2 2 4 3 2 3 3 3 3 3 3 2 3 2 2 2 2 2 2 4 3 2 4 2 2 2 2 4 3 2 4 ## [5477] 1 4 2 2 2 2 1 2 2 2 3 4 2 2 2 2 3 2 2 2 3 3 2 2 1 3 2 2 1 3 3 2 1 2 2 3 4 ## [5514] 3 4 3 3 4 2 2 2 2 3 2 2 4 2 4 2 3 2 2 2 2 3 2 2 2 3 2 4 2 2 2 2 2 2 2 2 2 ## [5551] 3 4 2 3 1 2 1 1 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 1 2 2 2 3 2 3 3 3 2 3 2 1 ## [5588] 2 4 2 1 2 2 3 4 2 1 2 2 2 1 4 2 2 2 2 4 3 1 2 3 2 2 3 3 3 2 3 2 3 4 2 2 2 ## [5625] 3 2 2 2 3 2 2 3 3 2 2 4 3 1 2 2 3 2 2 2 2 2 2 3 2 3 2 2 2 4 4 2 2 2 2 2 4 ## [5662] 2 2 2 2 3 3 1 3 2 2 2 3 2 2 2 2 2 2 3 2 2 3 2 2 3 2 2 2 4 2 2 2 2 2 2 2 2 ## [5699] 3 2 2 1 2 2 2 3 2 4 3 3 3 2 3 2 2 2 4 3 2 2 2 2 1 3 2 2 2 2 2 2 4 2 4 2 2 ## [5736] 4 2 2 3 2 4 2 2 2 4 2 3 2 3 3 4 3 2 2 2 2 2 2 2 3 1 4 2 2 2 2 3 3 2 2 4 3 ## [5773] 2 1 1 2 2 2 3 2 3 2 3 2 1 2 2 2 2 2 2 1 1 2 2 2 2 2 3 2 2 2 2 2 2 1 4 2 2 ## [5810] 2 3 2 2 1 3 3 2 4 2 2 2 2 1 2 4 1 3 2 2 3 3 2 3 2 4 3 3 2 1 1 2 1 2 2 4 2 ## [5847] 3 2 2 2 2 2 2 2 3 2 2 2 2 2 4 2 2 3 2 3 1 2 2 3 4 2 2 2 3 2 2 1 2 2 2 2 2 ## [5884] 2 2 2 1 2 3 3 2 3 2 4 2 2 2 2 2 2 3 2 2 3 2 2 3 2 3 3 3 2 4 2 2 2 2 2 2 2 ## [5921] 2 2 4 2 2 2 3 2 2 2 3 2 2 2 2 3 2 4 3 2 3 4 2 1 2 3 4 3 2 3 4 4 2 3 2 2 4 ## [5958] 1 1 3 2 2 2 2 4 2 3 2 3 4 3 1 2 3 4 4 2 2 2 4 2 2 2 2 2 2 2 3 2 2 3 2 2 2 ## [5995] 2 4 3 2 2 2 4 1 4 2 2 2 4 2 1 4 2 2 2 2 4 2 2 2 4 3 2 2 2 3 2 1 4 2 2 2 2 ## [6032] 2 2 2 3 2 2 2 2 2 1 2 2 2 2 2 2 4 3 2 2 3 2 2 2 2 3 2 2 3 1 2 4 2 2 3 1 2 ## [6069] 3 3 3 1 3 2 2 4 2 2 2 2 4 2 2 2 2 2 3 1 2 2 3 3 2 2 2 2 2 2 2 3 3 2 3 2 2 ## [6106] 2 2 2 2 2 2 3 2 2 2 2 2 1 4 3 2 2 2 2 2 2 3 2 3 2 3 2 2 2 4 2 2 3 2 3 2 2 ## [6143] 2 2 2 3 3 3 2 2 3 2 4 3 3 2 2 4 2 2 3 2 2 2 1 3 2 2 2 2 2 3 1 2 3 4 2 2 3 ## [6180] 4 3 4 4 2 2 4 2 2 2 2 3 4 2 2 2 2 2 2 3 2 2 2 2 1 3 2 2 2 2 2 3 2 2 3 2 3 ## [6217] 2 1 3 4 2 3 2 2 2 2 1 2 2 2 2 2 2 2 2 2 3 2 2 4 2 2 2 2 2 3 3 4 2 2 2 2 1 ## [6254] 2 3 2 4 4 1 4 2 3 2 1 4 2 2 2 1 3 4 2 4 3 2 2 2 2 3 4 2 2 2 2 2 2 3 2 2 3 ## [6291] 3 4 3 2 2 3 2 2 4 2 2 2 2 3 2 2 3 2 2 2 1 3 2 2 3 2 3 4 4 2 4 2 3 4 3 2 2 ## [6328] 2 2 2 1 2 2 2 3 2 2 3 2 2 2 1 2 2 3 2 1 3 2 2 2 3 3 2 2 2 4 2 4 2 3 2 2 2 ## [6365] 2 3 3 2 2 4 2 2 2 2 4 3 2 4 3 2 2 2 3 2 2 1 3 3 2 1 3 2 2 2 3 3 3 2 2 1 3 ## [6402] 2 3 2 2 2 3 2 2 3 2 2 2 2 1 2 2 2 2 2 2 2 3 2 2 2 2 2 3 4 2 2 4 2 2 3 2 4 ## [6439] 2 2 4 2 3 3 3 2 4 2 3 3 3 2 2 2 2 2 3 2 3 2 2 3 3 4 2 2 2 1 2 2 2 2 2 2 2 ## [6476] 2 1 3 2 3 2 3 3 2 2 2 2 4 3 4 4 1 2 2 2 4 3 4 2 2 3 3 2 3 2 2 2 2 4 2 2 2 ## [6513] 2 3 2 2 2 2 2 1 1 3 3 2 2 2 1 2 4 2 2 2 4 3 2 2 2 2 3 2 2 1 2 2 2 2 2 4 1 ## [6550] 3 2 3 2 3 2 2 2 2 3 2 2 3 2 2 3 2 2 2 2 2 3 2 1 3 2 4 3 3 3 2 2 2 2 2 4 2 ## [6587] 2 2 2 2 2 3 2 3 1 2 2 2 2 3 2 4 3 4 2 2 2 1 2 2 2 3 2 2 2 3 3 2 2 3 2 2 3 ## [6624] 2 2 2 2 2 3 3 2 4 1 2 4 2 1 2 2 2 3 2 2 2 2 3 2 3 2 2 2 2 2 2 2 2 3 1 2 2 ## [6661] 4 3 2 1 2 4 2 3 2 3 3 2 2 2 3 3 3 2 2 3 3 4 2 3 2 3 2 2 3 2 2 3 3 3 4 2 3 ## [6698] 2 2 2 2 2 4 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2 3 3 2 1 2 2 2 1 2 4 3 2 2 3 2 3 ## [6735] 2 3 2 3 3 3 2 2 3 1 2 1 3 4 3 2 2 4 3 4 2 2 2 2 2 3 1 2 3 2 1 2 3 2 2 3 1 ## [6772] 2 2 2 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2 2 2 3 2 4 2 4 2 3 2 2 3 ## [6809] 2 ## ## Within cluster sum of squares by cluster: ## [1] 1402.9972 4239.4711 2113.1202 921.7474 ## (between_SS / total_SS = 68.1 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; Usando nuestro modelo para predecir grupos de nuevos datos Utilizaremos un caso práctico, tomaremos nuevos datos y generar los clústeres a los que se acercan más los nuevos casos. Como por ejemplo que posea los siguientes datos: CD4 = 1 CD8b = 26 CD3 = 1 CD8 = 122 Pero primero comencemos por crear un tibble que contenga los datos anteriores, incluido un valor para cada variable del conjunto de datos en el que entrenamos el modelo. Una vez realizado lo anterior escalamos los datos de entrenamiento. La forma más sencilla de hacerlo es utilizar la función attr() para extraer el centro y los atributos de escala de los datos escalados. Debido a que la función scale() devuelve un objeto de matriz de clase (y la función predict () arrojará un error si le damos una matriz), necesitamos canalizar los datos escalados en la función as_tibble() para convertirlo de nuevo en tibble. Para predecir a qué grupo pertenece el nuevo caso, simplemente llamamos la función predict() . proporcionando el modelo como primer argumento y el nuevo caso como newdata argumento. newCell &lt;- tibble(CD4 = 1, CD8b = 26, CD3 = 1, CD8 = 122) %&gt;% scale(center = attr(gvhdScaled, &quot;scaled:center&quot;), scale = attr(gvhdScaled, &quot;scaled:scale&quot;)) %&gt;% as_tibble() newCell ## # A tibble: 1 x 4 ## CD4 CD8b CD3 CD8 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.89 -1.82 -1.17 -0.718 predict(tunedKMeansModel, newdata = newCell) ## Warning in predict.WrappedModel(tunedKMeansModel, newdata = newCell): Provided ## data for prediction is not a pure data.frame but from class tbl_df, hence it ## will be converted. ## Prediction: 1 observations ## predict.type: response ## threshold: ## time: 0.02 ## response ## 1 3 Podemos ver en la salida que este nuevo caso está más cerca del centroide de grupo 1. Para saber la características principales que posee el grupo 1 es importante ver las observaciones que pertenecen a este grupo y encontrar puntos en común con el fin de entender mejor la clasificación obtenida. 6.2.1.8 Que tipos de classif existen clase paquete Num Fac Ord Nas Pesos cluster.kkmeans kknn x x x prob, dos clase Delegados a con . Establecemos model en FALSE de forma predeterminada para ahorrar memoria. glm family = binomial(link = 'logit') classif. LiblineaRL1LogReg LiblineaR x dos clase, multiclase, prob, class.weights classif. LiblineaRL2LogReg LiblineaR x dos clase, multiclase, prob, class.weights type = 0 (el valor predeterminado) es primario y es doble problema.type = 7 &lt;div "]]
